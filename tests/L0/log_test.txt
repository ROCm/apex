test_add_param_group (test_add_param_group.TestAddParamGroup) ... ok
test_bce_is_float_with_allow_banned (test_basic_casts.TestBannedMethods) ... ok
test_bce_raises_by_default (test_basic_casts.TestBannedMethods) ... ok
test_batch_norm_is_match (test_basic_casts.TestBasicCastsBFloat16) ... ok
test_conv2d_is_bfloat16 (test_basic_casts.TestBasicCastsBFloat16) ... skipped "test doesn't currently work on ROCm stack."
test_group_norm_is_float (test_basic_casts.TestBasicCastsBFloat16) ... ok
test_linear_is_bfloat16 (test_basic_casts.TestBasicCastsBFloat16) ... skipped "test doesn't currently work on ROCm stack."
test_mse_loss_is_float (test_basic_casts.TestBasicCastsBFloat16) ... ok
test_relu_is_match (test_basic_casts.TestBasicCastsBFloat16) ... ok
test_softmax_is_float (test_basic_casts.TestBasicCastsBFloat16) ... ok
test_batch_norm_is_match (test_basic_casts.TestBasicCastsHalf) ... ok
test_conv2d_is_half (test_basic_casts.TestBasicCastsHalf) ... ok
test_group_norm_is_float (test_basic_casts.TestBasicCastsHalf) ... ok
test_linear_is_half (test_basic_casts.TestBasicCastsHalf) ... ok
test_mse_loss_is_float (test_basic_casts.TestBasicCastsHalf) ... ok
test_relu_is_match (test_basic_casts.TestBasicCastsHalf) ... ok
test_softmax_is_float (test_basic_casts.TestBasicCastsHalf) ... ok
test_cpu_is_float (test_basic_casts.TestTensorCastsBFloat16) ... ok
test_matmul_method_is_bfloat16 (test_basic_casts.TestTensorCastsBFloat16) ... skipped "test doesn't currently work on ROCm stack."
test_matmul_op_is_bfloat16 (test_basic_casts.TestTensorCastsBFloat16) ... skipped "test doesn't currently work on ROCm stack."
test_pow_method_is_float (test_basic_casts.TestTensorCastsBFloat16) ... ok
test_pow_op_is_float (test_basic_casts.TestTensorCastsBFloat16) ... ok
test_sum_is_float (test_basic_casts.TestTensorCastsBFloat16) ... ok
test_cpu_is_float (test_basic_casts.TestTensorCastsHalf) ... ok
test_matmul_method_is_half (test_basic_casts.TestTensorCastsHalf) ... ok
test_matmul_op_is_half (test_basic_casts.TestTensorCastsHalf) ... ok
test_pow_method_is_float (test_basic_casts.TestTensorCastsHalf) ... ok
test_pow_op_is_float (test_basic_casts.TestTensorCastsHalf) ... ok
test_sum_is_float (test_basic_casts.TestTensorCastsHalf) ... ok
test_blacklist_module_bfp16_weight (test_cache.TestCache) ... ok
test_blacklist_module_fp16_weight (test_cache.TestCache) ... ok
test_blacklist_module_fp32_weight (test_cache.TestCache) ... ok
test_promote_module_bfp16_weight (test_cache.TestCache) ... ok
test_promote_module_fp16_weight (test_cache.TestCache) ... ok
test_promote_module_fp32_weight (test_cache.TestCache) ... ok
test_whitelist_module_bfp16_weight (test_cache.TestCache) ... ok
test_whitelist_module_fp16_weight (test_cache.TestCache) ... ok
test_whitelist_module_fp32_weight (test_cache.TestCache) ... ok
test_loss_scale_decrease (test_checkpointing.TestCheckpointing) ... skipped 'Test is flaky.'
test_restoring (test_checkpointing.TestCheckpointing) ... ok
test_state_dict (test_checkpointing.TestCheckpointing) ... /skishore/github/pytorch/torch/utils/_device.py:100: UserWarning: Converting a tensor with requires_grad=True to a scalar may lead to unexpected behavior.
Consider using tensor.detach() first. (Triggered internally at /skishore/github/pytorch/aten/src/ATen/native/Scalar.cpp:22.)
  return func(*args, **kwargs)
ok
test_2models2losses1optimizer (test_fused_sgd.TestMultipleModelsOptimizersLosses) ... ok
test_2models2losses2optimizers (test_fused_sgd.TestMultipleModelsOptimizersLosses) ... ok
test_3models2losses1optimizer (test_fused_sgd.TestMultipleModelsOptimizersLosses) ... ok
test_3models2losses2optimizers (test_fused_sgd.TestMultipleModelsOptimizersLosses) ... ok
test_larc_mixed_precision (test_larc.TestLARC) ... ok
test_fuzz (test_multi_tensor_axpby.TestMultiTensorAxpby) ... ok
test_fuzz_nhwc (test_multi_tensor_axpby.TestMultiTensorAxpby) ... ok
test_fuzz (test_multi_tensor_l2norm.TestMultiTensorL2Norm) ... /skishore/github/apex/tests/L0/run_amp/test_multi_tensor_l2norm.py:37: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /skishore/github/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
  a = torch.cuda.FloatTensor(sizea).fill_(self.val)
ok
test_fuzz (test_multi_tensor_scale.TestMultiTensorScale) ... ok
test_2models2losses1optimizer (test_multiple_models_optimizers_losses.TestMultipleModelsOptimizersLosses) ... ok
test_2models2losses2optimizers (test_multiple_models_optimizers_losses.TestMultipleModelsOptimizersLosses) ... ok
test_3models2losses1optimizer (test_multiple_models_optimizers_losses.TestMultipleModelsOptimizersLosses) ... ok
test_3models2losses2optimizers (test_multiple_models_optimizers_losses.TestMultipleModelsOptimizersLosses) ... ok
test_cat_matches_widest (test_promotion.TestPromotionBFloat16) ... ok
test_inplace_add_matches_self (test_promotion.TestPromotionBFloat16) ... ok
test_inplace_exp_is_error_for_bfloat16 (test_promotion.TestPromotionBFloat16) ... ok
test_mul_matches_widest (test_promotion.TestPromotionBFloat16) ... ok
test_atan2_matches_widest (test_promotion.TestPromotionHalf) ... ok
test_cat_matches_widest (test_promotion.TestPromotionHalf) ... ok
test_inplace_add_matches_self (test_promotion.TestPromotionHalf) ... ok
test_inplace_exp_is_error_for_half (test_promotion.TestPromotionHalf) ... ok
test_mul_matches_widest (test_promotion.TestPromotionHalf) ... ok
test_gru_cell_is_half (test_rnn.TestRnnCells) ... ok
test_lstm_cell_is_half (test_rnn.TestRnnCells) ... ok
test_rnn_cell_is_half (test_rnn.TestRnnCells) ... ok
test_gru_is_half (test_rnn.TestRnns) ... skipped "test doesn't currently work on ROCm stack."
test_lstm_is_half (test_rnn.TestRnns) ... skipped "test doesn't currently work on ROCm stack."
test_rnn_is_half (test_rnn.TestRnns) ... skipped "test doesn't currently work on ROCm stack."
test_rnn_packed_sequence (test_rnn.TestRnns) ... skipped "test doesn't currently work on ROCm stack."

----------------------------------------------------------------------
Ran 70 tests in 77.770s

OK (skipped=9)
test_output_is_half (test_fp16util.TestFP16Model) ... ok
test_params_and_buffers (test_fp16util.TestFP16Model) ... ok

----------------------------------------------------------------------
Ran 2 tests in 0.577s

OK
testGradScaler (test_adam.AdamTest) ... ok
testGradScalerCapturable (test_adam.AdamTest) ... /skishore/github/pytorch/torch/amp/grad_scaler.py:423: FutureWarning: GradScaler is going to stop passing itself as a keyword argument to the passed optimizer. In the near future GradScaler registers `grad_scale: Tensor` and `found_inf: Tensor` to the passed optimizer and let the optimizer use them directly.
  warnings.warn(
ok
testGradScalerCapturableMaster (test_adam.AdamTest) ... ok
testLargeTensor (test_adam.AdamTest) ... ok
testNative (test_adam.AdamTest) ... ok
test_float (test_fused_novograd.TestFusedNovoGrad) ... ok
test_half (test_fused_novograd.TestFusedNovoGrad) ... ok
test_multi_device (test_fused_novograd.TestFusedNovoGrad) ... ok
test_multi_params (test_fused_novograd.TestFusedNovoGrad) ... ok
test_adagrad_option (test_fused_optimizer.TestFusedAdagrad) ... ok
test_float (test_fused_optimizer.TestFusedAdagrad) ... ok
test_half (test_fused_optimizer.TestFusedAdagrad) ... skipped 'PyTorch optimizer is not numerically correct for fp16'
test_multi_device (test_fused_optimizer.TestFusedAdagrad) ... ok
test_multi_params (test_fused_optimizer.TestFusedAdagrad) ... ok
test_multi_params_different_devices_throws (test_fused_optimizer.TestFusedAdagrad) ... ok
test_adam_option (test_fused_optimizer.TestFusedAdam) ... ok
test_bfloat16 (test_fused_optimizer.TestFusedAdam) ... skipped "test doesn't currently work on ROCm stack."
test_float (test_fused_optimizer.TestFusedAdam) ... ok
test_fp16_output (test_fused_optimizer.TestFusedAdam) ... skipped 'No longer support output fp16 param'
test_half (test_fused_optimizer.TestFusedAdam) ... skipped 'NaN issue observed on ROCm as of 12/1/2021. The failing unit test is introduced by a PyTorch commit sometime in between rocm/pytorch:rocm4.3.1_ubuntu18.04_py3.6_pytorch_1.9.0 and 2021/12/01. Please refer to https://github.com/ROCmSoftwarePlatform/apex/issues/63'
test_multi_device (test_fused_optimizer.TestFusedAdam) ... ok
test_multi_params (test_fused_optimizer.TestFusedAdam) ... skipped 'Disable until 8/1/2019 adam/adamw upstream picked'
test_scale (test_fused_optimizer.TestFusedAdam) ... skipped 'No longer support fuse scaling'
test_float (test_fused_optimizer.TestFusedSGD) ... ok
test_half (test_fused_optimizer.TestFusedSGD) ... ok
test_multi_device (test_fused_optimizer.TestFusedSGD) ... ok
test_float (test_fused_optimizer_channels_last.TestFusedSGDChannelLast) ... ok
test_half (test_fused_optimizer_channels_last.TestFusedSGDChannelLast) ... ok
test_multi_device (test_fused_optimizer_channels_last.TestFusedSGDChannelLast) ... ok
test_float (test_lamb.TestFusedLAMB) ... ok
test_half (test_lamb.TestFusedLAMB) ... skipped 'PyTorch optimizer is not numerically correct for fp16'
test_lamb_option (test_lamb.TestFusedLAMB) ... ok
test_multi_device (test_lamb.TestFusedLAMB) ... ok
test_multi_params (test_lamb.TestFusedLAMB) ... ok
test_float (test_lamb.TestFusedMixedPrecisionLamb) ... ok
test_half (test_lamb.TestFusedMixedPrecisionLamb) ... skipped 'PyTorch optimizer is not numerically correct for fp16'
test_lamb_option (test_lamb.TestFusedMixedPrecisionLamb) ... ok
test_multi_device (test_lamb.TestFusedMixedPrecisionLamb) ... skipped 'Skipped the test since it failed the accuracy test on the PyTorch as of 8/1/2022. Please refer to https://github.com/ROCmSoftwarePlatform/apex/issues/83'
test_multi_params (test_lamb.TestFusedMixedPrecisionLamb) ... ok

----------------------------------------------------------------------
Ran 39 tests in 15.676s

OK (skipped=9)
test_autocast_fused_layer_norm_bfloat16_elementwise_affine_False_memory_efficient_False_cuda_bfloat16 (test_fused_layer_norm.TestFusedLayerNormCUDA) ... /opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/_autocast_utils.py:26: FutureWarning: `torch.cuda.amp.autocast_mode._cast(value, dtype)` is deprecated. Please use `torch.amp.autocast_mode._cast(value, 'cuda', dtype)` instead.
  return torch.cuda.amp.autocast_mode._cast(args, torch.get_autocast_gpu_dtype())
ok
test_autocast_fused_layer_norm_bfloat16_elementwise_affine_False_memory_efficient_True_cuda_bfloat16 (test_fused_layer_norm.TestFusedLayerNormCUDA) ... ok
test_autocast_fused_layer_norm_bfloat16_elementwise_affine_True_memory_efficient_False_cuda_bfloat16 (test_fused_layer_norm.TestFusedLayerNormCUDA) ... ok
test_autocast_fused_layer_norm_bfloat16_elementwise_affine_True_memory_efficient_True_cuda_bfloat16 (test_fused_layer_norm.TestFusedLayerNormCUDA) ... ok
test_autocast_fused_layer_norm_float16_elementwise_affine_False_memory_efficient_False_cuda_float16 (test_fused_layer_norm.TestFusedLayerNormCUDA) ... ok
test_autocast_fused_layer_norm_float16_elementwise_affine_False_memory_efficient_True_cuda_float16 (test_fused_layer_norm.TestFusedLayerNormCUDA) ... ok
test_autocast_fused_layer_norm_float16_elementwise_affine_True_memory_efficient_False_cuda_float16 (test_fused_layer_norm.TestFusedLayerNormCUDA) ... ok
test_autocast_fused_layer_norm_float16_elementwise_affine_True_memory_efficient_True_cuda_float16 (test_fused_layer_norm.TestFusedLayerNormCUDA) ... ok
test_autocast_fused_rms_norm_bfloat16_elementwise_affine_False_memory_efficient_False_cuda_bfloat16 (test_fused_layer_norm.TestFusedLayerNormCUDA) ... ok
test_autocast_fused_rms_norm_bfloat16_elementwise_affine_False_memory_efficient_True_cuda_bfloat16 (test_fused_layer_norm.TestFusedLayerNormCUDA) ... ok
test_autocast_fused_rms_norm_bfloat16_elementwise_affine_True_memory_efficient_False_cuda_bfloat16 (test_fused_layer_norm.TestFusedLayerNormCUDA) ... ok
test_autocast_fused_rms_norm_bfloat16_elementwise_affine_True_memory_efficient_True_cuda_bfloat16 (test_fused_layer_norm.TestFusedLayerNormCUDA) ... ok
test_autocast_fused_rms_norm_float16_elementwise_affine_False_memory_efficient_False_cuda_float16 (test_fused_layer_norm.TestFusedLayerNormCUDA) ... ok
test_autocast_fused_rms_norm_float16_elementwise_affine_False_memory_efficient_True_cuda_float16 (test_fused_layer_norm.TestFusedLayerNormCUDA) ... ok
test_autocast_fused_rms_norm_float16_elementwise_affine_True_memory_efficient_False_cuda_float16 (test_fused_layer_norm.TestFusedLayerNormCUDA) ... ok
test_autocast_fused_rms_norm_float16_elementwise_affine_True_memory_efficient_True_cuda_float16 (test_fused_layer_norm.TestFusedLayerNormCUDA) ... ok
test_compile_fused_layer_norm_elementwise_affine_False_cuda (test_fused_layer_norm.TestFusedLayerNormCUDA) ... ok
test_compile_fused_layer_norm_elementwise_affine_True_cuda (test_fused_layer_norm.TestFusedLayerNormCUDA) ... ok
test_compile_fused_rms_norm_elementwise_affine_False_cuda (test_fused_layer_norm.TestFusedLayerNormCUDA) ... ok
test_compile_fused_rms_norm_elementwise_affine_True_cuda (test_fused_layer_norm.TestFusedLayerNormCUDA) ... ok
test_layer_norm_bfloat16_batch_size_16_contiguous_False_elementwise_affine_True_mixed_fused_False_bfloat16_memory_efficient_False_cuda_bfloat16 (test_fused_layer_norm.TestFusedLayerNormCUDA) ... ok
test_layer_norm_bfloat16_batch_size_16_contiguous_False_elementwise_affine_True_mixed_fused_False_bfloat16_memory_efficient_True_cuda_bfloat16 (test_fused_layer_norm.TestFusedLayerNormCUDA) ... ok
test_layer_norm_bfloat16_batch_size_16_contiguous_True_elementwise_affine_True_mixed_fused_False_bfloat16_memory_efficient_False_cuda_bfloat16 (test_fused_layer_norm.TestFusedLayerNormCUDA) ... ok
test_layer_norm_bfloat16_batch_size_16_contiguous_True_elementwise_affine_True_mixed_fused_False_bfloat16_memory_efficient_True_cuda_bfloat16 (test_fused_layer_norm.TestFusedLayerNormCUDA) ... ok
test_layer_norm_elemwise_batch_size_16_contiguous_False_elementwise_affine_True_mixed_fused_False_float32_memory_efficient_False_cuda_float32 (test_fused_layer_norm.TestFusedLayerNormCUDA) ... ok
test_layer_norm_elemwise_batch_size_16_contiguous_False_elementwise_affine_True_mixed_fused_False_float32_memory_efficient_True_cuda_float32 (test_fused_layer_norm.TestFusedLayerNormCUDA) ... ok
test_layer_norm_elemwise_batch_size_16_contiguous_True_elementwise_affine_True_mixed_fused_False_float32_memory_efficient_False_cuda_float32 (test_fused_layer_norm.TestFusedLayerNormCUDA) ... ok
test_layer_norm_elemwise_batch_size_16_contiguous_True_elementwise_affine_True_mixed_fused_False_float32_memory_efficient_True_cuda_float32 (test_fused_layer_norm.TestFusedLayerNormCUDA) ... ok
test_layer_norm_elemwise_batch_size_65536_contiguous_False_elementwise_affine_True_mixed_fused_False_float32_memory_efficient_False_cuda_float32 (test_fused_layer_norm.TestFusedLayerNormCUDA) ... ok
test_layer_norm_elemwise_batch_size_65536_contiguous_False_elementwise_affine_True_mixed_fused_False_float32_memory_efficient_True_cuda_float32 (test_fused_layer_norm.TestFusedLayerNormCUDA) ... ok
test_layer_norm_elemwise_batch_size_65536_contiguous_True_elementwise_affine_True_mixed_fused_False_float32_memory_efficient_False_cuda_float32 (test_fused_layer_norm.TestFusedLayerNormCUDA) ... ok
test_layer_norm_elemwise_batch_size_65536_contiguous_True_elementwise_affine_True_mixed_fused_False_float32_memory_efficient_True_cuda_float32 (test_fused_layer_norm.TestFusedLayerNormCUDA) ... ok
test_layer_norm_export_cuda (test_fused_layer_norm.TestFusedLayerNormCUDA) ... ok
test_layer_norm_half_batch_size_16_contiguous_False_elementwise_affine_True_mixed_fused_False_float16_memory_efficient_False_cuda_float16 (test_fused_layer_norm.TestFusedLayerNormCUDA) ... ok
test_layer_norm_half_batch_size_16_contiguous_False_elementwise_affine_True_mixed_fused_False_float16_memory_efficient_True_cuda_float16 (test_fused_layer_norm.TestFusedLayerNormCUDA) ... ok
test_layer_norm_half_batch_size_16_contiguous_True_elementwise_affine_True_mixed_fused_False_float16_memory_efficient_False_cuda_float16 (test_fused_layer_norm.TestFusedLayerNormCUDA) ... ok
test_layer_norm_half_batch_size_16_contiguous_True_elementwise_affine_True_mixed_fused_False_float16_memory_efficient_True_cuda_float16 (test_fused_layer_norm.TestFusedLayerNormCUDA) ... ok
test_layer_norm_mixed_batch_size_16_contiguous_False_elementwise_affine_True_mixed_fused_True_float32_memory_efficient_False_cuda_float32 (test_fused_layer_norm.TestFusedLayerNormCUDA) ... ok
test_layer_norm_mixed_batch_size_16_contiguous_False_elementwise_affine_True_mixed_fused_True_float32_memory_efficient_True_cuda_float32 (test_fused_layer_norm.TestFusedLayerNormCUDA) ... ok
test_layer_norm_mixed_batch_size_16_contiguous_True_elementwise_affine_True_mixed_fused_True_float32_memory_efficient_False_cuda_float32 (test_fused_layer_norm.TestFusedLayerNormCUDA) ... ok
test_layer_norm_mixed_batch_size_16_contiguous_True_elementwise_affine_True_mixed_fused_True_float32_memory_efficient_True_cuda_float32 (test_fused_layer_norm.TestFusedLayerNormCUDA) ... ok
test_layer_norm_mixed_batch_size_65536_contiguous_False_elementwise_affine_True_mixed_fused_True_float32_memory_efficient_False_cuda_float32 (test_fused_layer_norm.TestFusedLayerNormCUDA) ... ok
test_layer_norm_mixed_batch_size_65536_contiguous_False_elementwise_affine_True_mixed_fused_True_float32_memory_efficient_True_cuda_float32 (test_fused_layer_norm.TestFusedLayerNormCUDA) ... ok
test_layer_norm_mixed_batch_size_65536_contiguous_True_elementwise_affine_True_mixed_fused_True_float32_memory_efficient_False_cuda_float32 (test_fused_layer_norm.TestFusedLayerNormCUDA) ... ok
test_layer_norm_mixed_batch_size_65536_contiguous_True_elementwise_affine_True_mixed_fused_True_float32_memory_efficient_True_cuda_float32 (test_fused_layer_norm.TestFusedLayerNormCUDA) ... ok
test_layer_norm_regular_batch_size_16_contiguous_False_elementwise_affine_False_mixed_fused_False_float32_memory_efficient_False_cuda_float32 (test_fused_layer_norm.TestFusedLayerNormCUDA) ... ok
test_layer_norm_regular_batch_size_16_contiguous_False_elementwise_affine_False_mixed_fused_False_float32_memory_efficient_True_cuda_float32 (test_fused_layer_norm.TestFusedLayerNormCUDA) ... ok
test_layer_norm_regular_batch_size_16_contiguous_True_elementwise_affine_False_mixed_fused_False_float32_memory_efficient_False_cuda_float32 (test_fused_layer_norm.TestFusedLayerNormCUDA) ... ok
test_layer_norm_regular_batch_size_16_contiguous_True_elementwise_affine_False_mixed_fused_False_float32_memory_efficient_True_cuda_float32 (test_fused_layer_norm.TestFusedLayerNormCUDA) ... ok
test_layer_norm_regular_batch_size_65536_contiguous_False_elementwise_affine_False_mixed_fused_False_float32_memory_efficient_False_cuda_float32 (test_fused_layer_norm.TestFusedLayerNormCUDA) ... ok
test_layer_norm_regular_batch_size_65536_contiguous_False_elementwise_affine_False_mixed_fused_False_float32_memory_efficient_True_cuda_float32 (test_fused_layer_norm.TestFusedLayerNormCUDA) ... ok
test_layer_norm_regular_batch_size_65536_contiguous_True_elementwise_affine_False_mixed_fused_False_float32_memory_efficient_False_cuda_float32 (test_fused_layer_norm.TestFusedLayerNormCUDA) ... ok
test_layer_norm_regular_batch_size_65536_contiguous_True_elementwise_affine_False_mixed_fused_False_float32_memory_efficient_True_cuda_float32 (test_fused_layer_norm.TestFusedLayerNormCUDA) ... ok
test_rms_export_cuda (test_fused_layer_norm.TestFusedLayerNormCUDA) ... ok
test_rms_norm_bfloat16_batch_size_16_contiguous_False_elementwise_affine_True_mixed_fused_False_bfloat16_memory_efficient_False_cuda_bfloat16 (test_fused_layer_norm.TestFusedLayerNormCUDA) ... ok
test_rms_norm_bfloat16_batch_size_16_contiguous_False_elementwise_affine_True_mixed_fused_False_bfloat16_memory_efficient_True_cuda_bfloat16 (test_fused_layer_norm.TestFusedLayerNormCUDA) ... ok
test_rms_norm_bfloat16_batch_size_16_contiguous_True_elementwise_affine_True_mixed_fused_False_bfloat16_memory_efficient_False_cuda_bfloat16 (test_fused_layer_norm.TestFusedLayerNormCUDA) ... ok
test_rms_norm_bfloat16_batch_size_16_contiguous_True_elementwise_affine_True_mixed_fused_False_bfloat16_memory_efficient_True_cuda_bfloat16 (test_fused_layer_norm.TestFusedLayerNormCUDA) ... ok
test_rms_norm_elemwise_batch_size_16_contiguous_False_elementwise_affine_True_mixed_fused_False_float32_memory_efficient_False_cuda_float32 (test_fused_layer_norm.TestFusedLayerNormCUDA) ... ok
test_rms_norm_elemwise_batch_size_16_contiguous_False_elementwise_affine_True_mixed_fused_False_float32_memory_efficient_True_cuda_float32 (test_fused_layer_norm.TestFusedLayerNormCUDA) ... ok
test_rms_norm_elemwise_batch_size_16_contiguous_True_elementwise_affine_True_mixed_fused_False_float32_memory_efficient_False_cuda_float32 (test_fused_layer_norm.TestFusedLayerNormCUDA) ... ok
test_rms_norm_elemwise_batch_size_16_contiguous_True_elementwise_affine_True_mixed_fused_False_float32_memory_efficient_True_cuda_float32 (test_fused_layer_norm.TestFusedLayerNormCUDA) ... ok
test_rms_norm_elemwise_batch_size_65536_contiguous_False_elementwise_affine_True_mixed_fused_False_float32_memory_efficient_False_cuda_float32 (test_fused_layer_norm.TestFusedLayerNormCUDA) ... ok
test_rms_norm_elemwise_batch_size_65536_contiguous_False_elementwise_affine_True_mixed_fused_False_float32_memory_efficient_True_cuda_float32 (test_fused_layer_norm.TestFusedLayerNormCUDA) ... ok
test_rms_norm_elemwise_batch_size_65536_contiguous_True_elementwise_affine_True_mixed_fused_False_float32_memory_efficient_False_cuda_float32 (test_fused_layer_norm.TestFusedLayerNormCUDA) ... ok
test_rms_norm_elemwise_batch_size_65536_contiguous_True_elementwise_affine_True_mixed_fused_False_float32_memory_efficient_True_cuda_float32 (test_fused_layer_norm.TestFusedLayerNormCUDA) ... ok
test_rms_norm_half_batch_size_16_contiguous_False_elementwise_affine_True_mixed_fused_False_float16_memory_efficient_False_cuda_float16 (test_fused_layer_norm.TestFusedLayerNormCUDA) ... ok
test_rms_norm_half_batch_size_16_contiguous_False_elementwise_affine_True_mixed_fused_False_float16_memory_efficient_True_cuda_float16 (test_fused_layer_norm.TestFusedLayerNormCUDA) ... ok
test_rms_norm_half_batch_size_16_contiguous_True_elementwise_affine_True_mixed_fused_False_float16_memory_efficient_False_cuda_float16 (test_fused_layer_norm.TestFusedLayerNormCUDA) ... ok
test_rms_norm_half_batch_size_16_contiguous_True_elementwise_affine_True_mixed_fused_False_float16_memory_efficient_True_cuda_float16 (test_fused_layer_norm.TestFusedLayerNormCUDA) ... ok
test_rms_norm_mixed_batch_size_16_contiguous_False_elementwise_affine_True_mixed_fused_True_float32_memory_efficient_False_cuda_float32 (test_fused_layer_norm.TestFusedLayerNormCUDA) ... ok
test_rms_norm_mixed_batch_size_16_contiguous_False_elementwise_affine_True_mixed_fused_True_float32_memory_efficient_True_cuda_float32 (test_fused_layer_norm.TestFusedLayerNormCUDA) ... ok
test_rms_norm_mixed_batch_size_16_contiguous_True_elementwise_affine_True_mixed_fused_True_float32_memory_efficient_False_cuda_float32 (test_fused_layer_norm.TestFusedLayerNormCUDA) ... ok
test_rms_norm_mixed_batch_size_16_contiguous_True_elementwise_affine_True_mixed_fused_True_float32_memory_efficient_True_cuda_float32 (test_fused_layer_norm.TestFusedLayerNormCUDA) ... ok
test_rms_norm_mixed_batch_size_65536_contiguous_False_elementwise_affine_True_mixed_fused_True_float32_memory_efficient_False_cuda_float32 (test_fused_layer_norm.TestFusedLayerNormCUDA) ... ok
test_rms_norm_mixed_batch_size_65536_contiguous_False_elementwise_affine_True_mixed_fused_True_float32_memory_efficient_True_cuda_float32 (test_fused_layer_norm.TestFusedLayerNormCUDA) ... ok
test_rms_norm_mixed_batch_size_65536_contiguous_True_elementwise_affine_True_mixed_fused_True_float32_memory_efficient_False_cuda_float32 (test_fused_layer_norm.TestFusedLayerNormCUDA) ... ok
test_rms_norm_mixed_batch_size_65536_contiguous_True_elementwise_affine_True_mixed_fused_True_float32_memory_efficient_True_cuda_float32 (test_fused_layer_norm.TestFusedLayerNormCUDA) ... ok
test_rms_norm_regular_batch_size_16_contiguous_False_elementwise_affine_False_mixed_fused_False_float32_memory_efficient_False_cuda_float32 (test_fused_layer_norm.TestFusedLayerNormCUDA) ... ok
test_rms_norm_regular_batch_size_16_contiguous_False_elementwise_affine_False_mixed_fused_False_float32_memory_efficient_True_cuda_float32 (test_fused_layer_norm.TestFusedLayerNormCUDA) ... ok
test_rms_norm_regular_batch_size_16_contiguous_True_elementwise_affine_False_mixed_fused_False_float32_memory_efficient_False_cuda_float32 (test_fused_layer_norm.TestFusedLayerNormCUDA) ... ok
test_rms_norm_regular_batch_size_16_contiguous_True_elementwise_affine_False_mixed_fused_False_float32_memory_efficient_True_cuda_float32 (test_fused_layer_norm.TestFusedLayerNormCUDA) ... ok
test_rms_norm_regular_batch_size_65536_contiguous_False_elementwise_affine_False_mixed_fused_False_float32_memory_efficient_False_cuda_float32 (test_fused_layer_norm.TestFusedLayerNormCUDA) ... ok
test_rms_norm_regular_batch_size_65536_contiguous_False_elementwise_affine_False_mixed_fused_False_float32_memory_efficient_True_cuda_float32 (test_fused_layer_norm.TestFusedLayerNormCUDA) ... ok
test_rms_norm_regular_batch_size_65536_contiguous_True_elementwise_affine_False_mixed_fused_False_float32_memory_efficient_False_cuda_float32 (test_fused_layer_norm.TestFusedLayerNormCUDA) ... ok
test_rms_norm_regular_batch_size_65536_contiguous_True_elementwise_affine_False_mixed_fused_False_float32_memory_efficient_True_cuda_float32 (test_fused_layer_norm.TestFusedLayerNormCUDA) ... ok

----------------------------------------------------------------------
Ran 86 tests in 108.763s

OK
test_creation (test_mlp.TestMLP) ... ok
test_no_bias (test_mlp.TestMLP) ... skipped 'Test is flaky.'
test_no_grad (test_mlp.TestMLP) ... skipped 'Test is flaky.'
test_numeric (test_mlp.TestMLP) ... skipped 'Test is flaky.'
test_performance_half (test_mlp.TestMLP) ... ok
test_with_bias (test_mlp.TestMLP) ... skipped 'Test is flaky.'

----------------------------------------------------------------------
Ran 6 tests in 1.272s

OK (skipped=4)
test_fused_dense (test_fused_dense.FusedDenseTest) ... ok
test_fused_dense_gelu_dense (test_gelu.FusedDenseGeluDenseTest) ... ok

----------------------------------------------------------------------
Ran 2 tests in 0.052s

OK
test_batch_sampler_behavior (test_batch_sampler.TestBatchSamplerBehavior) ... /skishore/github/apex/tests/L0/run_amp

Executing tests from /skishore/github/apex/tests/L0/run_amp
Warning:  unscaling grads that are not FP32. Unscaling non-fp32 grads may indicate an error. When using Amp, you don't need to call .half() on your model.
/skishore/github/apex/tests/L0/run_fp16util

Executing tests from /skishore/github/apex/tests/L0/run_fp16util
/skishore/github/apex/tests/L0/run_optimizers

Executing tests from /skishore/github/apex/tests/L0/run_optimizers
/skishore/github/apex/tests/L0/run_fused_layer_norm

Executing tests from /skishore/github/apex/tests/L0/run_fused_layer_norm
/skishore/github/apex/tests/L0/run_mlp

Executing tests from /skishore/github/apex/tests/L0/run_mlp

Pytorch MLP time 1.4724 ms
C++ MLP time 0.7349 ms
/skishore/github/apex/tests/L0/run_fused_dense

Executing tests from /skishore/github/apex/tests/L0/run_fused_dense
/skishore/github/apex/tests/L0/run_transformer

Executing tests from /skishore/github/apex/tests/L0/run_transformer
ok
test_split_batch (test_batch_sampler.TestBatchSamplerBehavior) ... ok
test_cross_entropy (test_cross_entropy.NcclVocabParallelCrossEntropyTest) ... [rank3]:[W603 15:57:52.202502613 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank2]:[W603 15:57:52.204052493 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank1]:[W603 15:57:52.206180331 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank0]:[W603 15:57:52.628771833 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
/skishore/github/apex/tests/L0/run_transformer/test_cross_entropy.py:28: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /skishore/github/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
  target = torch.cuda.LongTensor(size=(batch_size, seq_length)).random_(0, vocab_size)
/skishore/github/apex/tests/L0/run_transformer/test_cross_entropy.py:28: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /skishore/github/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
  target = torch.cuda.LongTensor(size=(batch_size, seq_length)).random_(0, vocab_size)
/skishore/github/apex/tests/L0/run_transformer/test_cross_entropy.py:28: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /skishore/github/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
  target = torch.cuda.LongTensor(size=(batch_size, seq_length)).random_(0, vocab_size)
/skishore/github/apex/tests/L0/run_transformer/test_cross_entropy.py:28: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /skishore/github/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
  target = torch.cuda.LongTensor(size=(batch_size, seq_length)).random_(0, vocab_size)
[dist init] rank = 0, world_size = 4
[dist init] rank = 3, world_size = 4
[dist init] rank = 2, world_size = 4
[dist init] rank = 1, world_size = 4
ok
test_cross_entropy (test_cross_entropy.UccVocabParallelCrossEntropyTest) ... skipped 'Requires [`torch_ucc`](https://github.com/facebookresearch/torch_ucc)'
test_broadcast_data (test_data.NcclBroadcastDataTest) ... [rank3]:[W603 15:58:13.144932380 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank2]:[W603 15:58:13.146511574 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank1]:[W603 15:58:13.150470229 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank0]:[W603 15:58:13.156459458 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
25-06-03 15:58:15 - PID:18274 - rank:(0, 0, 0, 0) - parallel_state.py:145 - INFO - > initializing tensor model parallel with size 4
25-06-03 15:58:15 - PID:18274 - rank:(0, 0, 0, 0) - parallel_state.py:150 - INFO - > initializing pipeline model parallel with size 1
25-06-03 15:58:15 - PID:18274 - rank:(0, 0, 0, 0) - parallel_state.py:155 - INFO - > initializing data parallel with size 1
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/tensor_parallel/data.py:50: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /skishore/github/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
  sizes_cuda = torch.cuda.LongTensor(sizes)
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/tensor_parallel/data.py:50: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /skishore/github/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
  sizes_cuda = torch.cuda.LongTensor(sizes)
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/tensor_parallel/data.py:50: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /skishore/github/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
  sizes_cuda = torch.cuda.LongTensor(sizes)
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/tensor_parallel/data.py:50: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /skishore/github/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
  sizes_cuda = torch.cuda.LongTensor(sizes)
[dist init] rank = 0, world_size = 4
[dist init] rank = 3, world_size = 4
[dist init] rank = 2, world_size = 4
[dist init] rank = 1, world_size = 4
ok
test_broadcast_data (test_data.UccBroadcastDataTest) ... skipped 'Requires [`torch_ucc`](https://github.com/facebookresearch/torch_ucc)'
test_fused_bias_swiglu (test_fused_bias_swiglu.TestFusedBiasSwiGLU) ... ok
test_2d_forward_backward (test_fused_rope.TestFusedRoPE) ... ok
test_forward_backward (test_fused_rope.TestFusedRoPE) ... ok
test_thd_forward_backward (test_fused_rope.TestFusedRoPE) ... ok
test_autocast_fused_scale_mask_softmax (test_fused_softmax.TestFusedScaleMaskSoftmax) ... /skishore/github/apex/tests/L0/run_transformer/test_fused_softmax.py:119: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(dtype=dtype):
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/_autocast_utils.py:26: FutureWarning: `torch.cuda.amp.autocast_mode._cast(value, dtype)` is deprecated. Please use `torch.amp.autocast_mode._cast(value, 'cuda', dtype)` instead.
  return torch.cuda.amp.autocast_mode._cast(args, torch.get_autocast_gpu_dtype())
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/functional/fused_softmax.py:98: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=False):
ok
test_autocast_fused_upper_triangle_mask_softmax (test_fused_softmax.TestFusedScaleMaskSoftmax) ... /skishore/github/apex/tests/L0/run_transformer/test_fused_softmax.py:207: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(dtype=dtype):
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/functional/fused_softmax.py:59: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=False):
ok
test_fused_scale_mask_softmax (test_fused_softmax.TestFusedScaleMaskSoftmax)
attention_scores.shape = [4, 12, 24, 24] ... ok
test_fused_upper_triangle_mask_softmax (test_fused_softmax.TestFusedScaleMaskSoftmax)
attn_weights.shape: [4, 12, 24, 24] ... ok
test_affine_weight_init_column_parallel_cpu (test_layers.NcclTensorParallelLayerTest) ... Testing with data type: torch.float32
Test succeeded for data type: torch.float32
Testing with data type: torch.float64
Test succeeded for data type: torch.float64
Testing with data type: torch.float16
Test succeeded for data type: torch.float16
[rank1]:[W603 15:58:37.066884666 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank2]:[W603 15:58:37.235722975 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank0]:[W603 15:58:37.239866274 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank3]:[W603 15:58:37.245574686 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[dist init] rank = 0, world_size = 4
[dist init] rank = 3, world_size = 4
[dist init] rank = 1, world_size = 4
[dist init] rank = 2, world_size = 4
ok
test_affine_weight_init_column_parallel_gpu (test_layers.NcclTensorParallelLayerTest) ... [rank2]:[W603 15:58:57.652765635 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank0]:[W603 15:58:58.377812841 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank3]:[W603 15:58:58.434011858 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank1]:[W603 15:58:58.436293735 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[dist init] rank = 0, world_size = 4
[dist init] rank = 1, world_size = 4
[dist init] rank = 3, world_size = 4
[dist init] rank = 2, world_size = 4
ok
test_affine_weight_init_row_parallel_cpu (test_layers.NcclTensorParallelLayerTest) ... [rank0]:[W603 15:59:14.275444278 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank2]:[W603 15:59:14.276935120 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank3]:[W603 15:59:14.672555975 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank1]:[W603 15:59:14.674465709 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[dist init] rank = 0, world_size = 4
[dist init] rank = 3, world_size = 4
[dist init] rank = 2, world_size = 4
[dist init] rank = 1, world_size = 4
ok
test_affine_weight_init_row_parallel_gpu (test_layers.NcclTensorParallelLayerTest) ... [rank1]:[W603 15:59:29.087710480 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank0]:[W603 15:59:30.057818801 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank2]:[W603 15:59:30.066972767 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank3]:[W603 15:59:30.168463263 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[dist init] rank = 0, world_size = 4
[dist init] rank = 3, world_size = 4
[dist init] rank = 1, world_size = 4
[dist init] rank = 2, world_size = 4
ok
test_all_gather_parity (test_layers.NcclTensorParallelLayerTest) ... [rank0]:[W603 15:59:43.936977030 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank1]:[W603 15:59:44.440270376 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank3]:[W603 15:59:44.583908053 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank2]:[W603 15:59:44.586542463 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
/skishore/github/pytorch/torch/distributed/c10d_logger.py:81: FutureWarning: `torch.distributed._all_gather_base` is a private function and will be deprecated. Please use `torch.distributed.all_gather_into_tensor` instead.
  return func(*args, **kwargs)
/skishore/github/pytorch/torch/distributed/c10d_logger.py:81: FutureWarning: `torch.distributed._all_gather_base` is a private function and will be deprecated. Please use `torch.distributed.all_gather_into_tensor` instead.
  return func(*args, **kwargs)
/skishore/github/pytorch/torch/distributed/c10d_logger.py:81: FutureWarning: `torch.distributed._all_gather_base` is a private function and will be deprecated. Please use `torch.distributed.all_gather_into_tensor` instead.
  return func(*args, **kwargs)
/skishore/github/pytorch/torch/distributed/c10d_logger.py:81: FutureWarning: `torch.distributed._all_gather_base` is a private function and will be deprecated. Please use `torch.distributed.all_gather_into_tensor` instead.
  return func(*args, **kwargs)
[dist init] rank = 1, world_size = 4
[dist init] rank = 0, world_size = 4
[dist init] rank = 3, world_size = 4
[dist init] rank = 2, world_size = 4
ok
test_column_parallel_linear (test_layers.NcclTensorParallelLayerTest) ... [rank0]:[W603 16:00:01.921646084 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank2]:[W603 16:00:01.978806709 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank1]:[W603 16:00:01.015221152 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank3]:[W603 16:00:02.338829601 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[dist init] rank = 0, world_size = 4
[dist init] rank = 3, world_size = 4
[dist init] rank = 1, world_size = 4
[dist init] rank = 2, world_size = 4
ok
test_column_parallel_linear_async (test_layers.NcclTensorParallelLayerTest) ... [rank2]:[W603 16:00:28.003290927 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank1]:[W603 16:00:28.006099365 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank3]:[W603 16:00:29.187504064 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank0]:[W603 16:00:29.407647177 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[dist init] rank = 0, world_size = 4
[dist init] rank = 3, world_size = 4
[dist init] rank = 1, world_size = 4
[dist init] rank = 2, world_size = 4
ok
test_column_parallel_linear_exception (test_layers.NcclTensorParallelLayerTest) ... [rank2]:[W603 16:00:55.497104619 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank0]:[W603 16:00:55.499103466 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank3]:[W603 16:00:55.499710677 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank1]:[W603 16:00:55.505389085 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[dist init] rank = 0, world_size = 4
[dist init] rank = 3, world_size = 4
[dist init] rank = 1, world_size = 4
[dist init] rank = 2, world_size = 4
ok
test_column_parallel_linear_gradient_accumulation_fusion (test_layers.NcclTensorParallelLayerTest) ... [rank0]:[W603 16:01:08.438194094 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank3]:[W603 16:01:08.541519924 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank1]:[W603 16:01:08.588451817 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank2]:[W603 16:01:08.668895046 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[dist init] rank = 0, world_size = 4
[dist init] rank = 1, world_size = 4
[dist init] rank = 3, world_size = 4
[dist init] rank = 2, world_size = 4
ok
test_column_parallel_linear_gradient_accumulation_fusion_in_fp16 (test_layers.NcclTensorParallelLayerTest) ... [rank2]:[W603 16:01:36.871666800 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank1]:[W603 16:01:36.875749479 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank0]:[W603 16:01:36.881789253 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank3]:[W603 16:01:37.327369926 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[dist init] rank = 0, world_size = 4
[dist init] rank = 2, world_size = 4
[dist init] rank = 1, world_size = 4
[dist init] rank = 3, world_size = 4
ok
test_column_parallel_linear_sequence_parallel (test_layers.NcclTensorParallelLayerTest) ... [rank1]:[W603 16:02:03.137927117 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank2]:[W603 16:02:03.145352918 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank0]:[W603 16:02:03.157190847 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank3]:[W603 16:02:03.507710155 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/tensor_parallel/layers.py:547: UserWarning: `sequence_parallel_enabled` is set to `True`, but got world_size of 1
  warnings.warn(
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/tensor_parallel/layers.py:547: UserWarning: `sequence_parallel_enabled` is set to `True`, but got world_size of 1
  warnings.warn(
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/tensor_parallel/layers.py:547: UserWarning: `sequence_parallel_enabled` is set to `True`, but got world_size of 1
  warnings.warn(
/skishore/github/pytorch/torch/distributed/c10d_logger.py:81: FutureWarning: `torch.distributed._all_gather_base` is a private function and will be deprecated. Please use `torch.distributed.all_gather_into_tensor` instead.
  return func(*args, **kwargs)
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/tensor_parallel/layers.py:547: UserWarning: `sequence_parallel_enabled` is set to `True`, but got world_size of 1
  warnings.warn(
/skishore/github/pytorch/torch/distributed/c10d_logger.py:81: FutureWarning: `torch.distributed._all_gather_base` is a private function and will be deprecated. Please use `torch.distributed.all_gather_into_tensor` instead.
  return func(*args, **kwargs)
/skishore/github/pytorch/torch/distributed/c10d_logger.py:81: FutureWarning: `torch.distributed._all_gather_base` is a private function and will be deprecated. Please use `torch.distributed.all_gather_into_tensor` instead.
  return func(*args, **kwargs)
/skishore/github/pytorch/torch/distributed/c10d_logger.py:81: FutureWarning: `torch.distributed._all_gather_base` is a private function and will be deprecated. Please use `torch.distributed.all_gather_into_tensor` instead.
  return func(*args, **kwargs)
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/tensor_parallel/layers.py:358: FutureWarning: `torch.distributed._reduce_scatter_base` is a private function and will be deprecated. Please use `torch.distributed.reduce_scatter_tensor` instead.
  handle = torch.distributed._reduce_scatter_base(
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/tensor_parallel/layers.py:358: FutureWarning: `torch.distributed._reduce_scatter_base` is a private function and will be deprecated. Please use `torch.distributed.reduce_scatter_tensor` instead.
  handle = torch.distributed._reduce_scatter_base(
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/tensor_parallel/layers.py:358: FutureWarning: `torch.distributed._reduce_scatter_base` is a private function and will be deprecated. Please use `torch.distributed.reduce_scatter_tensor` instead.
  handle = torch.distributed._reduce_scatter_base(
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/tensor_parallel/layers.py:358: FutureWarning: `torch.distributed._reduce_scatter_base` is a private function and will be deprecated. Please use `torch.distributed.reduce_scatter_tensor` instead.
  handle = torch.distributed._reduce_scatter_base(
[dist init] rank = 3, world_size = 4
[dist init] rank = 1, world_size = 4
[dist init] rank = 2, world_size = 4
[dist init] rank = 0, world_size = 4
ok
test_parallel_embedding (test_layers.NcclTensorParallelLayerTest) ... [rank2]:[W603 16:02:32.355274319 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank0]:[W603 16:02:32.358432266 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank1]:[W603 16:02:32.358927070 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank3]:[W603 16:02:32.815480058 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[dist init] rank = 3, world_size = 4
[dist init] rank = 0, world_size = 4
[dist init] rank = 1, world_size = 4
[dist init] rank = 2, world_size = 4
ok
test_reduce_scatter_parity (test_layers.NcclTensorParallelLayerTest) ... [rank3]:[W603 16:02:51.824959503 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank2]:[W603 16:02:51.828917257 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank0]:[W603 16:02:51.871644950 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank1]:[W603 16:02:51.975317575 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
/skishore/github/apex/tests/L0/run_transformer/test_layers.py:127: FutureWarning: `torch.distributed._reduce_scatter_base` is a private function and will be deprecated. Please use `torch.distributed.reduce_scatter_tensor` instead.
  _reduce_scatter_base(
/skishore/github/apex/tests/L0/run_transformer/test_layers.py:127: FutureWarning: `torch.distributed._reduce_scatter_base` is a private function and will be deprecated. Please use `torch.distributed.reduce_scatter_tensor` instead.
  _reduce_scatter_base(
/skishore/github/apex/tests/L0/run_transformer/test_layers.py:127: FutureWarning: `torch.distributed._reduce_scatter_base` is a private function and will be deprecated. Please use `torch.distributed.reduce_scatter_tensor` instead.
  _reduce_scatter_base(
/skishore/github/apex/tests/L0/run_transformer/test_layers.py:127: FutureWarning: `torch.distributed._reduce_scatter_base` is a private function and will be deprecated. Please use `torch.distributed.reduce_scatter_tensor` instead.
  _reduce_scatter_base(
[dist init] rank = 0, world_size = 4
[dist init] rank = 3, world_size = 4
[dist init] rank = 2, world_size = 4
[dist init] rank = 1, world_size = 4
ok
test_row_parallel_linear (test_layers.NcclTensorParallelLayerTest) ... [rank1]:[W603 16:03:15.587822867 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank2]:[W603 16:03:15.593311653 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank0]:[W603 16:03:15.595137332 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank3]:[W603 16:03:15.597665144 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[dist init] rank = 0, world_size = 4
[dist init] rank = 3, world_size = 4
[dist init] rank = 2, world_size = 4
[dist init] rank = 1, world_size = 4
ok
test_row_parallel_linear_gradient_accumulation_fusion (test_layers.NcclTensorParallelLayerTest) ... [rank0]:[W603 16:03:46.418085342 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank2]:[W603 16:03:46.426745535 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank1]:[W603 16:03:46.428857389 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank3]:[W603 16:03:46.679548820 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[dist init] rank = 0, world_size = 4
[dist init] rank = 1, world_size = 4
[dist init] rank = 2, world_size = 4
[dist init] rank = 3, world_size = 4
ok
test_row_parallel_linear_gradient_accumulation_fusion_in_fp16 (test_layers.NcclTensorParallelLayerTest) ... [rank1]:[W603 16:04:17.567956750 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank3]:[W603 16:04:17.569834858 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank2]:[W603 16:04:17.571835267 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank0]:[W603 16:04:17.572720190 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[dist init] rank = 0, world_size = 4
[dist init] rank = 3, world_size = 4
[dist init] rank = 1, world_size = 4
[dist init] rank = 2, world_size = 4
ok
test_row_parallel_linear_sequence_parallel (test_layers.NcclTensorParallelLayerTest) ... [rank1]:[W603 16:04:43.923494574 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank3]:[W603 16:04:43.927424246 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank2]:[W603 16:04:43.934572735 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank0]:[W603 16:04:43.936981620 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/tensor_parallel/mappings.py:125: FutureWarning: `torch.distributed._reduce_scatter_base` is a private function and will be deprecated. Please use `torch.distributed.reduce_scatter_tensor` instead.
  torch.distributed._reduce_scatter_base(
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/tensor_parallel/mappings.py:125: FutureWarning: `torch.distributed._reduce_scatter_base` is a private function and will be deprecated. Please use `torch.distributed.reduce_scatter_tensor` instead.
  torch.distributed._reduce_scatter_base(
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/tensor_parallel/mappings.py:125: FutureWarning: `torch.distributed._reduce_scatter_base` is a private function and will be deprecated. Please use `torch.distributed.reduce_scatter_tensor` instead.
  torch.distributed._reduce_scatter_base(
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/tensor_parallel/mappings.py:125: FutureWarning: `torch.distributed._reduce_scatter_base` is a private function and will be deprecated. Please use `torch.distributed.reduce_scatter_tensor` instead.
  torch.distributed._reduce_scatter_base(
/skishore/github/pytorch/torch/distributed/c10d_logger.py:81: FutureWarning: `torch.distributed._all_gather_base` is a private function and will be deprecated. Please use `torch.distributed.all_gather_into_tensor` instead.
  return func(*args, **kwargs)
/skishore/github/pytorch/torch/distributed/c10d_logger.py:81: FutureWarning: `torch.distributed._all_gather_base` is a private function and will be deprecated. Please use `torch.distributed.all_gather_into_tensor` instead.
  return func(*args, **kwargs)
/skishore/github/pytorch/torch/distributed/c10d_logger.py:81: FutureWarning: `torch.distributed._all_gather_base` is a private function and will be deprecated. Please use `torch.distributed.all_gather_into_tensor` instead.
  return func(*args, **kwargs)
/skishore/github/pytorch/torch/distributed/c10d_logger.py:81: FutureWarning: `torch.distributed._all_gather_base` is a private function and will be deprecated. Please use `torch.distributed.all_gather_into_tensor` instead.
  return func(*args, **kwargs)
[dist init] rank = 3, world_size = 4
[dist init] rank = 2, world_size = 4
[dist init] rank = 0, world_size = 4
[dist init] rank = 1, world_size = 4
ok
test_affine_weight_init_column_parallel_cpu (test_layers.UccTensorParallelLayerTest) ... skipped 'Requires [`torch_ucc`](https://github.com/facebookresearch/torch_ucc)'
test_affine_weight_init_column_parallel_gpu (test_layers.UccTensorParallelLayerTest) ... skipped 'Requires [`torch_ucc`](https://github.com/facebookresearch/torch_ucc)'
test_affine_weight_init_row_parallel_cpu (test_layers.UccTensorParallelLayerTest) ... skipped 'Requires [`torch_ucc`](https://github.com/facebookresearch/torch_ucc)'
test_affine_weight_init_row_parallel_gpu (test_layers.UccTensorParallelLayerTest) ... skipped 'Requires [`torch_ucc`](https://github.com/facebookresearch/torch_ucc)'
test_all_gather_parity (test_layers.UccTensorParallelLayerTest) ... skipped 'Requires [`torch_ucc`](https://github.com/facebookresearch/torch_ucc)'
test_column_parallel_linear (test_layers.UccTensorParallelLayerTest) ... skipped 'Requires [`torch_ucc`](https://github.com/facebookresearch/torch_ucc)'
test_column_parallel_linear_async (test_layers.UccTensorParallelLayerTest) ... skipped 'Requires [`torch_ucc`](https://github.com/facebookresearch/torch_ucc)'
test_column_parallel_linear_exception (test_layers.UccTensorParallelLayerTest) ... skipped 'Requires [`torch_ucc`](https://github.com/facebookresearch/torch_ucc)'
test_column_parallel_linear_gradient_accumulation_fusion (test_layers.UccTensorParallelLayerTest) ... skipped 'Requires [`torch_ucc`](https://github.com/facebookresearch/torch_ucc)'
test_column_parallel_linear_gradient_accumulation_fusion_in_fp16 (test_layers.UccTensorParallelLayerTest) ... skipped 'Requires [`torch_ucc`](https://github.com/facebookresearch/torch_ucc)'
test_column_parallel_linear_sequence_parallel (test_layers.UccTensorParallelLayerTest) ... skipped 'Requires [`torch_ucc`](https://github.com/facebookresearch/torch_ucc)'
test_parallel_embedding (test_layers.UccTensorParallelLayerTest) ... skipped 'Requires [`torch_ucc`](https://github.com/facebookresearch/torch_ucc)'
test_reduce_scatter_parity (test_layers.UccTensorParallelLayerTest) ... skipped 'Requires [`torch_ucc`](https://github.com/facebookresearch/torch_ucc)'
test_row_parallel_linear (test_layers.UccTensorParallelLayerTest) ... skipped 'Requires [`torch_ucc`](https://github.com/facebookresearch/torch_ucc)'
test_row_parallel_linear_gradient_accumulation_fusion (test_layers.UccTensorParallelLayerTest) ... skipped 'Requires [`torch_ucc`](https://github.com/facebookresearch/torch_ucc)'
test_row_parallel_linear_gradient_accumulation_fusion_in_fp16 (test_layers.UccTensorParallelLayerTest) ... skipped 'Requires [`torch_ucc`](https://github.com/facebookresearch/torch_ucc)'
test_row_parallel_linear_sequence_parallel (test_layers.UccTensorParallelLayerTest) ... skipped 'Requires [`torch_ucc`](https://github.com/facebookresearch/torch_ucc)'
test_gather (test_mapping.NcclMappingTest) ... [rank0]:[W603 16:05:13.944825889 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank2]:[W603 16:05:13.947762668 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank1]:[W603 16:05:13.958828241 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank3]:[W603 16:05:14.500687005 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[dist init] rank = 0, world_size = 4
[dist init] rank = 3, world_size = 4
[dist init] rank = 1, world_size = 4
[dist init] rank = 2, world_size = 4
ok
test_reduce (test_mapping.NcclMappingTest) ... [rank1]:[W603 16:05:35.365038307 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank2]:[W603 16:05:35.370537609 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank0]:[W603 16:05:35.371594948 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank3]:[W603 16:05:35.697203236 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[dist init] rank = 0, world_size = 4
[dist init] rank = 3, world_size = 4
[dist init] rank = 1, world_size = 4
[dist init] rank = 2, world_size = 4
ok
test_split (test_mapping.NcclMappingTest) ... [rank1]:[W603 16:05:53.691328007 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank0]:[W603 16:05:53.696602195 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank3]:[W603 16:05:54.608251510 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank2]:[W603 16:05:54.632199522 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[dist init] rank = 0, world_size = 4
[dist init] rank = 3, world_size = 4
[dist init] rank = 1, world_size = 4
[dist init] rank = 2, world_size = 4
ok
test_gather (test_mapping.UccMappingTest) ... skipped 'Requires [`torch_ucc`](https://github.com/facebookresearch/torch_ucc)'
test_reduce (test_mapping.UccMappingTest) ... skipped 'Requires [`torch_ucc`](https://github.com/facebookresearch/torch_ucc)'
test_split (test_mapping.UccMappingTest) ... skipped 'Requires [`torch_ucc`](https://github.com/facebookresearch/torch_ucc)'
test_constant_microbatch_calculator (test_microbatches.NcclMicrobatchCalculatorTest) ... [rank2]:[W603 16:06:08.861642985 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank0]:[W603 16:06:08.862476903 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank1]:[W603 16:06:08.875005176 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank3]:[W603 16:06:08.876428989 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/utils.py:81: UserWarning: This function is only for unittest
  warnings.warn("This function is only for unittest")
[dist init] rank = 0, world_size = 4
[dist init] rank = 3, world_size = 4
[dist init] rank = 2, world_size = 4
[dist init] rank = 1, world_size = 4
ok
test_dynamic_microbatch_calculator (test_microbatches.NcclMicrobatchCalculatorTest) ... [rank2]:[W603 16:06:22.945233453 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank0]:[W603 16:06:22.947858800 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank3]:[W603 16:06:22.949114774 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank1]:[W603 16:06:23.084504080 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/utils.py:81: UserWarning: This function is only for unittest
  warnings.warn("This function is only for unittest")
[dist init] rank = 3, world_size = 4
[dist init] rank = 0, world_size = 4
[dist init] rank = 2, world_size = 4
[dist init] rank = 1, world_size = 4
ok
test_constant_microbatch_calculator (test_microbatches.UccMicrobatchCalculatorTest) ... skipped 'Requires [`torch_ucc`](https://github.com/facebookresearch/torch_ucc)'
test_dynamic_microbatch_calculator (test_microbatches.UccMicrobatchCalculatorTest) ... skipped 'Requires [`torch_ucc`](https://github.com/facebookresearch/torch_ucc)'
test_no_interleaving_warmup (test_p2p_comm.UccP2PCommTest) ... skipped 'Requires [`torch_ucc`](https://github.com/facebookresearch/torch_ucc)'
test_send_backward_recv_backward (test_p2p_comm.UccP2PCommTest) ... skipped 'Requires [`torch_ucc`](https://github.com/facebookresearch/torch_ucc)'
test_send_forward_recv_forward (test_p2p_comm.UccP2PCommTest) ... skipped 'Requires [`torch_ucc`](https://github.com/facebookresearch/torch_ucc)'
test_initialize_model_parallel (test_parallel_state.NcclParallelStateTest) ... [rank1]:[W603 16:06:37.933031514 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank0]:[W603 16:06:37.937152991 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank3]:[W603 16:06:37.938813605 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank2]:[W603 16:06:38.597196668 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[dist init] rank = 3, world_size = 4
[dist init] rank = 0, world_size = 4
[dist init] rank = 1, world_size = 4
[dist init] rank = 2, world_size = 4
ok
test_initialize_model_parallel_decoder_only (test_parallel_state.NcclParallelStateTest)
Initialize model parallelism for decoder-only Transformers like GPT-3 ... [rank0]:[W603 16:06:52.865035520 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank3]:[W603 16:06:52.884775276 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank1]:[W603 16:06:52.886067374 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank2]:[W603 16:06:53.276617324 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[dist init] rank = 0, world_size = 4
[dist init] rank = 3, world_size = 4
[dist init] rank = 1, world_size = 4
[dist init] rank = 2, world_size = 4
ok
test_initialize_model_parallel_with_virtual_and_split (test_parallel_state.NcclParallelStateTest) ... [rank2]:[W603 16:07:06.873261341 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank0]:[W603 16:07:06.883642889 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank1]:[W603 16:07:06.885682606 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank3]:[W603 16:07:07.471477788 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[dist init] rank = 0, world_size = 4
[dist init] rank = 3, world_size = 4
[dist init] rank = 2, world_size = 4
[dist init] rank = 1, world_size = 4
ok
test_initialize_model_parallel (test_parallel_state.UccParallelStateTest) ... skipped 'Requires [`torch_ucc`](https://github.com/facebookresearch/torch_ucc)'
test_initialize_model_parallel_decoder_only (test_parallel_state.UccParallelStateTest)
Initialize model parallelism for decoder-only Transformers like GPT-3 ... skipped 'Requires [`torch_ucc`](https://github.com/facebookresearch/torch_ucc)'
test_initialize_model_parallel_with_virtual_and_split (test_parallel_state.UccParallelStateTest) ... skipped 'Requires [`torch_ucc`](https://github.com/facebookresearch/torch_ucc)'
test_inference_async_pipelining_with_interleaving (test_pipeline_parallel_fwd_bwd.NcclPipelineParallelForwardBackwardTest) ... [rank5]:[W603 16:07:22.584738687 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 5]  using GPU 5 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank1]:[W603 16:07:22.591937221 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank3]:[W603 16:07:22.593373272 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank2]:[W603 16:07:22.027355011 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank0]:[W603 16:07:23.106428187 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank7]:[W603 16:07:23.145665811 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 7]  using GPU 7 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank4]:[W603 16:07:23.146369115 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 4]  using GPU 4 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank6]:[W603 16:07:23.153563473 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 6]  using GPU 6 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[dist init] rank = 4, world_size = 8
 > number of parameters on (tensor, pipeline) model parallel rank (0, 2): 2112
[dist init] rank = 2, world_size = 8
 > number of parameters on (tensor, pipeline) model parallel rank (0, 1): 2112
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/utils.py:81: UserWarning: This function is only for unittest
  warnings.warn("This function is only for unittest")
[dist init] rank = 6, world_size = 8
 > number of parameters on (tensor, pipeline) model parallel rank (0, 3): 2112
[dist init] rank = 0, world_size = 8
 > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 2112
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/common.py:295: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/common.py:295: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/common.py:295: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/common.py:295: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/common.py:295: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/common.py:295: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/common.py:295: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/common.py:295: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(
 > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 2112
 > number of parameters on (tensor, pipeline) model parallel rank (0, 1): 2112
 > number of parameters on (tensor, pipeline) model parallel rank (0, 2): 2112
 > number of parameters on (tensor, pipeline) model parallel rank (0, 3): 2112
 > number of parameters on (tensor, pipeline) model parallel rank (0, 1): 2112
 > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 2112
 > number of parameters on (tensor, pipeline) model parallel rank (0, 2): 2112
 > number of parameters on (tensor, pipeline) model parallel rank (0, 3): 2112
/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py:208: UserWarning: Converting a tensor with requires_grad=True to a scalar may lead to unexpected behavior.
Consider using tensor.detach() first. (Triggered internally at /skishore/github/pytorch/aten/src/ATen/native/Scalar.cpp:22.)
  self.assertEqual(x.item() / microbatch_size, target_loss.item())
 > number of parameters on (tensor, pipeline) model parallel rank (0, 3): 2112
 > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 2112
 > number of parameters on (tensor, pipeline) model parallel rank (0, 2): 2112
/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py:208: UserWarning: Converting a tensor with requires_grad=True to a scalar may lead to unexpected behavior.
Consider using tensor.detach() first. (Triggered internally at /skishore/github/pytorch/aten/src/ATen/native/Scalar.cpp:22.)
  self.assertEqual(x.item() / microbatch_size, target_loss.item())
 > number of parameters on (tensor, pipeline) model parallel rank (0, 1): 2112
/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py:126: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  torch.cuda.amp.GradScaler(init_scale=4.0)
 > number of parameters on (tensor, pipeline) model parallel rank (0, 1): 2112
/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py:126: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  torch.cuda.amp.GradScaler(init_scale=4.0)
/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py:126: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  torch.cuda.amp.GradScaler(init_scale=4.0)
 > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 2112
/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py:126: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  torch.cuda.amp.GradScaler(init_scale=4.0)
 > number of parameters on (tensor, pipeline) model parallel rank (0, 2): 2112
/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py:126: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  torch.cuda.amp.GradScaler(init_scale=4.0)
/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py:126: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  torch.cuda.amp.GradScaler(init_scale=4.0)
/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py:126: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  torch.cuda.amp.GradScaler(init_scale=4.0)
/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py:126: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  torch.cuda.amp.GradScaler(init_scale=4.0)
 > number of parameters on (tensor, pipeline) model parallel rank (0, 3): 2112
 > number of parameters on (tensor, pipeline) model parallel rank (0, 1): 2112
 > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 2112
 > number of parameters on (tensor, pipeline) model parallel rank (0, 2): 2112
 > number of parameters on (tensor, pipeline) model parallel rank (0, 3): 2112
 > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 2112
 > number of parameters on (tensor, pipeline) model parallel rank (0, 1): 2112
 > number of parameters on (tensor, pipeline) model parallel rank (0, 2): 2112
 > number of parameters on (tensor, pipeline) model parallel rank (0, 3): 2112
 > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 2112
 > number of parameters on (tensor, pipeline) model parallel rank (0, 1): 2112
 > number of parameters on (tensor, pipeline) model parallel rank (0, 2): 2112
 > number of parameters on (tensor, pipeline) model parallel rank (0, 3): 2112
[rank6]:E0603 16:08:24.922000 42075 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Caught exception: 
[rank6]:E0603 16:08:24.922000 42075 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Traceback (most recent call last):
[rank6]:E0603 16:08:24.922000 42075 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/testing/_internal/common_distributed.py", line 734, in run_test
[rank6]:E0603 16:08:24.922000 42075 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     getattr(self, test_name)()
[rank6]:E0603 16:08:24.922000 42075 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/testing/_internal/common_distributed.py", line 607, in wrapper
[rank6]:E0603 16:08:24.922000 42075 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     fn()
[rank6]:E0603 16:08:24.922000 42075 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/testing/_internal/common_utils.py", line 3173, in wrapper
[rank6]:E0603 16:08:24.922000 42075 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     method(*args, **kwargs)
[rank6]:E0603 16:08:24.922000 42075 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py", line 278, in test_inference_async_pipelining_with_interleaving
[rank6]:E0603 16:08:24.922000 42075 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     self._forward_backward_test_impl(
[rank6]:E0603 16:08:24.922000 42075 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py", line 180, in _forward_backward_test_impl
[rank6]:E0603 16:08:24.922000 42075 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     loss = fwd_bwd_func(
[rank6]:E0603 16:08:24.922000 42075 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_with_interleaving.py", line 219, in _forward_backward_pipelining_with_interleaving
[rank6]:E0603 16:08:24.922000 42075 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     p2p_communication.recv_forward(
[rank6]:E0603 16:08:24.922000 42075 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 342, in recv_forward
[rank6]:E0603 16:08:24.922000 42075 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     input_tensor, _ = _communicate(
[rank6]:E0603 16:08:24.922000 42075 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 259, in _communicate
[rank6]:E0603 16:08:24.922000 42075 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     tensor_send_prev_req, tensor_recv_prev_req, tensor_send_next_req, tensor_recv_next_req = _run_p2pops(tensor_send_prev, tensor_send_next, tensor_recv_prev, tensor_recv_next, async_comm=async_comm)
[rank6]:E0603 16:08:24.922000 42075 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 97, in _run_p2pops
[rank6]:E0603 16:08:24.922000 42075 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     reqs = torch.distributed.batch_isend_irecv(ops)
[rank6]:E0603 16:08:24.922000 42075 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/distributed/distributed_c10d.py", line 2717, in batch_isend_irecv
[rank6]:E0603 16:08:24.922000 42075 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     p2p_op.op(
[rank6]:E0603 16:08:24.922000 42075 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/distributed/distributed_c10d.py", line 2411, in irecv
[rank6]:E0603 16:08:24.922000 42075 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     return group.recv([tensor], group_src, tag)
[rank6]:E0603 16:08:24.922000 42075 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] torch.distributed.DistBackendError: NCCL error in: /skishore/github/pytorch/torch/csrc/distributed/c10d/NCCLUtils.cpp:77, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.25.1
[rank6]:E0603 16:08:24.922000 42075 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
[rank6]:E0603 16:08:24.922000 42075 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Last error:
[rank6]:E0603 16:08:24.922000 42075 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Error while creating shared memory segment /dev/shm/nccl-qmtFLO (size 8257920), error: No space left on device (28)
[rank6]:E0603 16:08:24.922000 42075 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] 
[rank6]:E0603 16:08:24.922000 42075 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] To execute this test, run the following from the base repo dir:
[rank6]:E0603 16:08:24.922000 42075 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     PYTORCH_TEST_WITH_ROCM=1 python test_pipeline_parallel_fwd_bwd.py NcclPipelineParallelForwardBackwardTest.test_inference_async_pipelining_with_interleaving
[rank6]:E0603 16:08:24.922000 42075 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] 
[rank6]:E0603 16:08:24.922000 42075 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0
[rank6]:E0603 16:08:24.922000 42075 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]  exiting process 6 with exit code: 10
[rank0]:E0603 16:08:24.923000 42069 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Caught exception: 
[rank0]:E0603 16:08:24.923000 42069 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Traceback (most recent call last):
[rank0]:E0603 16:08:24.923000 42069 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/testing/_internal/common_distributed.py", line 734, in run_test
[rank0]:E0603 16:08:24.923000 42069 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     getattr(self, test_name)()
[rank0]:E0603 16:08:24.923000 42069 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/testing/_internal/common_distributed.py", line 607, in wrapper
[rank0]:E0603 16:08:24.923000 42069 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     fn()
[rank0]:E0603 16:08:24.923000 42069 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/testing/_internal/common_utils.py", line 3173, in wrapper
[rank0]:E0603 16:08:24.923000 42069 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     method(*args, **kwargs)
[rank0]:E0603 16:08:24.923000 42069 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py", line 278, in test_inference_async_pipelining_with_interleaving
[rank0]:E0603 16:08:24.923000 42069 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     self._forward_backward_test_impl(
[rank0]:E0603 16:08:24.923000 42069 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py", line 180, in _forward_backward_test_impl
[rank0]:E0603 16:08:24.923000 42069 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     loss = fwd_bwd_func(
[rank0]:E0603 16:08:24.923000 42069 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_with_interleaving.py", line 276, in _forward_backward_pipelining_with_interleaving
[rank0]:E0603 16:08:24.923000 42069 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     input_tensor = p2p_communication.send_forward_recv_forward(
[rank0]:E0603 16:08:24.923000 42069 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 515, in send_forward_recv_forward
[rank0]:E0603 16:08:24.923000 42069 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     input_tensor, _ = _communicate(
[rank0]:E0603 16:08:24.923000 42069 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 259, in _communicate
[rank0]:E0603 16:08:24.923000 42069 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     tensor_send_prev_req, tensor_recv_prev_req, tensor_send_next_req, tensor_recv_next_req = _run_p2pops(tensor_send_prev, tensor_send_next, tensor_recv_prev, tensor_recv_next, async_comm=async_comm)
[rank0]:E0603 16:08:24.923000 42069 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 97, in _run_p2pops
[rank0]:E0603 16:08:24.923000 42069 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     reqs = torch.distributed.batch_isend_irecv(ops)
[rank0]:E0603 16:08:24.923000 42069 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/distributed/distributed_c10d.py", line 2717, in batch_isend_irecv
[rank0]:E0603 16:08:24.923000 42069 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     p2p_op.op(
[rank0]:E0603 16:08:24.923000 42069 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/distributed/distributed_c10d.py", line 2366, in isend
[rank0]:E0603 16:08:24.923000 42069 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     return group.send([tensor], group_dst, tag)
[rank0]:E0603 16:08:24.923000 42069 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] torch.distributed.DistBackendError: NCCL error in: /skishore/github/pytorch/torch/csrc/distributed/c10d/NCCLUtils.cpp:77, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.25.1
[rank0]:E0603 16:08:24.923000 42069 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
[rank0]:E0603 16:08:24.923000 42069 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Last error:
[rank0]:E0603 16:08:24.923000 42069 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Error while creating shared memory segment /dev/shm/nccl-CTs4dF (size 8257920), error: No space left on device (28)
[rank0]:E0603 16:08:24.923000 42069 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] 
[rank0]:E0603 16:08:24.923000 42069 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] To execute this test, run the following from the base repo dir:
[rank0]:E0603 16:08:24.923000 42069 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     PYTORCH_TEST_WITH_ROCM=1 python test_pipeline_parallel_fwd_bwd.py NcclPipelineParallelForwardBackwardTest.test_inference_async_pipelining_with_interleaving
[rank0]:E0603 16:08:24.923000 42069 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] 
[rank0]:E0603 16:08:24.923000 42069 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0
[rank0]:E0603 16:08:24.923000 42069 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]  exiting process 0 with exit code: 10
[rank4]:E0603 16:08:24.924000 42073 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Caught exception: 
[rank4]:E0603 16:08:24.924000 42073 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Traceback (most recent call last):
[rank4]:E0603 16:08:24.924000 42073 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/testing/_internal/common_distributed.py", line 734, in run_test
[rank4]:E0603 16:08:24.924000 42073 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     getattr(self, test_name)()
[rank4]:E0603 16:08:24.924000 42073 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/testing/_internal/common_distributed.py", line 607, in wrapper
[rank4]:E0603 16:08:24.924000 42073 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     fn()
[rank4]:E0603 16:08:24.924000 42073 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/testing/_internal/common_utils.py", line 3173, in wrapper
[rank4]:E0603 16:08:24.924000 42073 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     method(*args, **kwargs)
[rank4]:E0603 16:08:24.924000 42073 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py", line 278, in test_inference_async_pipelining_with_interleaving
[rank4]:E0603 16:08:24.924000 42073 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     self._forward_backward_test_impl(
[rank4]:E0603 16:08:24.924000 42073 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py", line 180, in _forward_backward_test_impl
[rank4]:E0603 16:08:24.924000 42073 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     loss = fwd_bwd_func(
[rank4]:E0603 16:08:24.924000 42073 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_with_interleaving.py", line 219, in _forward_backward_pipelining_with_interleaving
[rank4]:E0603 16:08:24.924000 42073 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     p2p_communication.recv_forward(
[rank4]:E0603 16:08:24.924000 42073 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 342, in recv_forward
[rank4]:E0603 16:08:24.924000 42073 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     input_tensor, _ = _communicate(
[rank4]:E0603 16:08:24.924000 42073 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 259, in _communicate
[rank4]:E0603 16:08:24.924000 42073 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     tensor_send_prev_req, tensor_recv_prev_req, tensor_send_next_req, tensor_recv_next_req = _run_p2pops(tensor_send_prev, tensor_send_next, tensor_recv_prev, tensor_recv_next, async_comm=async_comm)
[rank4]:E0603 16:08:24.924000 42073 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 97, in _run_p2pops
[rank4]:E0603 16:08:24.924000 42073 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     reqs = torch.distributed.batch_isend_irecv(ops)
[rank4]:E0603 16:08:24.924000 42073 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/distributed/distributed_c10d.py", line 2717, in batch_isend_irecv
[rank4]:E0603 16:08:24.924000 42073 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     p2p_op.op(
[rank4]:E0603 16:08:24.924000 42073 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/distributed/distributed_c10d.py", line 2411, in irecv
[rank4]:E0603 16:08:24.924000 42073 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     return group.recv([tensor], group_src, tag)
[rank4]:E0603 16:08:24.924000 42073 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] torch.distributed.DistBackendError: NCCL error in: /skishore/github/pytorch/torch/csrc/distributed/c10d/NCCLUtils.cpp:77, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.25.1
[rank4]:E0603 16:08:24.924000 42073 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
[rank4]:E0603 16:08:24.924000 42073 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Last error:
[rank4]:E0603 16:08:24.924000 42073 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Error while creating shared memory segment /dev/shm/nccl-ckiWx4 (size 8257920), error: No space left on device (28)
[rank4]:E0603 16:08:24.924000 42073 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] 
[rank4]:E0603 16:08:24.924000 42073 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] To execute this test, run the following from the base repo dir:
[rank4]:E0603 16:08:24.924000 42073 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     PYTORCH_TEST_WITH_ROCM=1 python test_pipeline_parallel_fwd_bwd.py NcclPipelineParallelForwardBackwardTest.test_inference_async_pipelining_with_interleaving
[rank4]:E0603 16:08:24.924000 42073 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] 
[rank4]:E0603 16:08:24.924000 42073 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0
[rank4]:E0603 16:08:24.924000 42073 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]  exiting process 4 with exit code: 10
[rank2]:E0603 16:08:24.924000 42071 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Caught exception: 
[rank2]:E0603 16:08:24.924000 42071 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Traceback (most recent call last):
[rank2]:E0603 16:08:24.924000 42071 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/testing/_internal/common_distributed.py", line 734, in run_test
[rank2]:E0603 16:08:24.924000 42071 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     getattr(self, test_name)()
[rank2]:E0603 16:08:24.924000 42071 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/testing/_internal/common_distributed.py", line 607, in wrapper
[rank2]:E0603 16:08:24.924000 42071 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     fn()
[rank2]:E0603 16:08:24.924000 42071 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/testing/_internal/common_utils.py", line 3173, in wrapper
[rank2]:E0603 16:08:24.924000 42071 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     method(*args, **kwargs)
[rank2]:E0603 16:08:24.924000 42071 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py", line 278, in test_inference_async_pipelining_with_interleaving
[rank2]:E0603 16:08:24.924000 42071 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     self._forward_backward_test_impl(
[rank2]:E0603 16:08:24.924000 42071 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py", line 180, in _forward_backward_test_impl
[rank2]:E0603 16:08:24.924000 42071 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     loss = fwd_bwd_func(
[rank2]:E0603 16:08:24.924000 42071 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_with_interleaving.py", line 219, in _forward_backward_pipelining_with_interleaving
[rank2]:E0603 16:08:24.924000 42071 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     p2p_communication.recv_forward(
[rank2]:E0603 16:08:24.924000 42071 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 342, in recv_forward
[rank2]:E0603 16:08:24.924000 42071 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     input_tensor, _ = _communicate(
[rank2]:E0603 16:08:24.924000 42071 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 259, in _communicate
[rank2]:E0603 16:08:24.924000 42071 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     tensor_send_prev_req, tensor_recv_prev_req, tensor_send_next_req, tensor_recv_next_req = _run_p2pops(tensor_send_prev, tensor_send_next, tensor_recv_prev, tensor_recv_next, async_comm=async_comm)
[rank2]:E0603 16:08:24.924000 42071 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 97, in _run_p2pops
[rank2]:E0603 16:08:24.924000 42071 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     reqs = torch.distributed.batch_isend_irecv(ops)
[rank2]:E0603 16:08:24.924000 42071 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/distributed/distributed_c10d.py", line 2717, in batch_isend_irecv
[rank2]:E0603 16:08:24.924000 42071 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     p2p_op.op(
[rank2]:E0603 16:08:24.924000 42071 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/distributed/distributed_c10d.py", line 2411, in irecv
[rank2]:E0603 16:08:24.924000 42071 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     return group.recv([tensor], group_src, tag)
[rank2]:E0603 16:08:24.924000 42071 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] torch.distributed.DistBackendError: NCCL error in: /skishore/github/pytorch/torch/csrc/distributed/c10d/NCCLUtils.cpp:77, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.25.1
[rank2]:E0603 16:08:24.924000 42071 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
[rank2]:E0603 16:08:24.924000 42071 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Last error:
[rank2]:E0603 16:08:24.924000 42071 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Error while creating shared memory segment /dev/shm/nccl-hsiffW (size 8257920), error: No space left on device (28)
[rank2]:E0603 16:08:24.924000 42071 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] 
[rank2]:E0603 16:08:24.924000 42071 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] To execute this test, run the following from the base repo dir:
[rank2]:E0603 16:08:24.924000 42071 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     PYTORCH_TEST_WITH_ROCM=1 python test_pipeline_parallel_fwd_bwd.py NcclPipelineParallelForwardBackwardTest.test_inference_async_pipelining_with_interleaving
[rank2]:E0603 16:08:24.924000 42071 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] 
[rank2]:E0603 16:08:24.924000 42071 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0
[rank2]:E0603 16:08:24.924000 42071 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]  exiting process 2 with exit code: 10
[rank3]:E0603 16:08:24.944000 42072 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Caught exception: 
[rank3]:E0603 16:08:24.944000 42072 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Traceback (most recent call last):
[rank3]:E0603 16:08:24.944000 42072 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/testing/_internal/common_distributed.py", line 734, in run_test
[rank3]:E0603 16:08:24.944000 42072 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     getattr(self, test_name)()
[rank3]:E0603 16:08:24.944000 42072 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/testing/_internal/common_distributed.py", line 607, in wrapper
[rank3]:E0603 16:08:24.944000 42072 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     fn()
[rank3]:E0603 16:08:24.944000 42072 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/testing/_internal/common_utils.py", line 3173, in wrapper
[rank3]:E0603 16:08:24.944000 42072 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     method(*args, **kwargs)
[rank3]:E0603 16:08:24.944000 42072 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py", line 278, in test_inference_async_pipelining_with_interleaving
[rank3]:E0603 16:08:24.944000 42072 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     self._forward_backward_test_impl(
[rank3]:E0603 16:08:24.944000 42072 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py", line 180, in _forward_backward_test_impl
[rank3]:E0603 16:08:24.944000 42072 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     loss = fwd_bwd_func(
[rank3]:E0603 16:08:24.944000 42072 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_with_interleaving.py", line 219, in _forward_backward_pipelining_with_interleaving
[rank3]:E0603 16:08:24.944000 42072 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     p2p_communication.recv_forward(
[rank3]:E0603 16:08:24.944000 42072 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 342, in recv_forward
[rank3]:E0603 16:08:24.944000 42072 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     input_tensor, _ = _communicate(
[rank3]:E0603 16:08:24.944000 42072 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 259, in _communicate
[rank3]:E0603 16:08:24.944000 42072 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     tensor_send_prev_req, tensor_recv_prev_req, tensor_send_next_req, tensor_recv_next_req = _run_p2pops(tensor_send_prev, tensor_send_next, tensor_recv_prev, tensor_recv_next, async_comm=async_comm)
[rank3]:E0603 16:08:24.944000 42072 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 97, in _run_p2pops
[rank3]:E0603 16:08:24.944000 42072 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     reqs = torch.distributed.batch_isend_irecv(ops)
[rank3]:E0603 16:08:24.944000 42072 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/distributed/distributed_c10d.py", line 2717, in batch_isend_irecv
[rank3]:E0603 16:08:24.944000 42072 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     p2p_op.op(
[rank3]:E0603 16:08:24.944000 42072 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/distributed/distributed_c10d.py", line 2411, in irecv
[rank3]:E0603 16:08:24.944000 42072 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     return group.recv([tensor], group_src, tag)
[rank3]:E0603 16:08:24.944000 42072 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] torch.distributed.DistBackendError: NCCL error in: /skishore/github/pytorch/torch/csrc/distributed/c10d/NCCLUtils.cpp:77, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.25.1
[rank3]:E0603 16:08:24.944000 42072 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
[rank3]:E0603 16:08:24.944000 42072 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Last error:
[rank3]:E0603 16:08:24.944000 42072 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Error while creating shared memory segment /dev/shm/nccl-BZatXC (size 8257920), error: No space left on device (28)
[rank3]:E0603 16:08:24.944000 42072 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] 
[rank3]:E0603 16:08:24.944000 42072 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] To execute this test, run the following from the base repo dir:
[rank3]:E0603 16:08:24.944000 42072 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     PYTORCH_TEST_WITH_ROCM=1 python test_pipeline_parallel_fwd_bwd.py NcclPipelineParallelForwardBackwardTest.test_inference_async_pipelining_with_interleaving
[rank3]:E0603 16:08:24.944000 42072 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] 
[rank3]:E0603 16:08:24.944000 42072 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0
[rank3]:E0603 16:08:24.944000 42072 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]  exiting process 3 with exit code: 10
[rank5]:E0603 16:08:24.944000 42074 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Caught exception: 
[rank5]:E0603 16:08:24.944000 42074 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Traceback (most recent call last):
[rank5]:E0603 16:08:24.944000 42074 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/testing/_internal/common_distributed.py", line 734, in run_test
[rank5]:E0603 16:08:24.944000 42074 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     getattr(self, test_name)()
[rank5]:E0603 16:08:24.944000 42074 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/testing/_internal/common_distributed.py", line 607, in wrapper
[rank5]:E0603 16:08:24.944000 42074 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     fn()
[rank5]:E0603 16:08:24.944000 42074 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/testing/_internal/common_utils.py", line 3173, in wrapper
[rank5]:E0603 16:08:24.944000 42074 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     method(*args, **kwargs)
[rank5]:E0603 16:08:24.944000 42074 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py", line 278, in test_inference_async_pipelining_with_interleaving
[rank5]:E0603 16:08:24.944000 42074 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     self._forward_backward_test_impl(
[rank5]:E0603 16:08:24.944000 42074 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py", line 180, in _forward_backward_test_impl
[rank5]:E0603 16:08:24.944000 42074 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     loss = fwd_bwd_func(
[rank5]:E0603 16:08:24.944000 42074 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_with_interleaving.py", line 219, in _forward_backward_pipelining_with_interleaving
[rank5]:E0603 16:08:24.944000 42074 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     p2p_communication.recv_forward(
[rank5]:E0603 16:08:24.944000 42074 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 342, in recv_forward
[rank5]:E0603 16:08:24.944000 42074 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     input_tensor, _ = _communicate(
[rank5]:E0603 16:08:24.944000 42074 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 259, in _communicate
[rank5]:E0603 16:08:24.944000 42074 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     tensor_send_prev_req, tensor_recv_prev_req, tensor_send_next_req, tensor_recv_next_req = _run_p2pops(tensor_send_prev, tensor_send_next, tensor_recv_prev, tensor_recv_next, async_comm=async_comm)
[rank5]:E0603 16:08:24.944000 42074 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 97, in _run_p2pops
[rank5]:E0603 16:08:24.944000 42074 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     reqs = torch.distributed.batch_isend_irecv(ops)
[rank5]:E0603 16:08:24.944000 42074 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/distributed/distributed_c10d.py", line 2717, in batch_isend_irecv
[rank5]:E0603 16:08:24.944000 42074 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     p2p_op.op(
[rank5]:E0603 16:08:24.944000 42074 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/distributed/distributed_c10d.py", line 2411, in irecv
[rank5]:E0603 16:08:24.944000 42074 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     return group.recv([tensor], group_src, tag)
[rank5]:E0603 16:08:24.944000 42074 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] torch.distributed.DistBackendError: NCCL error in: /skishore/github/pytorch/torch/csrc/distributed/c10d/NCCLUtils.cpp:77, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.25.1
[rank5]:E0603 16:08:24.944000 42074 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
[rank5]:E0603 16:08:24.944000 42074 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Last error:
[rank5]:E0603 16:08:24.944000 42074 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Error while creating shared memory segment /dev/shm/nccl-JfUoFZ (size 8257920), error: No space left on device (28)
[rank5]:E0603 16:08:24.944000 42074 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] 
[rank5]:E0603 16:08:24.944000 42074 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] To execute this test, run the following from the base repo dir:
[rank5]:E0603 16:08:24.944000 42074 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     PYTORCH_TEST_WITH_ROCM=1 python test_pipeline_parallel_fwd_bwd.py NcclPipelineParallelForwardBackwardTest.test_inference_async_pipelining_with_interleaving
[rank5]:E0603 16:08:24.944000 42074 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] 
[rank5]:E0603 16:08:24.944000 42074 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0
[rank5]:E0603 16:08:24.944000 42074 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]  exiting process 5 with exit code: 10
[rank7]:E0603 16:08:24.944000 42076 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Caught exception: 
[rank7]:E0603 16:08:24.944000 42076 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Traceback (most recent call last):
[rank7]:E0603 16:08:24.944000 42076 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/testing/_internal/common_distributed.py", line 734, in run_test
[rank7]:E0603 16:08:24.944000 42076 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     getattr(self, test_name)()
[rank7]:E0603 16:08:24.944000 42076 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/testing/_internal/common_distributed.py", line 607, in wrapper
[rank7]:E0603 16:08:24.944000 42076 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     fn()
[rank7]:E0603 16:08:24.944000 42076 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/testing/_internal/common_utils.py", line 3173, in wrapper
[rank7]:E0603 16:08:24.944000 42076 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     method(*args, **kwargs)
[rank7]:E0603 16:08:24.944000 42076 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py", line 278, in test_inference_async_pipelining_with_interleaving
[rank7]:E0603 16:08:24.944000 42076 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     self._forward_backward_test_impl(
[rank7]:E0603 16:08:24.944000 42076 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py", line 180, in _forward_backward_test_impl
[rank7]:E0603 16:08:24.944000 42076 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     loss = fwd_bwd_func(
[rank7]:E0603 16:08:24.944000 42076 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_with_interleaving.py", line 219, in _forward_backward_pipelining_with_interleaving
[rank7]:E0603 16:08:24.944000 42076 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     p2p_communication.recv_forward(
[rank7]:E0603 16:08:24.944000 42076 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 342, in recv_forward
[rank7]:E0603 16:08:24.944000 42076 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     input_tensor, _ = _communicate(
[rank7]:E0603 16:08:24.944000 42076 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 259, in _communicate
[rank7]:E0603 16:08:24.944000 42076 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     tensor_send_prev_req, tensor_recv_prev_req, tensor_send_next_req, tensor_recv_next_req = _run_p2pops(tensor_send_prev, tensor_send_next, tensor_recv_prev, tensor_recv_next, async_comm=async_comm)
[rank7]:E0603 16:08:24.944000 42076 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 97, in _run_p2pops
[rank7]:E0603 16:08:24.944000 42076 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     reqs = torch.distributed.batch_isend_irecv(ops)
[rank7]:E0603 16:08:24.944000 42076 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/distributed/distributed_c10d.py", line 2717, in batch_isend_irecv
[rank7]:E0603 16:08:24.944000 42076 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     p2p_op.op(
[rank7]:E0603 16:08:24.944000 42076 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/distributed/distributed_c10d.py", line 2411, in irecv
[rank7]:E0603 16:08:24.944000 42076 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     return group.recv([tensor], group_src, tag)
[rank7]:E0603 16:08:24.944000 42076 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] torch.distributed.DistBackendError: NCCL error in: /skishore/github/pytorch/torch/csrc/distributed/c10d/NCCLUtils.cpp:77, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.25.1
[rank7]:E0603 16:08:24.944000 42076 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
[rank7]:E0603 16:08:24.944000 42076 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Last error:
[rank7]:E0603 16:08:24.944000 42076 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Error while creating shared memory segment /dev/shm/nccl-LdJXc3 (size 8257920), error: No space left on device (28)
[rank7]:E0603 16:08:24.944000 42076 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] 
[rank7]:E0603 16:08:24.944000 42076 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] To execute this test, run the following from the base repo dir:
[rank7]:E0603 16:08:24.944000 42076 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     PYTORCH_TEST_WITH_ROCM=1 python test_pipeline_parallel_fwd_bwd.py NcclPipelineParallelForwardBackwardTest.test_inference_async_pipelining_with_interleaving
[rank7]:E0603 16:08:24.944000 42076 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] 
[rank7]:E0603 16:08:24.944000 42076 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0
[rank7]:E0603 16:08:24.944000 42076 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]  exiting process 7 with exit code: 10
[rank1]:E0603 16:08:24.944000 42070 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Caught exception: 
[rank1]:E0603 16:08:24.944000 42070 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Traceback (most recent call last):
[rank1]:E0603 16:08:24.944000 42070 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/testing/_internal/common_distributed.py", line 734, in run_test
[rank1]:E0603 16:08:24.944000 42070 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     getattr(self, test_name)()
[rank1]:E0603 16:08:24.944000 42070 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/testing/_internal/common_distributed.py", line 607, in wrapper
[rank1]:E0603 16:08:24.944000 42070 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     fn()
[rank1]:E0603 16:08:24.944000 42070 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/testing/_internal/common_utils.py", line 3173, in wrapper
[rank1]:E0603 16:08:24.944000 42070 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     method(*args, **kwargs)
[rank1]:E0603 16:08:24.944000 42070 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py", line 278, in test_inference_async_pipelining_with_interleaving
[rank1]:E0603 16:08:24.944000 42070 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     self._forward_backward_test_impl(
[rank1]:E0603 16:08:24.944000 42070 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py", line 180, in _forward_backward_test_impl
[rank1]:E0603 16:08:24.944000 42070 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     loss = fwd_bwd_func(
[rank1]:E0603 16:08:24.944000 42070 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_with_interleaving.py", line 276, in _forward_backward_pipelining_with_interleaving
[rank1]:E0603 16:08:24.944000 42070 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     input_tensor = p2p_communication.send_forward_recv_forward(
[rank1]:E0603 16:08:24.944000 42070 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 515, in send_forward_recv_forward
[rank1]:E0603 16:08:24.944000 42070 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     input_tensor, _ = _communicate(
[rank1]:E0603 16:08:24.944000 42070 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 259, in _communicate
[rank1]:E0603 16:08:24.944000 42070 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     tensor_send_prev_req, tensor_recv_prev_req, tensor_send_next_req, tensor_recv_next_req = _run_p2pops(tensor_send_prev, tensor_send_next, tensor_recv_prev, tensor_recv_next, async_comm=async_comm)
[rank1]:E0603 16:08:24.944000 42070 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 97, in _run_p2pops
[rank1]:E0603 16:08:24.944000 42070 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     reqs = torch.distributed.batch_isend_irecv(ops)
[rank1]:E0603 16:08:24.944000 42070 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/distributed/distributed_c10d.py", line 2717, in batch_isend_irecv
[rank1]:E0603 16:08:24.944000 42070 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     p2p_op.op(
[rank1]:E0603 16:08:24.944000 42070 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/distributed/distributed_c10d.py", line 2366, in isend
[rank1]:E0603 16:08:24.944000 42070 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     return group.send([tensor], group_dst, tag)
[rank1]:E0603 16:08:24.944000 42070 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] torch.distributed.DistBackendError: NCCL error in: /skishore/github/pytorch/torch/csrc/distributed/c10d/NCCLUtils.cpp:77, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.25.1
[rank1]:E0603 16:08:24.944000 42070 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
[rank1]:E0603 16:08:24.944000 42070 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Last error:
[rank1]:E0603 16:08:24.944000 42070 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Error while creating shared memory segment /dev/shm/nccl-DcKIWl (size 8257920), error: No space left on device (28)
[rank1]:E0603 16:08:24.944000 42070 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] 
[rank1]:E0603 16:08:24.944000 42070 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] To execute this test, run the following from the base repo dir:
[rank1]:E0603 16:08:24.944000 42070 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     PYTORCH_TEST_WITH_ROCM=1 python test_pipeline_parallel_fwd_bwd.py NcclPipelineParallelForwardBackwardTest.test_inference_async_pipelining_with_interleaving
[rank1]:E0603 16:08:24.944000 42070 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] 
[rank1]:E0603 16:08:24.944000 42070 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0
[rank1]:E0603 16:08:24.944000 42070 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]  exiting process 1 with exit code: 10
[dist init] rank = 3, world_size = 8
[dist init] rank = 5, world_size = 8
[dist init] rank = 1, world_size = 8
[dist init] rank = 7, world_size = 8
[rank0]:[W603 16:08:25.303926414 ProcessGroupNCCL.cpp:1553] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank4]:[W603 16:08:25.362910114 ProcessGroupNCCL.cpp:1553] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank6]:[W603 16:08:25.383088872 ProcessGroupNCCL.cpp:1553] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank2]:[W603 16:08:25.384296404 ProcessGroupNCCL.cpp:1553] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank3]:[W603 16:08:25.847527234 ProcessGroupNCCL.cpp:1553] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank5]:[W603 16:08:25.861409137 ProcessGroupNCCL.cpp:1553] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank7]:[W603 16:08:25.913980770 ProcessGroupNCCL.cpp:1553] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank1]:[W603 16:08:25.010703691 ProcessGroupNCCL.cpp:1553] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
ERROR
test_inference_async_pipelining_without_interleaving (test_pipeline_parallel_fwd_bwd.NcclPipelineParallelForwardBackwardTest) ... Process 3 terminated with exit code 10, terminating remaining processes.
[rank5]:[W603 16:08:47.287850779 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 5]  using GPU 5 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank3]:[W603 16:08:47.298819678 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank6]:[W603 16:08:47.301756687 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 6]  using GPU 6 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank0]:[W603 16:08:47.427623361 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank2]:[W603 16:08:47.498550246 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank4]:[W603 16:08:47.542999745 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 4]  using GPU 4 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank7]:[W603 16:08:47.836790322 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 7]  using GPU 7 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank1]:[W603 16:08:47.847676459 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[dist init] rank = 6, world_size = 8
 > number of parameters on (tensor, pipeline) model parallel rank (0, 3): 1056
[dist init] rank = 4, world_size = 8
 > number of parameters on (tensor, pipeline) model parallel rank (0, 2): 1056
[dist init] rank = 2, world_size = 8
 > number of parameters on (tensor, pipeline) model parallel rank (0, 1): 1056
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/utils.py:81: UserWarning: This function is only for unittest
  warnings.warn("This function is only for unittest")
[dist init] rank = 0, world_size = 8
 > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 1056
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/common.py:295: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/common.py:295: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/common.py:295: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/common.py:295: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(
 > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 1056
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/common.py:295: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/common.py:295: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(
 > number of parameters on (tensor, pipeline) model parallel rank (0, 1): 1056
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/common.py:295: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/common.py:295: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(
 > number of parameters on (tensor, pipeline) model parallel rank (0, 2): 1056
 > number of parameters on (tensor, pipeline) model parallel rank (0, 3): 1056
 > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 1056
 > number of parameters on (tensor, pipeline) model parallel rank (0, 1): 1056
 > number of parameters on (tensor, pipeline) model parallel rank (0, 2): 1056
 > number of parameters on (tensor, pipeline) model parallel rank (0, 3): 1056
 > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 1056
 > number of parameters on (tensor, pipeline) model parallel rank (0, 1): 1056
 > number of parameters on (tensor, pipeline) model parallel rank (0, 2): 1056
/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py:208: UserWarning: Converting a tensor with requires_grad=True to a scalar may lead to unexpected behavior.
Consider using tensor.detach() first. (Triggered internally at /skishore/github/pytorch/aten/src/ATen/native/Scalar.cpp:22.)
  self.assertEqual(x.item() / microbatch_size, target_loss.item())
/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py:208: UserWarning: Converting a tensor with requires_grad=True to a scalar may lead to unexpected behavior.
Consider using tensor.detach() first. (Triggered internally at /skishore/github/pytorch/aten/src/ATen/native/Scalar.cpp:22.)
  self.assertEqual(x.item() / microbatch_size, target_loss.item())
 > number of parameters on (tensor, pipeline) model parallel rank (0, 3): 1056
/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py:126: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  torch.cuda.amp.GradScaler(init_scale=4.0)
 > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 1056
/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py:126: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  torch.cuda.amp.GradScaler(init_scale=4.0)
/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py:126: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  torch.cuda.amp.GradScaler(init_scale=4.0)
/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py:126: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  torch.cuda.amp.GradScaler(init_scale=4.0)
 > number of parameters on (tensor, pipeline) model parallel rank (0, 2): 1056
/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py:126: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  torch.cuda.amp.GradScaler(init_scale=4.0)
/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py:126: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  torch.cuda.amp.GradScaler(init_scale=4.0)
 > number of parameters on (tensor, pipeline) model parallel rank (0, 3): 1056
/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py:126: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  torch.cuda.amp.GradScaler(init_scale=4.0)
/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py:126: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  torch.cuda.amp.GradScaler(init_scale=4.0)
 > number of parameters on (tensor, pipeline) model parallel rank (0, 1): 1056
 > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 1056
 > number of parameters on (tensor, pipeline) model parallel rank (0, 1): 1056
 > number of parameters on (tensor, pipeline) model parallel rank (0, 2): 1056
 > number of parameters on (tensor, pipeline) model parallel rank (0, 3): 1056
 > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 1056
 > number of parameters on (tensor, pipeline) model parallel rank (0, 1): 1056
 > number of parameters on (tensor, pipeline) model parallel rank (0, 2): 1056
 > number of parameters on (tensor, pipeline) model parallel rank (0, 3): 1056
 > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 1056
 > number of parameters on (tensor, pipeline) model parallel rank (0, 1): 1056
 > number of parameters on (tensor, pipeline) model parallel rank (0, 2): 1056
 > number of parameters on (tensor, pipeline) model parallel rank (0, 3): 1056
[rank2]:E0603 16:09:36.535000 46104 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Caught exception: 
[rank2]:E0603 16:09:36.535000 46104 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Traceback (most recent call last):
[rank2]:E0603 16:09:36.535000 46104 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/testing/_internal/common_distributed.py", line 734, in run_test
[rank2]:E0603 16:09:36.535000 46104 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     getattr(self, test_name)()
[rank2]:E0603 16:09:36.535000 46104 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/testing/_internal/common_distributed.py", line 607, in wrapper
[rank2]:E0603 16:09:36.535000 46104 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     fn()
[rank2]:E0603 16:09:36.535000 46104 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/testing/_internal/common_utils.py", line 3173, in wrapper
[rank2]:E0603 16:09:36.535000 46104 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     method(*args, **kwargs)
[rank2]:E0603 16:09:36.535000 46104 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py", line 254, in test_inference_async_pipelining_without_interleaving
[rank2]:E0603 16:09:36.535000 46104 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     self._forward_backward_test_impl(
[rank2]:E0603 16:09:36.535000 46104 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py", line 180, in _forward_backward_test_impl
[rank2]:E0603 16:09:36.535000 46104 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     loss = fwd_bwd_func(
[rank2]:E0603 16:09:36.535000 46104 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_without_interleaving.py", line 332, in forward_backward_pipelining_without_interleaving
[rank2]:E0603 16:09:36.535000 46104 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     input_tensor = recv_forward(
[rank2]:E0603 16:09:36.535000 46104 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_without_interleaving.py", line 99, in recv_forward
[rank2]:E0603 16:09:36.535000 46104 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     p2p_communication.recv_forward(
[rank2]:E0603 16:09:36.535000 46104 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 342, in recv_forward
[rank2]:E0603 16:09:36.535000 46104 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     input_tensor, _ = _communicate(
[rank2]:E0603 16:09:36.535000 46104 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 259, in _communicate
[rank2]:E0603 16:09:36.535000 46104 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     tensor_send_prev_req, tensor_recv_prev_req, tensor_send_next_req, tensor_recv_next_req = _run_p2pops(tensor_send_prev, tensor_send_next, tensor_recv_prev, tensor_recv_next, async_comm=async_comm)
[rank2]:E0603 16:09:36.535000 46104 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 97, in _run_p2pops
[rank2]:E0603 16:09:36.535000 46104 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     reqs = torch.distributed.batch_isend_irecv(ops)
[rank2]:E0603 16:09:36.535000 46104 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/distributed/distributed_c10d.py", line 2717, in batch_isend_irecv
[rank2]:E0603 16:09:36.535000 46104 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     p2p_op.op(
[rank2]:E0603 16:09:36.535000 46104 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/distributed/distributed_c10d.py", line 2411, in irecv
[rank2]:E0603 16:09:36.535000 46104 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     return group.recv([tensor], group_src, tag)
[rank2]:E0603 16:09:36.535000 46104 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] torch.distributed.DistBackendError: NCCL error in: /skishore/github/pytorch/torch/csrc/distributed/c10d/NCCLUtils.cpp:77, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.25.1
[rank2]:E0603 16:09:36.535000 46104 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
[rank2]:E0603 16:09:36.535000 46104 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Last error:
[rank2]:E0603 16:09:36.535000 46104 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Error while creating shared memory segment /dev/shm/nccl-fcsADG (size 8257920), error: No space left on device (28)
[rank2]:E0603 16:09:36.535000 46104 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] 
[rank2]:E0603 16:09:36.535000 46104 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] To execute this test, run the following from the base repo dir:
[rank2]:E0603 16:09:36.535000 46104 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     PYTORCH_TEST_WITH_ROCM=1 python test_pipeline_parallel_fwd_bwd.py NcclPipelineParallelForwardBackwardTest.test_inference_async_pipelining_without_interleaving
[rank2]:E0603 16:09:36.535000 46104 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] 
[rank2]:E0603 16:09:36.535000 46104 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0
[rank2]:E0603 16:09:36.535000 46104 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]  exiting process 2 with exit code: 10
[rank0]:E0603 16:09:36.536000 46102 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Caught exception: 
[rank0]:E0603 16:09:36.536000 46102 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Traceback (most recent call last):
[rank0]:E0603 16:09:36.536000 46102 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/testing/_internal/common_distributed.py", line 734, in run_test
[rank0]:E0603 16:09:36.536000 46102 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     getattr(self, test_name)()
[rank0]:E0603 16:09:36.536000 46102 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/testing/_internal/common_distributed.py", line 607, in wrapper
[rank0]:E0603 16:09:36.536000 46102 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     fn()
[rank0]:E0603 16:09:36.536000 46102 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/testing/_internal/common_utils.py", line 3173, in wrapper
[rank0]:E0603 16:09:36.536000 46102 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     method(*args, **kwargs)
[rank0]:E0603 16:09:36.536000 46102 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py", line 254, in test_inference_async_pipelining_without_interleaving
[rank0]:E0603 16:09:36.536000 46102 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     self._forward_backward_test_impl(
[rank0]:E0603 16:09:36.536000 46102 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py", line 180, in _forward_backward_test_impl
[rank0]:E0603 16:09:36.536000 46102 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     loss = fwd_bwd_func(
[rank0]:E0603 16:09:36.536000 46102 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_without_interleaving.py", line 349, in forward_backward_pipelining_without_interleaving
[rank0]:E0603 16:09:36.536000 46102 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     send_forward(
[rank0]:E0603 16:09:36.536000 46102 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_without_interleaving.py", line 145, in send_forward
[rank0]:E0603 16:09:36.536000 46102 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     p2p_communication.send_forward(
[rank0]:E0603 16:09:36.536000 46102 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 401, in send_forward
[rank0]:E0603 16:09:36.536000 46102 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     _communicate(
[rank0]:E0603 16:09:36.536000 46102 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 259, in _communicate
[rank0]:E0603 16:09:36.536000 46102 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     tensor_send_prev_req, tensor_recv_prev_req, tensor_send_next_req, tensor_recv_next_req = _run_p2pops(tensor_send_prev, tensor_send_next, tensor_recv_prev, tensor_recv_next, async_comm=async_comm)
[rank0]:E0603 16:09:36.536000 46102 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 97, in _run_p2pops
[rank0]:E0603 16:09:36.536000 46102 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     reqs = torch.distributed.batch_isend_irecv(ops)
[rank0]:E0603 16:09:36.536000 46102 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/distributed/distributed_c10d.py", line 2717, in batch_isend_irecv
[rank0]:E0603 16:09:36.536000 46102 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     p2p_op.op(
[rank0]:E0603 16:09:36.536000 46102 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/distributed/distributed_c10d.py", line 2366, in isend
[rank0]:E0603 16:09:36.536000 46102 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     return group.send([tensor], group_dst, tag)
[rank0]:E0603 16:09:36.536000 46102 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] torch.distributed.DistBackendError: NCCL error in: /skishore/github/pytorch/torch/csrc/distributed/c10d/NCCLUtils.cpp:77, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.25.1
[rank0]:E0603 16:09:36.536000 46102 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
[rank0]:E0603 16:09:36.536000 46102 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Last error:
[rank0]:E0603 16:09:36.536000 46102 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Error while creating shared memory segment /dev/shm/nccl-JQj2bD (size 8257920), error: No space left on device (28)
[rank0]:E0603 16:09:36.536000 46102 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] 
[rank0]:E0603 16:09:36.536000 46102 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] To execute this test, run the following from the base repo dir:
[rank0]:E0603 16:09:36.536000 46102 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     PYTORCH_TEST_WITH_ROCM=1 python test_pipeline_parallel_fwd_bwd.py NcclPipelineParallelForwardBackwardTest.test_inference_async_pipelining_without_interleaving
[rank0]:E0603 16:09:36.536000 46102 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] 
[rank0]:E0603 16:09:36.536000 46102 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0
[rank0]:E0603 16:09:36.536000 46102 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]  exiting process 0 with exit code: 10
[rank6]:E0603 16:09:36.536000 46108 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Caught exception: 
[rank6]:E0603 16:09:36.536000 46108 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Traceback (most recent call last):
[rank6]:E0603 16:09:36.536000 46108 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/testing/_internal/common_distributed.py", line 734, in run_test
[rank6]:E0603 16:09:36.536000 46108 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     getattr(self, test_name)()
[rank6]:E0603 16:09:36.536000 46108 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/testing/_internal/common_distributed.py", line 607, in wrapper
[rank6]:E0603 16:09:36.536000 46108 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     fn()
[rank6]:E0603 16:09:36.536000 46108 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/testing/_internal/common_utils.py", line 3173, in wrapper
[rank6]:E0603 16:09:36.536000 46108 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     method(*args, **kwargs)
[rank6]:E0603 16:09:36.536000 46108 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py", line 254, in test_inference_async_pipelining_without_interleaving
[rank6]:E0603 16:09:36.536000 46108 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     self._forward_backward_test_impl(
[rank6]:E0603 16:09:36.536000 46108 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py", line 180, in _forward_backward_test_impl
[rank6]:E0603 16:09:36.536000 46108 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     loss = fwd_bwd_func(
[rank6]:E0603 16:09:36.536000 46108 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_without_interleaving.py", line 367, in forward_backward_pipelining_without_interleaving
[rank6]:E0603 16:09:36.536000 46108 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     input_tensor: List[Union[None, torch.Tensor, FutureTensor]] = recv_forward(tensor_shapes=recv_tensor_shapes, dtype=dtype, async_comm=async_comm)
[rank6]:E0603 16:09:36.536000 46108 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_without_interleaving.py", line 99, in recv_forward
[rank6]:E0603 16:09:36.536000 46108 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     p2p_communication.recv_forward(
[rank6]:E0603 16:09:36.536000 46108 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 342, in recv_forward
[rank6]:E0603 16:09:36.536000 46108 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     input_tensor, _ = _communicate(
[rank6]:E0603 16:09:36.536000 46108 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 259, in _communicate
[rank6]:E0603 16:09:36.536000 46108 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     tensor_send_prev_req, tensor_recv_prev_req, tensor_send_next_req, tensor_recv_next_req = _run_p2pops(tensor_send_prev, tensor_send_next, tensor_recv_prev, tensor_recv_next, async_comm=async_comm)
[rank6]:E0603 16:09:36.536000 46108 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 97, in _run_p2pops
[rank6]:E0603 16:09:36.536000 46108 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     reqs = torch.distributed.batch_isend_irecv(ops)
[rank6]:E0603 16:09:36.536000 46108 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/distributed/distributed_c10d.py", line 2717, in batch_isend_irecv
[rank6]:E0603 16:09:36.536000 46108 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     p2p_op.op(
[rank6]:E0603 16:09:36.536000 46108 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/distributed/distributed_c10d.py", line 2411, in irecv
[rank6]:E0603 16:09:36.536000 46108 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     return group.recv([tensor], group_src, tag)
[rank6]:E0603 16:09:36.536000 46108 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] torch.distributed.DistBackendError: NCCL error in: /skishore/github/pytorch/torch/csrc/distributed/c10d/NCCLUtils.cpp:77, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.25.1
[rank6]:E0603 16:09:36.536000 46108 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
[rank6]:E0603 16:09:36.536000 46108 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Last error:
[rank6]:E0603 16:09:36.536000 46108 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Error while creating shared memory segment /dev/shm/nccl-jXAXk8 (size 8257920), error: No space left on device (28)
[rank6]:E0603 16:09:36.536000 46108 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] 
[rank6]:E0603 16:09:36.536000 46108 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] To execute this test, run the following from the base repo dir:
[rank6]:E0603 16:09:36.536000 46108 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     PYTORCH_TEST_WITH_ROCM=1 python test_pipeline_parallel_fwd_bwd.py NcclPipelineParallelForwardBackwardTest.test_inference_async_pipelining_without_interleaving
[rank6]:E0603 16:09:36.536000 46108 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] 
[rank6]:E0603 16:09:36.536000 46108 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0
[rank6]:E0603 16:09:36.536000 46108 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]  exiting process 6 with exit code: 10
[rank4]:E0603 16:09:36.536000 46106 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Caught exception: 
[rank4]:E0603 16:09:36.536000 46106 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Traceback (most recent call last):
[rank4]:E0603 16:09:36.536000 46106 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/testing/_internal/common_distributed.py", line 734, in run_test
[rank4]:E0603 16:09:36.536000 46106 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     getattr(self, test_name)()
[rank4]:E0603 16:09:36.536000 46106 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/testing/_internal/common_distributed.py", line 607, in wrapper
[rank4]:E0603 16:09:36.536000 46106 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     fn()
[rank4]:E0603 16:09:36.536000 46106 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/testing/_internal/common_utils.py", line 3173, in wrapper
[rank4]:E0603 16:09:36.536000 46106 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     method(*args, **kwargs)
[rank4]:E0603 16:09:36.536000 46106 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py", line 254, in test_inference_async_pipelining_without_interleaving
[rank4]:E0603 16:09:36.536000 46106 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     self._forward_backward_test_impl(
[rank4]:E0603 16:09:36.536000 46106 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py", line 180, in _forward_backward_test_impl
[rank4]:E0603 16:09:36.536000 46106 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     loss = fwd_bwd_func(
[rank4]:E0603 16:09:36.536000 46106 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_without_interleaving.py", line 332, in forward_backward_pipelining_without_interleaving
[rank4]:E0603 16:09:36.536000 46106 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     input_tensor = recv_forward(
[rank4]:E0603 16:09:36.536000 46106 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_without_interleaving.py", line 99, in recv_forward
[rank4]:E0603 16:09:36.536000 46106 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     p2p_communication.recv_forward(
[rank4]:E0603 16:09:36.536000 46106 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 342, in recv_forward
[rank4]:E0603 16:09:36.536000 46106 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     input_tensor, _ = _communicate(
[rank4]:E0603 16:09:36.536000 46106 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 259, in _communicate
[rank4]:E0603 16:09:36.536000 46106 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     tensor_send_prev_req, tensor_recv_prev_req, tensor_send_next_req, tensor_recv_next_req = _run_p2pops(tensor_send_prev, tensor_send_next, tensor_recv_prev, tensor_recv_next, async_comm=async_comm)
[rank4]:E0603 16:09:36.536000 46106 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 97, in _run_p2pops
[rank4]:E0603 16:09:36.536000 46106 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     reqs = torch.distributed.batch_isend_irecv(ops)
[rank4]:E0603 16:09:36.536000 46106 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/distributed/distributed_c10d.py", line 2717, in batch_isend_irecv
[rank4]:E0603 16:09:36.536000 46106 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     p2p_op.op(
[rank4]:E0603 16:09:36.536000 46106 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/distributed/distributed_c10d.py", line 2411, in irecv
[rank4]:E0603 16:09:36.536000 46106 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     return group.recv([tensor], group_src, tag)
[rank4]:E0603 16:09:36.536000 46106 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] torch.distributed.DistBackendError: NCCL error in: /skishore/github/pytorch/torch/csrc/distributed/c10d/NCCLUtils.cpp:77, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.25.1
[rank4]:E0603 16:09:36.536000 46106 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
[rank4]:E0603 16:09:36.536000 46106 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Last error:
[rank4]:E0603 16:09:36.536000 46106 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Error while creating shared memory segment /dev/shm/nccl-py8DkW (size 8257920), error: No space left on device (28)
[rank4]:E0603 16:09:36.536000 46106 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] 
[rank4]:E0603 16:09:36.536000 46106 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] To execute this test, run the following from the base repo dir:
[rank4]:E0603 16:09:36.536000 46106 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     PYTORCH_TEST_WITH_ROCM=1 python test_pipeline_parallel_fwd_bwd.py NcclPipelineParallelForwardBackwardTest.test_inference_async_pipelining_without_interleaving
[rank4]:E0603 16:09:36.536000 46106 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] 
[rank4]:E0603 16:09:36.536000 46106 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0
[rank4]:E0603 16:09:36.536000 46106 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]  exiting process 4 with exit code: 10
[rank5]:E0603 16:09:36.557000 46107 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Caught exception: 
[rank5]:E0603 16:09:36.557000 46107 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Traceback (most recent call last):
[rank5]:E0603 16:09:36.557000 46107 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/testing/_internal/common_distributed.py", line 734, in run_test
[rank5]:E0603 16:09:36.557000 46107 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     getattr(self, test_name)()
[rank5]:E0603 16:09:36.557000 46107 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/testing/_internal/common_distributed.py", line 607, in wrapper
[rank5]:E0603 16:09:36.557000 46107 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     fn()
[rank5]:E0603 16:09:36.557000 46107 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/testing/_internal/common_utils.py", line 3173, in wrapper
[rank5]:E0603 16:09:36.557000 46107 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     method(*args, **kwargs)
[rank5]:E0603 16:09:36.557000 46107 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py", line 254, in test_inference_async_pipelining_without_interleaving
[rank5]:E0603 16:09:36.557000 46107 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     self._forward_backward_test_impl(
[rank5]:E0603 16:09:36.557000 46107 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py", line 180, in _forward_backward_test_impl
[rank5]:E0603 16:09:36.557000 46107 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     loss = fwd_bwd_func(
[rank5]:E0603 16:09:36.557000 46107 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_without_interleaving.py", line 332, in forward_backward_pipelining_without_interleaving
[rank5]:E0603 16:09:36.557000 46107 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     input_tensor = recv_forward(
[rank5]:E0603 16:09:36.557000 46107 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_without_interleaving.py", line 99, in recv_forward
[rank5]:E0603 16:09:36.557000 46107 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     p2p_communication.recv_forward(
[rank5]:E0603 16:09:36.557000 46107 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 342, in recv_forward
[rank5]:E0603 16:09:36.557000 46107 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     input_tensor, _ = _communicate(
[rank5]:E0603 16:09:36.557000 46107 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 259, in _communicate
[rank5]:E0603 16:09:36.557000 46107 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     tensor_send_prev_req, tensor_recv_prev_req, tensor_send_next_req, tensor_recv_next_req = _run_p2pops(tensor_send_prev, tensor_send_next, tensor_recv_prev, tensor_recv_next, async_comm=async_comm)
[rank5]:E0603 16:09:36.557000 46107 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 97, in _run_p2pops
[rank5]:E0603 16:09:36.557000 46107 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     reqs = torch.distributed.batch_isend_irecv(ops)
[rank5]:E0603 16:09:36.557000 46107 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/distributed/distributed_c10d.py", line 2717, in batch_isend_irecv
[rank5]:E0603 16:09:36.557000 46107 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     p2p_op.op(
[rank5]:E0603 16:09:36.557000 46107 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/distributed/distributed_c10d.py", line 2411, in irecv
[rank5]:E0603 16:09:36.557000 46107 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     return group.recv([tensor], group_src, tag)
[rank5]:E0603 16:09:36.557000 46107 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] torch.distributed.DistBackendError: NCCL error in: /skishore/github/pytorch/torch/csrc/distributed/c10d/NCCLUtils.cpp:77, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.25.1
[rank5]:E0603 16:09:36.557000 46107 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
[rank5]:E0603 16:09:36.557000 46107 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Last error:
[rank5]:E0603 16:09:36.557000 46107 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Error while creating shared memory segment /dev/shm/nccl-MOrMPr (size 8257920), error: No space left on device (28)
[rank5]:E0603 16:09:36.557000 46107 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] 
[rank5]:E0603 16:09:36.557000 46107 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] To execute this test, run the following from the base repo dir:
[rank5]:E0603 16:09:36.557000 46107 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     PYTORCH_TEST_WITH_ROCM=1 python test_pipeline_parallel_fwd_bwd.py NcclPipelineParallelForwardBackwardTest.test_inference_async_pipelining_without_interleaving
[rank5]:E0603 16:09:36.557000 46107 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] 
[rank5]:E0603 16:09:36.557000 46107 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0
[rank5]:E0603 16:09:36.557000 46107 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]  exiting process 5 with exit code: 10
[rank3]:E0603 16:09:36.557000 46105 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Caught exception: 
[rank3]:E0603 16:09:36.557000 46105 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Traceback (most recent call last):
[rank3]:E0603 16:09:36.557000 46105 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/testing/_internal/common_distributed.py", line 734, in run_test
[rank3]:E0603 16:09:36.557000 46105 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     getattr(self, test_name)()
[rank3]:E0603 16:09:36.557000 46105 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/testing/_internal/common_distributed.py", line 607, in wrapper
[rank3]:E0603 16:09:36.557000 46105 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     fn()
[rank3]:E0603 16:09:36.557000 46105 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/testing/_internal/common_utils.py", line 3173, in wrapper
[rank3]:E0603 16:09:36.557000 46105 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     method(*args, **kwargs)
[rank3]:E0603 16:09:36.557000 46105 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py", line 254, in test_inference_async_pipelining_without_interleaving
[rank3]:E0603 16:09:36.557000 46105 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     self._forward_backward_test_impl(
[rank3]:E0603 16:09:36.557000 46105 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py", line 180, in _forward_backward_test_impl
[rank3]:E0603 16:09:36.557000 46105 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     loss = fwd_bwd_func(
[rank3]:E0603 16:09:36.557000 46105 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_without_interleaving.py", line 332, in forward_backward_pipelining_without_interleaving
[rank3]:E0603 16:09:36.557000 46105 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     input_tensor = recv_forward(
[rank3]:E0603 16:09:36.557000 46105 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_without_interleaving.py", line 99, in recv_forward
[rank3]:E0603 16:09:36.557000 46105 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     p2p_communication.recv_forward(
[rank3]:E0603 16:09:36.557000 46105 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 342, in recv_forward
[rank3]:E0603 16:09:36.557000 46105 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     input_tensor, _ = _communicate(
[rank3]:E0603 16:09:36.557000 46105 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 259, in _communicate
[rank3]:E0603 16:09:36.557000 46105 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     tensor_send_prev_req, tensor_recv_prev_req, tensor_send_next_req, tensor_recv_next_req = _run_p2pops(tensor_send_prev, tensor_send_next, tensor_recv_prev, tensor_recv_next, async_comm=async_comm)
[rank3]:E0603 16:09:36.557000 46105 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 97, in _run_p2pops
[rank3]:E0603 16:09:36.557000 46105 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     reqs = torch.distributed.batch_isend_irecv(ops)
[rank3]:E0603 16:09:36.557000 46105 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/distributed/distributed_c10d.py", line 2717, in batch_isend_irecv
[rank3]:E0603 16:09:36.557000 46105 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     p2p_op.op(
[rank3]:E0603 16:09:36.557000 46105 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/distributed/distributed_c10d.py", line 2411, in irecv
[rank3]:E0603 16:09:36.557000 46105 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     return group.recv([tensor], group_src, tag)
[rank3]:E0603 16:09:36.557000 46105 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] torch.distributed.DistBackendError: NCCL error in: /skishore/github/pytorch/torch/csrc/distributed/c10d/NCCLUtils.cpp:77, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.25.1
[rank3]:E0603 16:09:36.557000 46105 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
[rank3]:E0603 16:09:36.557000 46105 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Last error:
[rank3]:E0603 16:09:36.557000 46105 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Error while creating shared memory segment /dev/shm/nccl-zATTI0 (size 8257920), error: No space left on device (28)
[rank3]:E0603 16:09:36.557000 46105 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] 
[rank3]:E0603 16:09:36.557000 46105 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] To execute this test, run the following from the base repo dir:
[rank3]:E0603 16:09:36.557000 46105 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     PYTORCH_TEST_WITH_ROCM=1 python test_pipeline_parallel_fwd_bwd.py NcclPipelineParallelForwardBackwardTest.test_inference_async_pipelining_without_interleaving
[rank3]:E0603 16:09:36.557000 46105 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] 
[rank3]:E0603 16:09:36.557000 46105 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0
[rank3]:E0603 16:09:36.557000 46105 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]  exiting process 3 with exit code: 10
[rank7]:E0603 16:09:36.557000 46109 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Caught exception: 
[rank7]:E0603 16:09:36.557000 46109 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Traceback (most recent call last):
[rank7]:E0603 16:09:36.557000 46109 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/testing/_internal/common_distributed.py", line 734, in run_test
[rank7]:E0603 16:09:36.557000 46109 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     getattr(self, test_name)()
[rank7]:E0603 16:09:36.557000 46109 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/testing/_internal/common_distributed.py", line 607, in wrapper
[rank7]:E0603 16:09:36.557000 46109 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     fn()
[rank7]:E0603 16:09:36.557000 46109 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/testing/_internal/common_utils.py", line 3173, in wrapper
[rank7]:E0603 16:09:36.557000 46109 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     method(*args, **kwargs)
[rank7]:E0603 16:09:36.557000 46109 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py", line 254, in test_inference_async_pipelining_without_interleaving
[rank7]:E0603 16:09:36.557000 46109 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     self._forward_backward_test_impl(
[rank7]:E0603 16:09:36.557000 46109 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py", line 180, in _forward_backward_test_impl
[rank7]:E0603 16:09:36.557000 46109 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     loss = fwd_bwd_func(
[rank7]:E0603 16:09:36.557000 46109 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_without_interleaving.py", line 367, in forward_backward_pipelining_without_interleaving
[rank7]:E0603 16:09:36.557000 46109 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     input_tensor: List[Union[None, torch.Tensor, FutureTensor]] = recv_forward(tensor_shapes=recv_tensor_shapes, dtype=dtype, async_comm=async_comm)
[rank7]:E0603 16:09:36.557000 46109 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_without_interleaving.py", line 99, in recv_forward
[rank7]:E0603 16:09:36.557000 46109 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     p2p_communication.recv_forward(
[rank7]:E0603 16:09:36.557000 46109 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 342, in recv_forward
[rank7]:E0603 16:09:36.557000 46109 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     input_tensor, _ = _communicate(
[rank7]:E0603 16:09:36.557000 46109 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 259, in _communicate
[rank7]:E0603 16:09:36.557000 46109 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     tensor_send_prev_req, tensor_recv_prev_req, tensor_send_next_req, tensor_recv_next_req = _run_p2pops(tensor_send_prev, tensor_send_next, tensor_recv_prev, tensor_recv_next, async_comm=async_comm)
[rank7]:E0603 16:09:36.557000 46109 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 97, in _run_p2pops
[rank7]:E0603 16:09:36.557000 46109 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     reqs = torch.distributed.batch_isend_irecv(ops)
[rank7]:E0603 16:09:36.557000 46109 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/distributed/distributed_c10d.py", line 2717, in batch_isend_irecv
[rank7]:E0603 16:09:36.557000 46109 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     p2p_op.op(
[rank7]:E0603 16:09:36.557000 46109 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/distributed/distributed_c10d.py", line 2411, in irecv
[rank7]:E0603 16:09:36.557000 46109 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     return group.recv([tensor], group_src, tag)
[rank7]:E0603 16:09:36.557000 46109 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] torch.distributed.DistBackendError: NCCL error in: /skishore/github/pytorch/torch/csrc/distributed/c10d/NCCLUtils.cpp:77, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.25.1
[rank7]:E0603 16:09:36.557000 46109 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
[rank7]:E0603 16:09:36.557000 46109 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Last error:
[rank7]:E0603 16:09:36.557000 46109 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Error while creating shared memory segment /dev/shm/nccl-XMZVej (size 8257920), error: No space left on device (28)
[rank7]:E0603 16:09:36.557000 46109 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] 
[rank7]:E0603 16:09:36.557000 46109 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] To execute this test, run the following from the base repo dir:
[rank7]:E0603 16:09:36.557000 46109 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     PYTORCH_TEST_WITH_ROCM=1 python test_pipeline_parallel_fwd_bwd.py NcclPipelineParallelForwardBackwardTest.test_inference_async_pipelining_without_interleaving
[rank7]:E0603 16:09:36.557000 46109 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] 
[rank7]:E0603 16:09:36.557000 46109 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0
[rank7]:E0603 16:09:36.557000 46109 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]  exiting process 7 with exit code: 10
[dist init] rank = 5, world_size = 8
[dist init] rank = 7, world_size = 8
[dist init] rank = 3, world_size = 8
[rank1]:E0603 16:09:36.558000 46103 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Caught exception: 
[rank1]:E0603 16:09:36.558000 46103 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Traceback (most recent call last):
[rank1]:E0603 16:09:36.558000 46103 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/testing/_internal/common_distributed.py", line 734, in run_test
[rank1]:E0603 16:09:36.558000 46103 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     getattr(self, test_name)()
[rank1]:E0603 16:09:36.558000 46103 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/testing/_internal/common_distributed.py", line 607, in wrapper
[rank1]:E0603 16:09:36.558000 46103 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     fn()
[rank1]:E0603 16:09:36.558000 46103 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/testing/_internal/common_utils.py", line 3173, in wrapper
[rank1]:E0603 16:09:36.558000 46103 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     method(*args, **kwargs)
[rank1]:E0603 16:09:36.558000 46103 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py", line 254, in test_inference_async_pipelining_without_interleaving
[rank1]:E0603 16:09:36.558000 46103 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     self._forward_backward_test_impl(
[rank1]:E0603 16:09:36.558000 46103 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py", line 180, in _forward_backward_test_impl
[rank1]:E0603 16:09:36.558000 46103 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     loss = fwd_bwd_func(
[rank1]:E0603 16:09:36.558000 46103 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_without_interleaving.py", line 349, in forward_backward_pipelining_without_interleaving
[rank1]:E0603 16:09:36.558000 46103 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     send_forward(
[rank1]:E0603 16:09:36.558000 46103 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_without_interleaving.py", line 145, in send_forward
[rank1]:E0603 16:09:36.558000 46103 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     p2p_communication.send_forward(
[rank1]:E0603 16:09:36.558000 46103 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 401, in send_forward
[rank1]:E0603 16:09:36.558000 46103 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     _communicate(
[rank1]:E0603 16:09:36.558000 46103 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 259, in _communicate
[rank1]:E0603 16:09:36.558000 46103 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     tensor_send_prev_req, tensor_recv_prev_req, tensor_send_next_req, tensor_recv_next_req = _run_p2pops(tensor_send_prev, tensor_send_next, tensor_recv_prev, tensor_recv_next, async_comm=async_comm)
[rank1]:E0603 16:09:36.558000 46103 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 97, in _run_p2pops
[rank1]:E0603 16:09:36.558000 46103 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     reqs = torch.distributed.batch_isend_irecv(ops)
[rank1]:E0603 16:09:36.558000 46103 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/distributed/distributed_c10d.py", line 2717, in batch_isend_irecv
[rank1]:E0603 16:09:36.558000 46103 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     p2p_op.op(
[rank1]:E0603 16:09:36.558000 46103 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/distributed/distributed_c10d.py", line 2366, in isend
[rank1]:E0603 16:09:36.558000 46103 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     return group.send([tensor], group_dst, tag)
[rank1]:E0603 16:09:36.558000 46103 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] torch.distributed.DistBackendError: NCCL error in: /skishore/github/pytorch/torch/csrc/distributed/c10d/NCCLUtils.cpp:77, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.25.1
[rank1]:E0603 16:09:36.558000 46103 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
[rank1]:E0603 16:09:36.558000 46103 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Last error:
[rank1]:E0603 16:09:36.558000 46103 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Error while creating shared memory segment /dev/shm/nccl-G7NK1l (size 8257920), error: No space left on device (28)
[rank1]:E0603 16:09:36.558000 46103 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] 
[rank1]:E0603 16:09:36.558000 46103 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] To execute this test, run the following from the base repo dir:
[rank1]:E0603 16:09:36.558000 46103 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     PYTORCH_TEST_WITH_ROCM=1 python test_pipeline_parallel_fwd_bwd.py NcclPipelineParallelForwardBackwardTest.test_inference_async_pipelining_without_interleaving
[rank1]:E0603 16:09:36.558000 46103 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] 
[rank1]:E0603 16:09:36.558000 46103 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0
[rank1]:E0603 16:09:36.558000 46103 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]  exiting process 1 with exit code: 10
[dist init] rank = 1, world_size = 8
[rank2]:[W603 16:09:36.882339790 ProcessGroupNCCL.cpp:1553] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank4]:[W603 16:09:36.885695790 ProcessGroupNCCL.cpp:1553] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank0]:[W603 16:09:36.995526595 ProcessGroupNCCL.cpp:1553] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank6]:[W603 16:09:36.004199016 ProcessGroupNCCL.cpp:1553] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank3]:[W603 16:09:37.466539945 ProcessGroupNCCL.cpp:1553] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank1]:[W603 16:09:37.478031881 ProcessGroupNCCL.cpp:1553] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank5]:[W603 16:09:37.493209837 ProcessGroupNCCL.cpp:1553] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank7]:[W603 16:09:37.595542162 ProcessGroupNCCL.cpp:1553] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
ERROR
test_inference_no_pipelining (test_pipeline_parallel_fwd_bwd.NcclPipelineParallelForwardBackwardTest) ... Process 1 terminated with exit code 10, terminating remaining processes.
[rank7]:[W603 16:09:58.973415270 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 7]  using GPU 7 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank5]:[W603 16:09:58.982096174 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 5]  using GPU 5 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank0]:[W603 16:09:58.986582472 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank1]:[W603 16:09:59.597541251 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank3]:[W603 16:09:59.614054759 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank4]:[W603 16:09:59.618555048 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 4]  using GPU 4 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank6]:[W603 16:09:59.619095230 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 6]  using GPU 6 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank2]:[W603 16:09:59.619297020 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/utils.py:81: UserWarning: This function is only for unittest
  warnings.warn("This function is only for unittest")
[dist init] rank = 0, world_size = 8
 > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 1056
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/common.py:295: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/common.py:295: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/common.py:295: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/common.py:295: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/common.py:295: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/common.py:295: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/common.py:295: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/common.py:295: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(
 > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 1056
 > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 1056
/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py:208: UserWarning: Converting a tensor with requires_grad=True to a scalar may lead to unexpected behavior.
Consider using tensor.detach() first. (Triggered internally at /skishore/github/pytorch/aten/src/ATen/native/Scalar.cpp:22.)
  self.assertEqual(x.item() / microbatch_size, target_loss.item())
/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py:208: UserWarning: Converting a tensor with requires_grad=True to a scalar may lead to unexpected behavior.
Consider using tensor.detach() first. (Triggered internally at /skishore/github/pytorch/aten/src/ATen/native/Scalar.cpp:22.)
  self.assertEqual(x.item() / microbatch_size, target_loss.item())
/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py:208: UserWarning: Converting a tensor with requires_grad=True to a scalar may lead to unexpected behavior.
Consider using tensor.detach() first. (Triggered internally at /skishore/github/pytorch/aten/src/ATen/native/Scalar.cpp:22.)
  self.assertEqual(x.item() / microbatch_size, target_loss.item())
/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py:208: UserWarning: Converting a tensor with requires_grad=True to a scalar may lead to unexpected behavior.
Consider using tensor.detach() first. (Triggered internally at /skishore/github/pytorch/aten/src/ATen/native/Scalar.cpp:22.)
  self.assertEqual(x.item() / microbatch_size, target_loss.item())
/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py:208: UserWarning: Converting a tensor with requires_grad=True to a scalar may lead to unexpected behavior.
Consider using tensor.detach() first. (Triggered internally at /skishore/github/pytorch/aten/src/ATen/native/Scalar.cpp:22.)
  self.assertEqual(x.item() / microbatch_size, target_loss.item())
/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py:208: UserWarning: Converting a tensor with requires_grad=True to a scalar may lead to unexpected behavior.
Consider using tensor.detach() first. (Triggered internally at /skishore/github/pytorch/aten/src/ATen/native/Scalar.cpp:22.)
  self.assertEqual(x.item() / microbatch_size, target_loss.item())
 > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 1056
/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py:208: UserWarning: Converting a tensor with requires_grad=True to a scalar may lead to unexpected behavior.
Consider using tensor.detach() first. (Triggered internally at /skishore/github/pytorch/aten/src/ATen/native/Scalar.cpp:22.)
  self.assertEqual(x.item() / microbatch_size, target_loss.item())
/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py:208: UserWarning: Converting a tensor with requires_grad=True to a scalar may lead to unexpected behavior.
Consider using tensor.detach() first. (Triggered internally at /skishore/github/pytorch/aten/src/ATen/native/Scalar.cpp:22.)
  self.assertEqual(x.item() / microbatch_size, target_loss.item())
/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py:126: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  torch.cuda.amp.GradScaler(init_scale=4.0)
/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py:126: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  torch.cuda.amp.GradScaler(init_scale=4.0)
/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py:126: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  torch.cuda.amp.GradScaler(init_scale=4.0)
/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py:126: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  torch.cuda.amp.GradScaler(init_scale=4.0)
 > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 1056
/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py:126: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  torch.cuda.amp.GradScaler(init_scale=4.0)
/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py:126: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  torch.cuda.amp.GradScaler(init_scale=4.0)
/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py:126: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  torch.cuda.amp.GradScaler(init_scale=4.0)
/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py:126: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  torch.cuda.amp.GradScaler(init_scale=4.0)
 > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 1056
 > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 1056
 > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 1056
[dist init] rank = 5, world_size = 8
[dist init] rank = 7, world_size = 8
[dist init] rank = 6, world_size = 8
[dist init] rank = 4, world_size = 8
[dist init] rank = 3, world_size = 8
[dist init] rank = 1, world_size = 8
[dist init] rank = 2, world_size = 8
ok
test_inference_pipelining_with_interleaving (test_pipeline_parallel_fwd_bwd.NcclPipelineParallelForwardBackwardTest) ... [rank3]:[W603 16:11:13.919670522 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank0]:[W603 16:11:13.938326980 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank5]:[W603 16:11:13.939926885 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 5]  using GPU 5 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank4]:[W603 16:11:13.941251481 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 4]  using GPU 4 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank1]:[W603 16:11:13.942232367 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank7]:[W603 16:11:13.945283054 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 7]  using GPU 7 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank6]:[W603 16:11:13.956809010 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 6]  using GPU 6 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank2]:[W603 16:11:13.960771772 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/utils.py:81: UserWarning: This function is only for unittest
  warnings.warn("This function is only for unittest")
[dist init] rank = 0, world_size = 8
 > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 2112
[dist init] rank = 4, world_size = 8
 > number of parameters on (tensor, pipeline) model parallel rank (0, 2): 2112
[dist init] rank = 2, world_size = 8
 > number of parameters on (tensor, pipeline) model parallel rank (0, 1): 2112
[dist init] rank = 6, world_size = 8
 > number of parameters on (tensor, pipeline) model parallel rank (0, 3): 2112
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/common.py:295: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/common.py:295: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/common.py:295: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/common.py:295: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/common.py:295: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/common.py:295: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/common.py:295: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/common.py:295: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(
 > number of parameters on (tensor, pipeline) model parallel rank (0, 1): 2112
 > number of parameters on (tensor, pipeline) model parallel rank (0, 2): 2112
 > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 2112
 > number of parameters on (tensor, pipeline) model parallel rank (0, 3): 2112
 > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 2112
 > number of parameters on (tensor, pipeline) model parallel rank (0, 1): 2112
 > number of parameters on (tensor, pipeline) model parallel rank (0, 2): 2112
 > number of parameters on (tensor, pipeline) model parallel rank (0, 3): 2112
 > number of parameters on (tensor, pipeline) model parallel rank (0, 1): 2112
/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py:208: UserWarning: Converting a tensor with requires_grad=True to a scalar may lead to unexpected behavior.
Consider using tensor.detach() first. (Triggered internally at /skishore/github/pytorch/aten/src/ATen/native/Scalar.cpp:22.)
  self.assertEqual(x.item() / microbatch_size, target_loss.item())
/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py:208: UserWarning: Converting a tensor with requires_grad=True to a scalar may lead to unexpected behavior.
Consider using tensor.detach() first. (Triggered internally at /skishore/github/pytorch/aten/src/ATen/native/Scalar.cpp:22.)
  self.assertEqual(x.item() / microbatch_size, target_loss.item())
 > number of parameters on (tensor, pipeline) model parallel rank (0, 3): 2112
 > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 2112
 > number of parameters on (tensor, pipeline) model parallel rank (0, 2): 2112
/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py:126: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  torch.cuda.amp.GradScaler(init_scale=4.0)
/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py:126: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  torch.cuda.amp.GradScaler(init_scale=4.0)
/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py:126: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  torch.cuda.amp.GradScaler(init_scale=4.0)
/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py:126: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  torch.cuda.amp.GradScaler(init_scale=4.0)
 > number of parameters on (tensor, pipeline) model parallel rank (0, 1): 2112
/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py:126: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  torch.cuda.amp.GradScaler(init_scale=4.0)
 > number of parameters on (tensor, pipeline) model parallel rank (0, 2): 2112
/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py:126: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  torch.cuda.amp.GradScaler(init_scale=4.0)
/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py:126: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  torch.cuda.amp.GradScaler(init_scale=4.0)
 > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 2112
/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py:126: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  torch.cuda.amp.GradScaler(init_scale=4.0)
 > number of parameters on (tensor, pipeline) model parallel rank (0, 3): 2112
 > number of parameters on (tensor, pipeline) model parallel rank (0, 1): 2112
 > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 2112
 > number of parameters on (tensor, pipeline) model parallel rank (0, 2): 2112
 > number of parameters on (tensor, pipeline) model parallel rank (0, 3): 2112
 > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 2112
 > number of parameters on (tensor, pipeline) model parallel rank (0, 1): 2112
 > number of parameters on (tensor, pipeline) model parallel rank (0, 2): 2112
 > number of parameters on (tensor, pipeline) model parallel rank (0, 3): 2112
 > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 2112
 > number of parameters on (tensor, pipeline) model parallel rank (0, 1): 2112
 > number of parameters on (tensor, pipeline) model parallel rank (0, 2): 2112
 > number of parameters on (tensor, pipeline) model parallel rank (0, 3): 2112
[rank5]:E0603 16:12:26.862000 53769 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Caught exception: 
[rank5]:E0603 16:12:26.862000 53769 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Traceback (most recent call last):
[rank5]:E0603 16:12:26.862000 53769 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/testing/_internal/common_distributed.py", line 734, in run_test
[rank5]:E0603 16:12:26.862000 53769 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     getattr(self, test_name)()
[rank5]:E0603 16:12:26.862000 53769 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/testing/_internal/common_distributed.py", line 607, in wrapper
[rank5]:E0603 16:12:26.862000 53769 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     fn()
[rank5]:E0603 16:12:26.862000 53769 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/testing/_internal/common_utils.py", line 3173, in wrapper
[rank5]:E0603 16:12:26.862000 53769 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     method(*args, **kwargs)
[rank5]:E0603 16:12:26.862000 53769 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py", line 266, in test_inference_pipelining_with_interleaving
[rank5]:E0603 16:12:26.862000 53769 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     self._forward_backward_test_impl(
[rank5]:E0603 16:12:26.862000 53769 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py", line 180, in _forward_backward_test_impl
[rank5]:E0603 16:12:26.862000 53769 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     loss = fwd_bwd_func(
[rank5]:E0603 16:12:26.862000 53769 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_with_interleaving.py", line 219, in _forward_backward_pipelining_with_interleaving
[rank5]:E0603 16:12:26.862000 53769 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     p2p_communication.recv_forward(
[rank5]:E0603 16:12:26.862000 53769 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 342, in recv_forward
[rank5]:E0603 16:12:26.862000 53769 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     input_tensor, _ = _communicate(
[rank5]:E0603 16:12:26.862000 53769 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 259, in _communicate
[rank5]:E0603 16:12:26.862000 53769 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     tensor_send_prev_req, tensor_recv_prev_req, tensor_send_next_req, tensor_recv_next_req = _run_p2pops(tensor_send_prev, tensor_send_next, tensor_recv_prev, tensor_recv_next, async_comm=async_comm)
[rank5]:E0603 16:12:26.862000 53769 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 97, in _run_p2pops
[rank5]:E0603 16:12:26.862000 53769 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     reqs = torch.distributed.batch_isend_irecv(ops)
[rank5]:E0603 16:12:26.862000 53769 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/distributed/distributed_c10d.py", line 2717, in batch_isend_irecv
[rank5]:E0603 16:12:26.862000 53769 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     p2p_op.op(
[rank5]:E0603 16:12:26.862000 53769 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/distributed/distributed_c10d.py", line 2411, in irecv
[rank5]:E0603 16:12:26.862000 53769 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     return group.recv([tensor], group_src, tag)
[rank5]:E0603 16:12:26.862000 53769 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] torch.distributed.DistBackendError: NCCL error in: /skishore/github/pytorch/torch/csrc/distributed/c10d/NCCLUtils.cpp:77, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.25.1
[rank5]:E0603 16:12:26.862000 53769 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
[rank5]:E0603 16:12:26.862000 53769 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Last error:
[rank5]:E0603 16:12:26.862000 53769 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Error while creating shared memory segment /dev/shm/nccl-F0ZAWL (size 8257920), error: No space left on device (28)
[rank5]:E0603 16:12:26.862000 53769 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] 
[rank5]:E0603 16:12:26.862000 53769 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] To execute this test, run the following from the base repo dir:
[rank5]:E0603 16:12:26.862000 53769 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     PYTORCH_TEST_WITH_ROCM=1 python test_pipeline_parallel_fwd_bwd.py NcclPipelineParallelForwardBackwardTest.test_inference_pipelining_with_interleaving
[rank5]:E0603 16:12:26.862000 53769 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] 
[rank5]:E0603 16:12:26.862000 53769 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0
[rank5]:E0603 16:12:26.862000 53769 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]  exiting process 5 with exit code: 10
[dist init] rank = 5, world_size = 8
[rank1]:E0603 16:12:26.863000 53765 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Caught exception: 
[rank1]:E0603 16:12:26.863000 53765 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Traceback (most recent call last):
[rank1]:E0603 16:12:26.863000 53765 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/testing/_internal/common_distributed.py", line 734, in run_test
[rank1]:E0603 16:12:26.863000 53765 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     getattr(self, test_name)()
[rank1]:E0603 16:12:26.863000 53765 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/testing/_internal/common_distributed.py", line 607, in wrapper
[rank1]:E0603 16:12:26.863000 53765 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     fn()
[rank1]:E0603 16:12:26.863000 53765 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/testing/_internal/common_utils.py", line 3173, in wrapper
[rank1]:E0603 16:12:26.863000 53765 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     method(*args, **kwargs)
[rank1]:E0603 16:12:26.863000 53765 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py", line 266, in test_inference_pipelining_with_interleaving
[rank1]:E0603 16:12:26.863000 53765 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     self._forward_backward_test_impl(
[rank1]:E0603 16:12:26.863000 53765 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py", line 180, in _forward_backward_test_impl
[rank1]:E0603 16:12:26.863000 53765 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     loss = fwd_bwd_func(
[rank1]:E0603 16:12:26.863000 53765 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_with_interleaving.py", line 276, in _forward_backward_pipelining_with_interleaving
[rank1]:E0603 16:12:26.863000 53765 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     input_tensor = p2p_communication.send_forward_recv_forward(
[rank1]:E0603 16:12:26.863000 53765 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 515, in send_forward_recv_forward
[rank1]:E0603 16:12:26.863000 53765 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     input_tensor, _ = _communicate(
[rank1]:E0603 16:12:26.863000 53765 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 259, in _communicate
[rank1]:E0603 16:12:26.863000 53765 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     tensor_send_prev_req, tensor_recv_prev_req, tensor_send_next_req, tensor_recv_next_req = _run_p2pops(tensor_send_prev, tensor_send_next, tensor_recv_prev, tensor_recv_next, async_comm=async_comm)
[rank1]:E0603 16:12:26.863000 53765 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 97, in _run_p2pops
[rank1]:E0603 16:12:26.863000 53765 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     reqs = torch.distributed.batch_isend_irecv(ops)
[rank1]:E0603 16:12:26.863000 53765 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/distributed/distributed_c10d.py", line 2717, in batch_isend_irecv
[rank1]:E0603 16:12:26.863000 53765 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     p2p_op.op(
[rank1]:E0603 16:12:26.863000 53765 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/distributed/distributed_c10d.py", line 2366, in isend
[rank1]:E0603 16:12:26.863000 53765 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     return group.send([tensor], group_dst, tag)
[rank1]:E0603 16:12:26.863000 53765 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] torch.distributed.DistBackendError: NCCL error in: /skishore/github/pytorch/torch/csrc/distributed/c10d/NCCLUtils.cpp:77, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.25.1
[rank1]:E0603 16:12:26.863000 53765 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
[rank1]:E0603 16:12:26.863000 53765 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Last error:
[rank1]:E0603 16:12:26.863000 53765 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Error while creating shared memory segment /dev/shm/nccl-QgAUbi (size 8257920), error: No space left on device (28)
[rank1]:E0603 16:12:26.863000 53765 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] 
[rank1]:E0603 16:12:26.863000 53765 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] To execute this test, run the following from the base repo dir:
[rank1]:E0603 16:12:26.863000 53765 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     PYTORCH_TEST_WITH_ROCM=1 python test_pipeline_parallel_fwd_bwd.py NcclPipelineParallelForwardBackwardTest.test_inference_pipelining_with_interleaving
[rank1]:E0603 16:12:26.863000 53765 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] 
[rank1]:E0603 16:12:26.863000 53765 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0
[rank1]:E0603 16:12:26.863000 53765 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]  exiting process 1 with exit code: 10
[rank7]:E0603 16:12:26.863000 53771 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Caught exception: 
[rank7]:E0603 16:12:26.863000 53771 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Traceback (most recent call last):
[rank7]:E0603 16:12:26.863000 53771 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/testing/_internal/common_distributed.py", line 734, in run_test
[rank7]:E0603 16:12:26.863000 53771 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     getattr(self, test_name)()
[rank7]:E0603 16:12:26.863000 53771 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/testing/_internal/common_distributed.py", line 607, in wrapper
[rank7]:E0603 16:12:26.863000 53771 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     fn()
[rank7]:E0603 16:12:26.863000 53771 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/testing/_internal/common_utils.py", line 3173, in wrapper
[rank7]:E0603 16:12:26.863000 53771 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     method(*args, **kwargs)
[rank7]:E0603 16:12:26.863000 53771 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py", line 266, in test_inference_pipelining_with_interleaving
[rank7]:E0603 16:12:26.863000 53771 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     self._forward_backward_test_impl(
[rank7]:E0603 16:12:26.863000 53771 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py", line 180, in _forward_backward_test_impl
[rank7]:E0603 16:12:26.863000 53771 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     loss = fwd_bwd_func(
[rank7]:E0603 16:12:26.863000 53771 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_with_interleaving.py", line 219, in _forward_backward_pipelining_with_interleaving
[rank7]:E0603 16:12:26.863000 53771 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     p2p_communication.recv_forward(
[rank7]:E0603 16:12:26.863000 53771 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 342, in recv_forward
[rank7]:E0603 16:12:26.863000 53771 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     input_tensor, _ = _communicate(
[rank7]:E0603 16:12:26.863000 53771 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 259, in _communicate
[rank7]:E0603 16:12:26.863000 53771 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     tensor_send_prev_req, tensor_recv_prev_req, tensor_send_next_req, tensor_recv_next_req = _run_p2pops(tensor_send_prev, tensor_send_next, tensor_recv_prev, tensor_recv_next, async_comm=async_comm)
[rank7]:E0603 16:12:26.863000 53771 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 97, in _run_p2pops
[rank7]:E0603 16:12:26.863000 53771 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     reqs = torch.distributed.batch_isend_irecv(ops)
[rank7]:E0603 16:12:26.863000 53771 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/distributed/distributed_c10d.py", line 2717, in batch_isend_irecv
[rank7]:E0603 16:12:26.863000 53771 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     p2p_op.op(
[rank7]:E0603 16:12:26.863000 53771 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/distributed/distributed_c10d.py", line 2411, in irecv
[rank7]:E0603 16:12:26.863000 53771 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     return group.recv([tensor], group_src, tag)
[rank7]:E0603 16:12:26.863000 53771 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] torch.distributed.DistBackendError: NCCL error in: /skishore/github/pytorch/torch/csrc/distributed/c10d/NCCLUtils.cpp:77, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.25.1
[rank7]:E0603 16:12:26.863000 53771 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
[rank7]:E0603 16:12:26.863000 53771 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Last error:
[rank7]:E0603 16:12:26.863000 53771 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Error while creating shared memory segment /dev/shm/nccl-lidQW9 (size 8257920), error: No space left on device (28)
[rank7]:E0603 16:12:26.863000 53771 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] 
[rank7]:E0603 16:12:26.863000 53771 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] To execute this test, run the following from the base repo dir:
[rank7]:E0603 16:12:26.863000 53771 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     PYTORCH_TEST_WITH_ROCM=1 python test_pipeline_parallel_fwd_bwd.py NcclPipelineParallelForwardBackwardTest.test_inference_pipelining_with_interleaving
[rank7]:E0603 16:12:26.863000 53771 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] 
[rank7]:E0603 16:12:26.863000 53771 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0
[rank7]:E0603 16:12:26.863000 53771 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]  exiting process 7 with exit code: 10
[rank3]:E0603 16:12:26.863000 53767 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Caught exception: 
[rank3]:E0603 16:12:26.863000 53767 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Traceback (most recent call last):
[rank3]:E0603 16:12:26.863000 53767 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/testing/_internal/common_distributed.py", line 734, in run_test
[rank3]:E0603 16:12:26.863000 53767 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     getattr(self, test_name)()
[rank3]:E0603 16:12:26.863000 53767 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/testing/_internal/common_distributed.py", line 607, in wrapper
[rank3]:E0603 16:12:26.863000 53767 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     fn()
[rank3]:E0603 16:12:26.863000 53767 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/testing/_internal/common_utils.py", line 3173, in wrapper
[rank3]:E0603 16:12:26.863000 53767 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     method(*args, **kwargs)
[rank3]:E0603 16:12:26.863000 53767 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py", line 266, in test_inference_pipelining_with_interleaving
[rank3]:E0603 16:12:26.863000 53767 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     self._forward_backward_test_impl(
[rank3]:E0603 16:12:26.863000 53767 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py", line 180, in _forward_backward_test_impl
[rank3]:E0603 16:12:26.863000 53767 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     loss = fwd_bwd_func(
[rank3]:E0603 16:12:26.863000 53767 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_with_interleaving.py", line 219, in _forward_backward_pipelining_with_interleaving
[rank3]:E0603 16:12:26.863000 53767 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     p2p_communication.recv_forward(
[rank3]:E0603 16:12:26.863000 53767 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 342, in recv_forward
[rank3]:E0603 16:12:26.863000 53767 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     input_tensor, _ = _communicate(
[rank3]:E0603 16:12:26.863000 53767 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 259, in _communicate
[rank3]:E0603 16:12:26.863000 53767 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     tensor_send_prev_req, tensor_recv_prev_req, tensor_send_next_req, tensor_recv_next_req = _run_p2pops(tensor_send_prev, tensor_send_next, tensor_recv_prev, tensor_recv_next, async_comm=async_comm)
[rank3]:E0603 16:12:26.863000 53767 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 97, in _run_p2pops
[rank3]:E0603 16:12:26.863000 53767 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     reqs = torch.distributed.batch_isend_irecv(ops)
[rank3]:E0603 16:12:26.863000 53767 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/distributed/distributed_c10d.py", line 2717, in batch_isend_irecv
[rank3]:E0603 16:12:26.863000 53767 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     p2p_op.op(
[rank3]:E0603 16:12:26.863000 53767 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/distributed/distributed_c10d.py", line 2411, in irecv
[rank3]:E0603 16:12:26.863000 53767 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     return group.recv([tensor], group_src, tag)
[rank3]:E0603 16:12:26.863000 53767 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] torch.distributed.DistBackendError: NCCL error in: /skishore/github/pytorch/torch/csrc/distributed/c10d/NCCLUtils.cpp:77, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.25.1
[rank3]:E0603 16:12:26.863000 53767 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
[rank3]:E0603 16:12:26.863000 53767 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Last error:
[rank3]:E0603 16:12:26.863000 53767 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Error while creating shared memory segment /dev/shm/nccl-ya39Qh (size 8257920), error: No space left on device (28)
[rank3]:E0603 16:12:26.863000 53767 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] 
[rank3]:E0603 16:12:26.863000 53767 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] To execute this test, run the following from the base repo dir:
[rank3]:E0603 16:12:26.863000 53767 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     PYTORCH_TEST_WITH_ROCM=1 python test_pipeline_parallel_fwd_bwd.py NcclPipelineParallelForwardBackwardTest.test_inference_pipelining_with_interleaving
[rank3]:E0603 16:12:26.863000 53767 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] 
[rank3]:E0603 16:12:26.863000 53767 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0
[rank3]:E0603 16:12:26.863000 53767 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]  exiting process 3 with exit code: 10
[dist init] rank = 1, world_size = 8
[dist init] rank = 7, world_size = 8
[dist init] rank = 3, world_size = 8
[rank6]:E0603 16:12:26.877000 53770 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Caught exception: 
[rank6]:E0603 16:12:26.877000 53770 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Traceback (most recent call last):
[rank6]:E0603 16:12:26.877000 53770 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/testing/_internal/common_distributed.py", line 734, in run_test
[rank6]:E0603 16:12:26.877000 53770 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     getattr(self, test_name)()
[rank6]:E0603 16:12:26.877000 53770 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/testing/_internal/common_distributed.py", line 607, in wrapper
[rank6]:E0603 16:12:26.877000 53770 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     fn()
[rank6]:E0603 16:12:26.877000 53770 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/testing/_internal/common_utils.py", line 3173, in wrapper
[rank6]:E0603 16:12:26.877000 53770 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     method(*args, **kwargs)
[rank6]:E0603 16:12:26.877000 53770 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py", line 266, in test_inference_pipelining_with_interleaving
[rank6]:E0603 16:12:26.877000 53770 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     self._forward_backward_test_impl(
[rank6]:E0603 16:12:26.877000 53770 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py", line 180, in _forward_backward_test_impl
[rank6]:E0603 16:12:26.877000 53770 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     loss = fwd_bwd_func(
[rank6]:E0603 16:12:26.877000 53770 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_with_interleaving.py", line 219, in _forward_backward_pipelining_with_interleaving
[rank6]:E0603 16:12:26.877000 53770 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     p2p_communication.recv_forward(
[rank6]:E0603 16:12:26.877000 53770 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 342, in recv_forward
[rank6]:E0603 16:12:26.877000 53770 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     input_tensor, _ = _communicate(
[rank6]:E0603 16:12:26.877000 53770 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 259, in _communicate
[rank6]:E0603 16:12:26.877000 53770 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     tensor_send_prev_req, tensor_recv_prev_req, tensor_send_next_req, tensor_recv_next_req = _run_p2pops(tensor_send_prev, tensor_send_next, tensor_recv_prev, tensor_recv_next, async_comm=async_comm)
[rank6]:E0603 16:12:26.877000 53770 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 97, in _run_p2pops
[rank6]:E0603 16:12:26.877000 53770 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     reqs = torch.distributed.batch_isend_irecv(ops)
[rank6]:E0603 16:12:26.877000 53770 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/distributed/distributed_c10d.py", line 2717, in batch_isend_irecv
[rank6]:E0603 16:12:26.877000 53770 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     p2p_op.op(
[rank6]:E0603 16:12:26.877000 53770 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/distributed/distributed_c10d.py", line 2411, in irecv
[rank6]:E0603 16:12:26.877000 53770 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     return group.recv([tensor], group_src, tag)
[rank6]:E0603 16:12:26.877000 53770 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] torch.distributed.DistBackendError: NCCL error in: /skishore/github/pytorch/torch/csrc/distributed/c10d/NCCLUtils.cpp:77, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.25.1
[rank6]:E0603 16:12:26.877000 53770 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
[rank6]:E0603 16:12:26.877000 53770 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Last error:
[rank6]:E0603 16:12:26.877000 53770 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Error while creating shared memory segment /dev/shm/nccl-7nTQUd (size 8257920), error: No space left on device (28)
[rank6]:E0603 16:12:26.877000 53770 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] 
[rank6]:E0603 16:12:26.877000 53770 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] To execute this test, run the following from the base repo dir:
[rank6]:E0603 16:12:26.877000 53770 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     PYTORCH_TEST_WITH_ROCM=1 python test_pipeline_parallel_fwd_bwd.py NcclPipelineParallelForwardBackwardTest.test_inference_pipelining_with_interleaving
[rank6]:E0603 16:12:26.877000 53770 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] 
[rank6]:E0603 16:12:26.877000 53770 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0
[rank6]:E0603 16:12:26.877000 53770 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]  exiting process 6 with exit code: 10
[rank4]:E0603 16:12:26.877000 53768 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Caught exception: 
[rank4]:E0603 16:12:26.877000 53768 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Traceback (most recent call last):
[rank4]:E0603 16:12:26.877000 53768 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/testing/_internal/common_distributed.py", line 734, in run_test
[rank4]:E0603 16:12:26.877000 53768 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     getattr(self, test_name)()
[rank4]:E0603 16:12:26.877000 53768 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/testing/_internal/common_distributed.py", line 607, in wrapper
[rank4]:E0603 16:12:26.877000 53768 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     fn()
[rank4]:E0603 16:12:26.877000 53768 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/testing/_internal/common_utils.py", line 3173, in wrapper
[rank4]:E0603 16:12:26.877000 53768 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     method(*args, **kwargs)
[rank4]:E0603 16:12:26.877000 53768 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py", line 266, in test_inference_pipelining_with_interleaving
[rank4]:E0603 16:12:26.877000 53768 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     self._forward_backward_test_impl(
[rank4]:E0603 16:12:26.877000 53768 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py", line 180, in _forward_backward_test_impl
[rank4]:E0603 16:12:26.877000 53768 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     loss = fwd_bwd_func(
[rank4]:E0603 16:12:26.877000 53768 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_with_interleaving.py", line 219, in _forward_backward_pipelining_with_interleaving
[rank4]:E0603 16:12:26.877000 53768 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     p2p_communication.recv_forward(
[rank4]:E0603 16:12:26.877000 53768 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 342, in recv_forward
[rank4]:E0603 16:12:26.877000 53768 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     input_tensor, _ = _communicate(
[rank4]:E0603 16:12:26.877000 53768 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 259, in _communicate
[rank4]:E0603 16:12:26.877000 53768 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     tensor_send_prev_req, tensor_recv_prev_req, tensor_send_next_req, tensor_recv_next_req = _run_p2pops(tensor_send_prev, tensor_send_next, tensor_recv_prev, tensor_recv_next, async_comm=async_comm)
[rank4]:E0603 16:12:26.877000 53768 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 97, in _run_p2pops
[rank4]:E0603 16:12:26.877000 53768 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     reqs = torch.distributed.batch_isend_irecv(ops)
[rank4]:E0603 16:12:26.877000 53768 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/distributed/distributed_c10d.py", line 2717, in batch_isend_irecv
[rank4]:E0603 16:12:26.877000 53768 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     p2p_op.op(
[rank4]:E0603 16:12:26.877000 53768 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/distributed/distributed_c10d.py", line 2411, in irecv
[rank4]:E0603 16:12:26.877000 53768 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     return group.recv([tensor], group_src, tag)
[rank4]:E0603 16:12:26.877000 53768 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] torch.distributed.DistBackendError: NCCL error in: /skishore/github/pytorch/torch/csrc/distributed/c10d/NCCLUtils.cpp:77, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.25.1
[rank4]:E0603 16:12:26.877000 53768 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
[rank4]:E0603 16:12:26.877000 53768 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Last error:
[rank4]:E0603 16:12:26.877000 53768 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Error while creating shared memory segment /dev/shm/nccl-UxtCFa (size 8257920), error: No space left on device (28)
[rank4]:E0603 16:12:26.877000 53768 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] 
[rank4]:E0603 16:12:26.877000 53768 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] To execute this test, run the following from the base repo dir:
[rank4]:E0603 16:12:26.877000 53768 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     PYTORCH_TEST_WITH_ROCM=1 python test_pipeline_parallel_fwd_bwd.py NcclPipelineParallelForwardBackwardTest.test_inference_pipelining_with_interleaving
[rank4]:E0603 16:12:26.877000 53768 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] 
[rank4]:E0603 16:12:26.877000 53768 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0
[rank4]:E0603 16:12:26.877000 53768 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]  exiting process 4 with exit code: 10
[rank0]:E0603 16:12:26.877000 53764 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Caught exception: 
[rank0]:E0603 16:12:26.877000 53764 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Traceback (most recent call last):
[rank0]:E0603 16:12:26.877000 53764 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/testing/_internal/common_distributed.py", line 734, in run_test
[rank0]:E0603 16:12:26.877000 53764 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     getattr(self, test_name)()
[rank0]:E0603 16:12:26.877000 53764 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/testing/_internal/common_distributed.py", line 607, in wrapper
[rank0]:E0603 16:12:26.877000 53764 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     fn()
[rank0]:E0603 16:12:26.877000 53764 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/testing/_internal/common_utils.py", line 3173, in wrapper
[rank0]:E0603 16:12:26.877000 53764 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     method(*args, **kwargs)
[rank0]:E0603 16:12:26.877000 53764 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py", line 266, in test_inference_pipelining_with_interleaving
[rank0]:E0603 16:12:26.877000 53764 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     self._forward_backward_test_impl(
[rank0]:E0603 16:12:26.877000 53764 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py", line 180, in _forward_backward_test_impl
[rank0]:E0603 16:12:26.877000 53764 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     loss = fwd_bwd_func(
[rank0]:E0603 16:12:26.877000 53764 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_with_interleaving.py", line 276, in _forward_backward_pipelining_with_interleaving
[rank0]:E0603 16:12:26.877000 53764 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     input_tensor = p2p_communication.send_forward_recv_forward(
[rank0]:E0603 16:12:26.877000 53764 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 515, in send_forward_recv_forward
[rank0]:E0603 16:12:26.877000 53764 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     input_tensor, _ = _communicate(
[rank0]:E0603 16:12:26.877000 53764 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 259, in _communicate
[rank0]:E0603 16:12:26.877000 53764 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     tensor_send_prev_req, tensor_recv_prev_req, tensor_send_next_req, tensor_recv_next_req = _run_p2pops(tensor_send_prev, tensor_send_next, tensor_recv_prev, tensor_recv_next, async_comm=async_comm)
[rank0]:E0603 16:12:26.877000 53764 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 97, in _run_p2pops
[rank0]:E0603 16:12:26.877000 53764 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     reqs = torch.distributed.batch_isend_irecv(ops)
[rank0]:E0603 16:12:26.877000 53764 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/distributed/distributed_c10d.py", line 2717, in batch_isend_irecv
[rank0]:E0603 16:12:26.877000 53764 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     p2p_op.op(
[rank0]:E0603 16:12:26.877000 53764 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/distributed/distributed_c10d.py", line 2366, in isend
[rank0]:E0603 16:12:26.877000 53764 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     return group.send([tensor], group_dst, tag)
[rank0]:E0603 16:12:26.877000 53764 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] torch.distributed.DistBackendError: NCCL error in: /skishore/github/pytorch/torch/csrc/distributed/c10d/NCCLUtils.cpp:77, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.25.1
[rank0]:E0603 16:12:26.877000 53764 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
[rank0]:E0603 16:12:26.877000 53764 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Last error:
[rank0]:E0603 16:12:26.877000 53764 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Error while creating shared memory segment /dev/shm/nccl-VMshzE (size 8257920), error: No space left on device (28)
[rank0]:E0603 16:12:26.877000 53764 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] 
[rank0]:E0603 16:12:26.877000 53764 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] To execute this test, run the following from the base repo dir:
[rank0]:E0603 16:12:26.877000 53764 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     PYTORCH_TEST_WITH_ROCM=1 python test_pipeline_parallel_fwd_bwd.py NcclPipelineParallelForwardBackwardTest.test_inference_pipelining_with_interleaving
[rank0]:E0603 16:12:26.877000 53764 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] 
[rank0]:E0603 16:12:26.877000 53764 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0
[rank0]:E0603 16:12:26.877000 53764 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]  exiting process 0 with exit code: 10
[rank2]:E0603 16:12:26.877000 53766 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Caught exception: 
[rank2]:E0603 16:12:26.877000 53766 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Traceback (most recent call last):
[rank2]:E0603 16:12:26.877000 53766 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/testing/_internal/common_distributed.py", line 734, in run_test
[rank2]:E0603 16:12:26.877000 53766 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     getattr(self, test_name)()
[rank2]:E0603 16:12:26.877000 53766 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/testing/_internal/common_distributed.py", line 607, in wrapper
[rank2]:E0603 16:12:26.877000 53766 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     fn()
[rank2]:E0603 16:12:26.877000 53766 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/testing/_internal/common_utils.py", line 3173, in wrapper
[rank2]:E0603 16:12:26.877000 53766 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     method(*args, **kwargs)
[rank2]:E0603 16:12:26.877000 53766 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py", line 266, in test_inference_pipelining_with_interleaving
[rank2]:E0603 16:12:26.877000 53766 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     self._forward_backward_test_impl(
[rank2]:E0603 16:12:26.877000 53766 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py", line 180, in _forward_backward_test_impl
[rank2]:E0603 16:12:26.877000 53766 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     loss = fwd_bwd_func(
[rank2]:E0603 16:12:26.877000 53766 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_with_interleaving.py", line 219, in _forward_backward_pipelining_with_interleaving
[rank2]:E0603 16:12:26.877000 53766 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     p2p_communication.recv_forward(
[rank2]:E0603 16:12:26.877000 53766 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 342, in recv_forward
[rank2]:E0603 16:12:26.877000 53766 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     input_tensor, _ = _communicate(
[rank2]:E0603 16:12:26.877000 53766 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 259, in _communicate
[rank2]:E0603 16:12:26.877000 53766 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     tensor_send_prev_req, tensor_recv_prev_req, tensor_send_next_req, tensor_recv_next_req = _run_p2pops(tensor_send_prev, tensor_send_next, tensor_recv_prev, tensor_recv_next, async_comm=async_comm)
[rank2]:E0603 16:12:26.877000 53766 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 97, in _run_p2pops
[rank2]:E0603 16:12:26.877000 53766 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     reqs = torch.distributed.batch_isend_irecv(ops)
[rank2]:E0603 16:12:26.877000 53766 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/distributed/distributed_c10d.py", line 2717, in batch_isend_irecv
[rank2]:E0603 16:12:26.877000 53766 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     p2p_op.op(
[rank2]:E0603 16:12:26.877000 53766 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/distributed/distributed_c10d.py", line 2411, in irecv
[rank2]:E0603 16:12:26.877000 53766 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     return group.recv([tensor], group_src, tag)
[rank2]:E0603 16:12:26.877000 53766 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] torch.distributed.DistBackendError: NCCL error in: /skishore/github/pytorch/torch/csrc/distributed/c10d/NCCLUtils.cpp:77, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.25.1
[rank2]:E0603 16:12:26.877000 53766 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
[rank2]:E0603 16:12:26.877000 53766 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Last error:
[rank2]:E0603 16:12:26.877000 53766 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Error while creating shared memory segment /dev/shm/nccl-veLfEs (size 8257920), error: No space left on device (28)
[rank2]:E0603 16:12:26.877000 53766 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] 
[rank2]:E0603 16:12:26.877000 53766 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] To execute this test, run the following from the base repo dir:
[rank2]:E0603 16:12:26.877000 53766 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     PYTORCH_TEST_WITH_ROCM=1 python test_pipeline_parallel_fwd_bwd.py NcclPipelineParallelForwardBackwardTest.test_inference_pipelining_with_interleaving
[rank2]:E0603 16:12:26.877000 53766 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] 
[rank2]:E0603 16:12:26.877000 53766 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0
[rank2]:E0603 16:12:26.877000 53766 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]  exiting process 2 with exit code: 10
[rank6]:[W603 16:12:27.325486798 ProcessGroupNCCL.cpp:1553] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank2]:[W603 16:12:27.341952636 ProcessGroupNCCL.cpp:1553] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank4]:[W603 16:12:27.359584073 ProcessGroupNCCL.cpp:1553] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank0]:[W603 16:12:27.428477181 ProcessGroupNCCL.cpp:1553] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank7]:[W603 16:12:27.760319732 ProcessGroupNCCL.cpp:1553] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank3]:[W603 16:12:27.767121477 ProcessGroupNCCL.cpp:1553] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank1]:[W603 16:12:27.809854378 ProcessGroupNCCL.cpp:1553] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank5]:[W603 16:12:27.932506932 ProcessGroupNCCL.cpp:1553] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
ERROR
test_inference_pipelining_without_interleaving (test_pipeline_parallel_fwd_bwd.NcclPipelineParallelForwardBackwardTest) ... Process 7 terminated with exit code 10, terminating remaining processes.
[rank0]:[W603 16:12:49.087699947 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank5]:[W603 16:12:49.103615317 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 5]  using GPU 5 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank7]:[W603 16:12:49.117766760 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 7]  using GPU 7 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank1]:[W603 16:12:49.126433052 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank4]:[W603 16:12:49.181958389 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 4]  using GPU 4 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank3]:[W603 16:12:49.185125920 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank6]:[W603 16:12:49.191761358 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 6]  using GPU 6 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank2]:[W603 16:12:49.201756693 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[dist init] rank = 2, world_size = 8
 > number of parameters on (tensor, pipeline) model parallel rank (0, 1): 1056
[dist init] rank = 4, world_size = 8
 > number of parameters on (tensor, pipeline) model parallel rank (0, 2): 1056
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/utils.py:81: UserWarning: This function is only for unittest
  warnings.warn("This function is only for unittest")
[dist init] rank = 6, world_size = 8
 > number of parameters on (tensor, pipeline) model parallel rank (0, 3): 1056
[dist init] rank = 0, world_size = 8
 > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 1056
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/common.py:295: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/common.py:295: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/common.py:295: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/common.py:295: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(
 > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 1056
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/common.py:295: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/common.py:295: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(
 > number of parameters on (tensor, pipeline) model parallel rank (0, 1): 1056
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/common.py:295: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/common.py:295: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(
 > number of parameters on (tensor, pipeline) model parallel rank (0, 2): 1056
 > number of parameters on (tensor, pipeline) model parallel rank (0, 3): 1056
 > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 1056
 > number of parameters on (tensor, pipeline) model parallel rank (0, 1): 1056
 > number of parameters on (tensor, pipeline) model parallel rank (0, 2): 1056
 > number of parameters on (tensor, pipeline) model parallel rank (0, 3): 1056
 > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 1056
 > number of parameters on (tensor, pipeline) model parallel rank (0, 1): 1056
 > number of parameters on (tensor, pipeline) model parallel rank (0, 2): 1056
/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py:208: UserWarning: Converting a tensor with requires_grad=True to a scalar may lead to unexpected behavior.
Consider using tensor.detach() first. (Triggered internally at /skishore/github/pytorch/aten/src/ATen/native/Scalar.cpp:22.)
  self.assertEqual(x.item() / microbatch_size, target_loss.item())
 > number of parameters on (tensor, pipeline) model parallel rank (0, 3): 1056
/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py:208: UserWarning: Converting a tensor with requires_grad=True to a scalar may lead to unexpected behavior.
Consider using tensor.detach() first. (Triggered internally at /skishore/github/pytorch/aten/src/ATen/native/Scalar.cpp:22.)
  self.assertEqual(x.item() / microbatch_size, target_loss.item())
/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py:126: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  torch.cuda.amp.GradScaler(init_scale=4.0)
 > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 1056
/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py:126: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  torch.cuda.amp.GradScaler(init_scale=4.0)
/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py:126: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  torch.cuda.amp.GradScaler(init_scale=4.0)
 > number of parameters on (tensor, pipeline) model parallel rank (0, 1): 1056
/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py:126: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  torch.cuda.amp.GradScaler(init_scale=4.0)
/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py:126: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  torch.cuda.amp.GradScaler(init_scale=4.0)
 > number of parameters on (tensor, pipeline) model parallel rank (0, 2): 1056
/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py:126: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  torch.cuda.amp.GradScaler(init_scale=4.0)
/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py:126: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  torch.cuda.amp.GradScaler(init_scale=4.0)
/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py:126: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  torch.cuda.amp.GradScaler(init_scale=4.0)
 > number of parameters on (tensor, pipeline) model parallel rank (0, 3): 1056
 > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 1056
 > number of parameters on (tensor, pipeline) model parallel rank (0, 1): 1056
 > number of parameters on (tensor, pipeline) model parallel rank (0, 2): 1056
 > number of parameters on (tensor, pipeline) model parallel rank (0, 3): 1056
 > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 1056
 > number of parameters on (tensor, pipeline) model parallel rank (0, 1): 1056
 > number of parameters on (tensor, pipeline) model parallel rank (0, 3): 1056
 > number of parameters on (tensor, pipeline) model parallel rank (0, 2): 1056
 > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 1056
 > number of parameters on (tensor, pipeline) model parallel rank (0, 1): 1056
 > number of parameters on (tensor, pipeline) model parallel rank (0, 2): 1056
 > number of parameters on (tensor, pipeline) model parallel rank (0, 3): 1056
[rank4]:E0603 16:13:52.280000 57801 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Caught exception: 
[rank4]:E0603 16:13:52.280000 57801 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Traceback (most recent call last):
[rank4]:E0603 16:13:52.280000 57801 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/testing/_internal/common_distributed.py", line 734, in run_test
[rank4]:E0603 16:13:52.280000 57801 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     getattr(self, test_name)()
[rank4]:E0603 16:13:52.280000 57801 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/testing/_internal/common_distributed.py", line 607, in wrapper
[rank4]:E0603 16:13:52.280000 57801 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     fn()
[rank4]:E0603 16:13:52.280000 57801 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/testing/_internal/common_utils.py", line 3173, in wrapper
[rank4]:E0603 16:13:52.280000 57801 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     method(*args, **kwargs)
[rank4]:E0603 16:13:52.280000 57801 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py", line 244, in test_inference_pipelining_without_interleaving
[rank4]:E0603 16:13:52.280000 57801 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     self._forward_backward_test_impl(
[rank4]:E0603 16:13:52.280000 57801 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py", line 180, in _forward_backward_test_impl
[rank4]:E0603 16:13:52.280000 57801 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     loss = fwd_bwd_func(
[rank4]:E0603 16:13:52.280000 57801 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_without_interleaving.py", line 332, in forward_backward_pipelining_without_interleaving
[rank4]:E0603 16:13:52.280000 57801 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     input_tensor = recv_forward(
[rank4]:E0603 16:13:52.280000 57801 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_without_interleaving.py", line 99, in recv_forward
[rank4]:E0603 16:13:52.280000 57801 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     p2p_communication.recv_forward(
[rank4]:E0603 16:13:52.280000 57801 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 342, in recv_forward
[rank4]:E0603 16:13:52.280000 57801 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     input_tensor, _ = _communicate(
[rank4]:E0603 16:13:52.280000 57801 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 259, in _communicate
[rank4]:E0603 16:13:52.280000 57801 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     tensor_send_prev_req, tensor_recv_prev_req, tensor_send_next_req, tensor_recv_next_req = _run_p2pops(tensor_send_prev, tensor_send_next, tensor_recv_prev, tensor_recv_next, async_comm=async_comm)
[rank4]:E0603 16:13:52.280000 57801 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 97, in _run_p2pops
[rank4]:E0603 16:13:52.280000 57801 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     reqs = torch.distributed.batch_isend_irecv(ops)
[rank4]:E0603 16:13:52.280000 57801 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/distributed/distributed_c10d.py", line 2717, in batch_isend_irecv
[rank4]:E0603 16:13:52.280000 57801 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     p2p_op.op(
[rank4]:E0603 16:13:52.280000 57801 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/distributed/distributed_c10d.py", line 2411, in irecv
[rank4]:E0603 16:13:52.280000 57801 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     return group.recv([tensor], group_src, tag)
[rank4]:E0603 16:13:52.280000 57801 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] torch.distributed.DistBackendError: NCCL error in: /skishore/github/pytorch/torch/csrc/distributed/c10d/NCCLUtils.cpp:77, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.25.1
[rank4]:E0603 16:13:52.280000 57801 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
[rank4]:E0603 16:13:52.280000 57801 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Last error:
[rank4]:E0603 16:13:52.280000 57801 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Error while creating shared memory segment /dev/shm/nccl-WflEm3 (size 8257920), error: No space left on device (28)
[rank4]:E0603 16:13:52.280000 57801 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] 
[rank4]:E0603 16:13:52.280000 57801 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] To execute this test, run the following from the base repo dir:
[rank4]:E0603 16:13:52.280000 57801 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     PYTORCH_TEST_WITH_ROCM=1 python test_pipeline_parallel_fwd_bwd.py NcclPipelineParallelForwardBackwardTest.test_inference_pipelining_without_interleaving
[rank4]:E0603 16:13:52.280000 57801 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] 
[rank4]:E0603 16:13:52.280000 57801 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0
[rank4]:E0603 16:13:52.280000 57801 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]  exiting process 4 with exit code: 10
[rank2]:E0603 16:13:52.281000 57799 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Caught exception: 
[rank2]:E0603 16:13:52.281000 57799 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Traceback (most recent call last):
[rank2]:E0603 16:13:52.281000 57799 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/testing/_internal/common_distributed.py", line 734, in run_test
[rank2]:E0603 16:13:52.281000 57799 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     getattr(self, test_name)()
[rank2]:E0603 16:13:52.281000 57799 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/testing/_internal/common_distributed.py", line 607, in wrapper
[rank2]:E0603 16:13:52.281000 57799 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     fn()
[rank2]:E0603 16:13:52.281000 57799 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/testing/_internal/common_utils.py", line 3173, in wrapper
[rank2]:E0603 16:13:52.281000 57799 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     method(*args, **kwargs)
[rank2]:E0603 16:13:52.281000 57799 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py", line 244, in test_inference_pipelining_without_interleaving
[rank2]:E0603 16:13:52.281000 57799 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     self._forward_backward_test_impl(
[rank2]:E0603 16:13:52.281000 57799 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py", line 180, in _forward_backward_test_impl
[rank2]:E0603 16:13:52.281000 57799 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     loss = fwd_bwd_func(
[rank2]:E0603 16:13:52.281000 57799 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_without_interleaving.py", line 332, in forward_backward_pipelining_without_interleaving
[rank2]:E0603 16:13:52.281000 57799 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     input_tensor = recv_forward(
[rank2]:E0603 16:13:52.281000 57799 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_without_interleaving.py", line 99, in recv_forward
[rank2]:E0603 16:13:52.281000 57799 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     p2p_communication.recv_forward(
[rank2]:E0603 16:13:52.281000 57799 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 342, in recv_forward
[rank2]:E0603 16:13:52.281000 57799 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     input_tensor, _ = _communicate(
[rank2]:E0603 16:13:52.281000 57799 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 259, in _communicate
[rank2]:E0603 16:13:52.281000 57799 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     tensor_send_prev_req, tensor_recv_prev_req, tensor_send_next_req, tensor_recv_next_req = _run_p2pops(tensor_send_prev, tensor_send_next, tensor_recv_prev, tensor_recv_next, async_comm=async_comm)
[rank2]:E0603 16:13:52.281000 57799 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 97, in _run_p2pops
[rank2]:E0603 16:13:52.281000 57799 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     reqs = torch.distributed.batch_isend_irecv(ops)
[rank2]:E0603 16:13:52.281000 57799 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/distributed/distributed_c10d.py", line 2717, in batch_isend_irecv
[rank2]:E0603 16:13:52.281000 57799 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     p2p_op.op(
[rank2]:E0603 16:13:52.281000 57799 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/distributed/distributed_c10d.py", line 2411, in irecv
[rank2]:E0603 16:13:52.281000 57799 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     return group.recv([tensor], group_src, tag)
[rank2]:E0603 16:13:52.281000 57799 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] torch.distributed.DistBackendError: NCCL error in: /skishore/github/pytorch/torch/csrc/distributed/c10d/NCCLUtils.cpp:77, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.25.1
[rank2]:E0603 16:13:52.281000 57799 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
[rank2]:E0603 16:13:52.281000 57799 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Last error:
[rank2]:E0603 16:13:52.281000 57799 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Error while creating shared memory segment /dev/shm/nccl-8PJSpS (size 8257920), error: No space left on device (28)
[rank2]:E0603 16:13:52.281000 57799 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] 
[rank2]:E0603 16:13:52.281000 57799 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] To execute this test, run the following from the base repo dir:
[rank2]:E0603 16:13:52.281000 57799 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     PYTORCH_TEST_WITH_ROCM=1 python test_pipeline_parallel_fwd_bwd.py NcclPipelineParallelForwardBackwardTest.test_inference_pipelining_without_interleaving
[rank2]:E0603 16:13:52.281000 57799 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] 
[rank2]:E0603 16:13:52.281000 57799 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0
[rank2]:E0603 16:13:52.281000 57799 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]  exiting process 2 with exit code: 10
[rank0]:E0603 16:13:52.281000 57797 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Caught exception: 
[rank0]:E0603 16:13:52.281000 57797 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Traceback (most recent call last):
[rank0]:E0603 16:13:52.281000 57797 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/testing/_internal/common_distributed.py", line 734, in run_test
[rank0]:E0603 16:13:52.281000 57797 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     getattr(self, test_name)()
[rank0]:E0603 16:13:52.281000 57797 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/testing/_internal/common_distributed.py", line 607, in wrapper
[rank0]:E0603 16:13:52.281000 57797 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     fn()
[rank0]:E0603 16:13:52.281000 57797 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/testing/_internal/common_utils.py", line 3173, in wrapper
[rank0]:E0603 16:13:52.281000 57797 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     method(*args, **kwargs)
[rank0]:E0603 16:13:52.281000 57797 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py", line 244, in test_inference_pipelining_without_interleaving
[rank0]:E0603 16:13:52.281000 57797 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     self._forward_backward_test_impl(
[rank0]:E0603 16:13:52.281000 57797 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py", line 180, in _forward_backward_test_impl
[rank0]:E0603 16:13:52.281000 57797 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     loss = fwd_bwd_func(
[rank0]:E0603 16:13:52.281000 57797 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_without_interleaving.py", line 349, in forward_backward_pipelining_without_interleaving
[rank0]:E0603 16:13:52.281000 57797 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     send_forward(
[rank0]:E0603 16:13:52.281000 57797 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_without_interleaving.py", line 145, in send_forward
[rank0]:E0603 16:13:52.281000 57797 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     p2p_communication.send_forward(
[rank0]:E0603 16:13:52.281000 57797 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 401, in send_forward
[rank0]:E0603 16:13:52.281000 57797 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     _communicate(
[rank0]:E0603 16:13:52.281000 57797 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 259, in _communicate
[rank0]:E0603 16:13:52.281000 57797 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     tensor_send_prev_req, tensor_recv_prev_req, tensor_send_next_req, tensor_recv_next_req = _run_p2pops(tensor_send_prev, tensor_send_next, tensor_recv_prev, tensor_recv_next, async_comm=async_comm)
[rank0]:E0603 16:13:52.281000 57797 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 97, in _run_p2pops
[rank0]:E0603 16:13:52.281000 57797 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     reqs = torch.distributed.batch_isend_irecv(ops)
[rank0]:E0603 16:13:52.281000 57797 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/distributed/distributed_c10d.py", line 2717, in batch_isend_irecv
[rank0]:E0603 16:13:52.281000 57797 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     p2p_op.op(
[rank0]:E0603 16:13:52.281000 57797 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/distributed/distributed_c10d.py", line 2366, in isend
[rank0]:E0603 16:13:52.281000 57797 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     return group.send([tensor], group_dst, tag)
[rank0]:E0603 16:13:52.281000 57797 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] torch.distributed.DistBackendError: NCCL error in: /skishore/github/pytorch/torch/csrc/distributed/c10d/NCCLUtils.cpp:77, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.25.1
[rank0]:E0603 16:13:52.281000 57797 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
[rank0]:E0603 16:13:52.281000 57797 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Last error:
[rank0]:E0603 16:13:52.281000 57797 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Error while creating shared memory segment /dev/shm/nccl-76Ma69 (size 8257920), error: No space left on device (28)
[rank0]:E0603 16:13:52.281000 57797 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] 
[rank0]:E0603 16:13:52.281000 57797 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] To execute this test, run the following from the base repo dir:
[rank0]:E0603 16:13:52.281000 57797 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     PYTORCH_TEST_WITH_ROCM=1 python test_pipeline_parallel_fwd_bwd.py NcclPipelineParallelForwardBackwardTest.test_inference_pipelining_without_interleaving
[rank0]:E0603 16:13:52.281000 57797 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] 
[rank0]:E0603 16:13:52.281000 57797 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0
[rank0]:E0603 16:13:52.281000 57797 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]  exiting process 0 with exit code: 10
[rank6]:E0603 16:13:52.283000 57803 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Caught exception: 
[rank6]:E0603 16:13:52.283000 57803 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Traceback (most recent call last):
[rank6]:E0603 16:13:52.283000 57803 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/testing/_internal/common_distributed.py", line 734, in run_test
[rank6]:E0603 16:13:52.283000 57803 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     getattr(self, test_name)()
[rank6]:E0603 16:13:52.283000 57803 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/testing/_internal/common_distributed.py", line 607, in wrapper
[rank6]:E0603 16:13:52.283000 57803 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     fn()
[rank6]:E0603 16:13:52.283000 57803 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/testing/_internal/common_utils.py", line 3173, in wrapper
[rank6]:E0603 16:13:52.283000 57803 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     method(*args, **kwargs)
[rank6]:E0603 16:13:52.283000 57803 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py", line 244, in test_inference_pipelining_without_interleaving
[rank6]:E0603 16:13:52.283000 57803 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     self._forward_backward_test_impl(
[rank6]:E0603 16:13:52.283000 57803 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py", line 180, in _forward_backward_test_impl
[rank6]:E0603 16:13:52.283000 57803 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     loss = fwd_bwd_func(
[rank6]:E0603 16:13:52.283000 57803 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_without_interleaving.py", line 367, in forward_backward_pipelining_without_interleaving
[rank6]:E0603 16:13:52.283000 57803 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     input_tensor: List[Union[None, torch.Tensor, FutureTensor]] = recv_forward(tensor_shapes=recv_tensor_shapes, dtype=dtype, async_comm=async_comm)
[rank6]:E0603 16:13:52.283000 57803 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_without_interleaving.py", line 99, in recv_forward
[rank6]:E0603 16:13:52.283000 57803 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     p2p_communication.recv_forward(
[rank6]:E0603 16:13:52.283000 57803 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 342, in recv_forward
[rank6]:E0603 16:13:52.283000 57803 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     input_tensor, _ = _communicate(
[rank6]:E0603 16:13:52.283000 57803 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 259, in _communicate
[rank6]:E0603 16:13:52.283000 57803 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     tensor_send_prev_req, tensor_recv_prev_req, tensor_send_next_req, tensor_recv_next_req = _run_p2pops(tensor_send_prev, tensor_send_next, tensor_recv_prev, tensor_recv_next, async_comm=async_comm)
[rank6]:E0603 16:13:52.283000 57803 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 97, in _run_p2pops
[rank6]:E0603 16:13:52.283000 57803 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     reqs = torch.distributed.batch_isend_irecv(ops)
[rank6]:E0603 16:13:52.283000 57803 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/distributed/distributed_c10d.py", line 2717, in batch_isend_irecv
[rank6]:E0603 16:13:52.283000 57803 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     p2p_op.op(
[rank6]:E0603 16:13:52.283000 57803 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/distributed/distributed_c10d.py", line 2411, in irecv
[rank6]:E0603 16:13:52.283000 57803 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     return group.recv([tensor], group_src, tag)
[rank6]:E0603 16:13:52.283000 57803 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] torch.distributed.DistBackendError: NCCL error in: /skishore/github/pytorch/torch/csrc/distributed/c10d/NCCLUtils.cpp:77, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.25.1
[rank6]:E0603 16:13:52.283000 57803 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
[rank6]:E0603 16:13:52.283000 57803 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Last error:
[rank6]:E0603 16:13:52.283000 57803 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Error while creating shared memory segment /dev/shm/nccl-5OXFxk (size 8257920), error: No space left on device (28)
[rank6]:E0603 16:13:52.283000 57803 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] 
[rank6]:E0603 16:13:52.283000 57803 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] To execute this test, run the following from the base repo dir:
[rank6]:E0603 16:13:52.283000 57803 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     PYTORCH_TEST_WITH_ROCM=1 python test_pipeline_parallel_fwd_bwd.py NcclPipelineParallelForwardBackwardTest.test_inference_pipelining_without_interleaving
[rank6]:E0603 16:13:52.283000 57803 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] 
[rank6]:E0603 16:13:52.283000 57803 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0
[rank6]:E0603 16:13:52.283000 57803 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]  exiting process 6 with exit code: 10
[rank1]:E0603 16:13:52.303000 57798 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Caught exception: 
[rank1]:E0603 16:13:52.303000 57798 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Traceback (most recent call last):
[rank1]:E0603 16:13:52.303000 57798 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/testing/_internal/common_distributed.py", line 734, in run_test
[rank1]:E0603 16:13:52.303000 57798 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     getattr(self, test_name)()
[rank1]:E0603 16:13:52.303000 57798 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/testing/_internal/common_distributed.py", line 607, in wrapper
[rank1]:E0603 16:13:52.303000 57798 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     fn()
[rank1]:E0603 16:13:52.303000 57798 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/testing/_internal/common_utils.py", line 3173, in wrapper
[rank1]:E0603 16:13:52.303000 57798 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     method(*args, **kwargs)
[rank1]:E0603 16:13:52.303000 57798 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py", line 244, in test_inference_pipelining_without_interleaving
[rank1]:E0603 16:13:52.303000 57798 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     self._forward_backward_test_impl(
[rank1]:E0603 16:13:52.303000 57798 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py", line 180, in _forward_backward_test_impl
[rank1]:E0603 16:13:52.303000 57798 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     loss = fwd_bwd_func(
[rank1]:E0603 16:13:52.303000 57798 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_without_interleaving.py", line 349, in forward_backward_pipelining_without_interleaving
[rank1]:E0603 16:13:52.303000 57798 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     send_forward(
[rank1]:E0603 16:13:52.303000 57798 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_without_interleaving.py", line 145, in send_forward
[rank1]:E0603 16:13:52.303000 57798 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     p2p_communication.send_forward(
[rank1]:E0603 16:13:52.303000 57798 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 401, in send_forward
[rank1]:E0603 16:13:52.303000 57798 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     _communicate(
[rank1]:E0603 16:13:52.303000 57798 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 259, in _communicate
[rank1]:E0603 16:13:52.303000 57798 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     tensor_send_prev_req, tensor_recv_prev_req, tensor_send_next_req, tensor_recv_next_req = _run_p2pops(tensor_send_prev, tensor_send_next, tensor_recv_prev, tensor_recv_next, async_comm=async_comm)
[rank1]:E0603 16:13:52.303000 57798 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 97, in _run_p2pops
[rank1]:E0603 16:13:52.303000 57798 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     reqs = torch.distributed.batch_isend_irecv(ops)
[rank1]:E0603 16:13:52.303000 57798 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/distributed/distributed_c10d.py", line 2717, in batch_isend_irecv
[rank1]:E0603 16:13:52.303000 57798 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     p2p_op.op(
[rank1]:E0603 16:13:52.303000 57798 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/distributed/distributed_c10d.py", line 2366, in isend
[rank1]:E0603 16:13:52.303000 57798 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     return group.send([tensor], group_dst, tag)
[rank1]:E0603 16:13:52.303000 57798 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] torch.distributed.DistBackendError: NCCL error in: /skishore/github/pytorch/torch/csrc/distributed/c10d/NCCLUtils.cpp:77, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.25.1
[rank1]:E0603 16:13:52.303000 57798 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
[rank1]:E0603 16:13:52.303000 57798 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Last error:
[rank1]:E0603 16:13:52.303000 57798 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Error while creating shared memory segment /dev/shm/nccl-G3jTgB (size 8257920), error: No space left on device (28)
[rank1]:E0603 16:13:52.303000 57798 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] 
[rank1]:E0603 16:13:52.303000 57798 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] To execute this test, run the following from the base repo dir:
[rank1]:E0603 16:13:52.303000 57798 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     PYTORCH_TEST_WITH_ROCM=1 python test_pipeline_parallel_fwd_bwd.py NcclPipelineParallelForwardBackwardTest.test_inference_pipelining_without_interleaving
[rank1]:E0603 16:13:52.303000 57798 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] 
[rank1]:E0603 16:13:52.303000 57798 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0
[rank1]:E0603 16:13:52.303000 57798 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]  exiting process 1 with exit code: 10
[dist init] rank = 1, world_size = 8
[rank3]:E0603 16:13:52.304000 57800 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Caught exception: 
[rank3]:E0603 16:13:52.304000 57800 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Traceback (most recent call last):
[rank3]:E0603 16:13:52.304000 57800 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/testing/_internal/common_distributed.py", line 734, in run_test
[rank3]:E0603 16:13:52.304000 57800 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     getattr(self, test_name)()
[rank3]:E0603 16:13:52.304000 57800 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/testing/_internal/common_distributed.py", line 607, in wrapper
[rank3]:E0603 16:13:52.304000 57800 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     fn()
[rank3]:E0603 16:13:52.304000 57800 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/testing/_internal/common_utils.py", line 3173, in wrapper
[rank3]:E0603 16:13:52.304000 57800 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     method(*args, **kwargs)
[rank3]:E0603 16:13:52.304000 57800 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py", line 244, in test_inference_pipelining_without_interleaving
[rank3]:E0603 16:13:52.304000 57800 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     self._forward_backward_test_impl(
[rank3]:E0603 16:13:52.304000 57800 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py", line 180, in _forward_backward_test_impl
[rank3]:E0603 16:13:52.304000 57800 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     loss = fwd_bwd_func(
[rank3]:E0603 16:13:52.304000 57800 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_without_interleaving.py", line 332, in forward_backward_pipelining_without_interleaving
[rank3]:E0603 16:13:52.304000 57800 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     input_tensor = recv_forward(
[rank3]:E0603 16:13:52.304000 57800 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_without_interleaving.py", line 99, in recv_forward
[rank3]:E0603 16:13:52.304000 57800 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     p2p_communication.recv_forward(
[rank3]:E0603 16:13:52.304000 57800 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 342, in recv_forward
[rank3]:E0603 16:13:52.304000 57800 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     input_tensor, _ = _communicate(
[rank3]:E0603 16:13:52.304000 57800 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 259, in _communicate
[rank3]:E0603 16:13:52.304000 57800 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     tensor_send_prev_req, tensor_recv_prev_req, tensor_send_next_req, tensor_recv_next_req = _run_p2pops(tensor_send_prev, tensor_send_next, tensor_recv_prev, tensor_recv_next, async_comm=async_comm)
[rank3]:E0603 16:13:52.304000 57800 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 97, in _run_p2pops
[rank3]:E0603 16:13:52.304000 57800 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     reqs = torch.distributed.batch_isend_irecv(ops)
[rank3]:E0603 16:13:52.304000 57800 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/distributed/distributed_c10d.py", line 2717, in batch_isend_irecv
[rank3]:E0603 16:13:52.304000 57800 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     p2p_op.op(
[rank3]:E0603 16:13:52.304000 57800 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/distributed/distributed_c10d.py", line 2411, in irecv
[rank3]:E0603 16:13:52.304000 57800 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     return group.recv([tensor], group_src, tag)
[rank3]:E0603 16:13:52.304000 57800 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] torch.distributed.DistBackendError: NCCL error in: /skishore/github/pytorch/torch/csrc/distributed/c10d/NCCLUtils.cpp:77, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.25.1
[rank3]:E0603 16:13:52.304000 57800 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
[rank3]:E0603 16:13:52.304000 57800 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Last error:
[rank3]:E0603 16:13:52.304000 57800 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Error while creating shared memory segment /dev/shm/nccl-UuQrwh (size 8257920), error: No space left on device (28)
[rank3]:E0603 16:13:52.304000 57800 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] 
[rank3]:E0603 16:13:52.304000 57800 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] To execute this test, run the following from the base repo dir:
[rank3]:E0603 16:13:52.304000 57800 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     PYTORCH_TEST_WITH_ROCM=1 python test_pipeline_parallel_fwd_bwd.py NcclPipelineParallelForwardBackwardTest.test_inference_pipelining_without_interleaving
[rank3]:E0603 16:13:52.304000 57800 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] 
[rank3]:E0603 16:13:52.304000 57800 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0
[rank3]:E0603 16:13:52.304000 57800 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]  exiting process 3 with exit code: 10
[rank7]:E0603 16:13:52.304000 57804 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Caught exception: 
[rank7]:E0603 16:13:52.304000 57804 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Traceback (most recent call last):
[rank7]:E0603 16:13:52.304000 57804 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/testing/_internal/common_distributed.py", line 734, in run_test
[rank7]:E0603 16:13:52.304000 57804 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     getattr(self, test_name)()
[rank7]:E0603 16:13:52.304000 57804 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/testing/_internal/common_distributed.py", line 607, in wrapper
[rank7]:E0603 16:13:52.304000 57804 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     fn()
[rank7]:E0603 16:13:52.304000 57804 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/testing/_internal/common_utils.py", line 3173, in wrapper
[rank7]:E0603 16:13:52.304000 57804 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     method(*args, **kwargs)
[rank7]:E0603 16:13:52.304000 57804 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py", line 244, in test_inference_pipelining_without_interleaving
[rank7]:E0603 16:13:52.304000 57804 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     self._forward_backward_test_impl(
[rank7]:E0603 16:13:52.304000 57804 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py", line 180, in _forward_backward_test_impl
[rank7]:E0603 16:13:52.304000 57804 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     loss = fwd_bwd_func(
[rank7]:E0603 16:13:52.304000 57804 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_without_interleaving.py", line 367, in forward_backward_pipelining_without_interleaving
[rank7]:E0603 16:13:52.304000 57804 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     input_tensor: List[Union[None, torch.Tensor, FutureTensor]] = recv_forward(tensor_shapes=recv_tensor_shapes, dtype=dtype, async_comm=async_comm)
[rank7]:E0603 16:13:52.304000 57804 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_without_interleaving.py", line 99, in recv_forward
[rank7]:E0603 16:13:52.304000 57804 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     p2p_communication.recv_forward(
[rank7]:E0603 16:13:52.304000 57804 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 342, in recv_forward
[rank7]:E0603 16:13:52.304000 57804 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     input_tensor, _ = _communicate(
[rank7]:E0603 16:13:52.304000 57804 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 259, in _communicate
[rank7]:E0603 16:13:52.304000 57804 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     tensor_send_prev_req, tensor_recv_prev_req, tensor_send_next_req, tensor_recv_next_req = _run_p2pops(tensor_send_prev, tensor_send_next, tensor_recv_prev, tensor_recv_next, async_comm=async_comm)
[rank7]:E0603 16:13:52.304000 57804 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 97, in _run_p2pops
[rank7]:E0603 16:13:52.304000 57804 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     reqs = torch.distributed.batch_isend_irecv(ops)
[rank7]:E0603 16:13:52.304000 57804 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/distributed/distributed_c10d.py", line 2717, in batch_isend_irecv
[rank7]:E0603 16:13:52.304000 57804 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     p2p_op.op(
[rank7]:E0603 16:13:52.304000 57804 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/distributed/distributed_c10d.py", line 2411, in irecv
[rank7]:E0603 16:13:52.304000 57804 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     return group.recv([tensor], group_src, tag)
[rank7]:E0603 16:13:52.304000 57804 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] torch.distributed.DistBackendError: NCCL error in: /skishore/github/pytorch/torch/csrc/distributed/c10d/NCCLUtils.cpp:77, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.25.1
[rank7]:E0603 16:13:52.304000 57804 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
[rank7]:E0603 16:13:52.304000 57804 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Last error:
[rank7]:E0603 16:13:52.304000 57804 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Error while creating shared memory segment /dev/shm/nccl-SMCF6k (size 8257920), error: No space left on device (28)
[rank7]:E0603 16:13:52.304000 57804 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] 
[rank7]:E0603 16:13:52.304000 57804 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] To execute this test, run the following from the base repo dir:
[rank7]:E0603 16:13:52.304000 57804 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     PYTORCH_TEST_WITH_ROCM=1 python test_pipeline_parallel_fwd_bwd.py NcclPipelineParallelForwardBackwardTest.test_inference_pipelining_without_interleaving
[rank7]:E0603 16:13:52.304000 57804 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] 
[rank7]:E0603 16:13:52.304000 57804 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0
[rank7]:E0603 16:13:52.304000 57804 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]  exiting process 7 with exit code: 10
[dist init] rank = 3, world_size = 8
[rank5]:E0603 16:13:52.304000 57802 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Caught exception: 
[rank5]:E0603 16:13:52.304000 57802 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Traceback (most recent call last):
[rank5]:E0603 16:13:52.304000 57802 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/testing/_internal/common_distributed.py", line 734, in run_test
[rank5]:E0603 16:13:52.304000 57802 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     getattr(self, test_name)()
[rank5]:E0603 16:13:52.304000 57802 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/testing/_internal/common_distributed.py", line 607, in wrapper
[rank5]:E0603 16:13:52.304000 57802 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     fn()
[rank5]:E0603 16:13:52.304000 57802 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/testing/_internal/common_utils.py", line 3173, in wrapper
[rank5]:E0603 16:13:52.304000 57802 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     method(*args, **kwargs)
[rank5]:E0603 16:13:52.304000 57802 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py", line 244, in test_inference_pipelining_without_interleaving
[rank5]:E0603 16:13:52.304000 57802 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     self._forward_backward_test_impl(
[rank5]:E0603 16:13:52.304000 57802 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py", line 180, in _forward_backward_test_impl
[rank5]:E0603 16:13:52.304000 57802 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     loss = fwd_bwd_func(
[rank5]:E0603 16:13:52.304000 57802 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_without_interleaving.py", line 332, in forward_backward_pipelining_without_interleaving
[rank5]:E0603 16:13:52.304000 57802 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     input_tensor = recv_forward(
[rank5]:E0603 16:13:52.304000 57802 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_without_interleaving.py", line 99, in recv_forward
[rank5]:E0603 16:13:52.304000 57802 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     p2p_communication.recv_forward(
[rank5]:E0603 16:13:52.304000 57802 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 342, in recv_forward
[rank5]:E0603 16:13:52.304000 57802 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     input_tensor, _ = _communicate(
[rank5]:E0603 16:13:52.304000 57802 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 259, in _communicate
[rank5]:E0603 16:13:52.304000 57802 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     tensor_send_prev_req, tensor_recv_prev_req, tensor_send_next_req, tensor_recv_next_req = _run_p2pops(tensor_send_prev, tensor_send_next, tensor_recv_prev, tensor_recv_next, async_comm=async_comm)
[rank5]:E0603 16:13:52.304000 57802 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 97, in _run_p2pops
[rank5]:E0603 16:13:52.304000 57802 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     reqs = torch.distributed.batch_isend_irecv(ops)
[rank5]:E0603 16:13:52.304000 57802 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/distributed/distributed_c10d.py", line 2717, in batch_isend_irecv
[rank5]:E0603 16:13:52.304000 57802 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     p2p_op.op(
[rank5]:E0603 16:13:52.304000 57802 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/distributed/distributed_c10d.py", line 2411, in irecv
[rank5]:E0603 16:13:52.304000 57802 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     return group.recv([tensor], group_src, tag)
[rank5]:E0603 16:13:52.304000 57802 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] torch.distributed.DistBackendError: NCCL error in: /skishore/github/pytorch/torch/csrc/distributed/c10d/NCCLUtils.cpp:77, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.25.1
[rank5]:E0603 16:13:52.304000 57802 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
[rank5]:E0603 16:13:52.304000 57802 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Last error:
[rank5]:E0603 16:13:52.304000 57802 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Error while creating shared memory segment /dev/shm/nccl-eVwAPm (size 8257920), error: No space left on device (28)
[rank5]:E0603 16:13:52.304000 57802 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] 
[rank5]:E0603 16:13:52.304000 57802 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] To execute this test, run the following from the base repo dir:
[rank5]:E0603 16:13:52.304000 57802 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     PYTORCH_TEST_WITH_ROCM=1 python test_pipeline_parallel_fwd_bwd.py NcclPipelineParallelForwardBackwardTest.test_inference_pipelining_without_interleaving
[rank5]:E0603 16:13:52.304000 57802 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] 
[rank5]:E0603 16:13:52.304000 57802 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0
[rank5]:E0603 16:13:52.304000 57802 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]  exiting process 5 with exit code: 10
[dist init] rank = 7, world_size = 8
[dist init] rank = 5, world_size = 8
[rank2]:[W603 16:13:52.645901574 ProcessGroupNCCL.cpp:1553] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank4]:[W603 16:13:52.659257967 ProcessGroupNCCL.cpp:1553] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank0]:[W603 16:13:52.771542944 ProcessGroupNCCL.cpp:1553] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank6]:[W603 16:13:52.781064435 ProcessGroupNCCL.cpp:1553] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank1]:[W603 16:13:53.269871734 ProcessGroupNCCL.cpp:1553] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank3]:[W603 16:13:53.278681750 ProcessGroupNCCL.cpp:1553] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank7]:[W603 16:13:53.365874516 ProcessGroupNCCL.cpp:1553] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank5]:[W603 16:13:53.374848385 ProcessGroupNCCL.cpp:1553] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
ERROR
test_inference_pipelining_without_interleaving_ucc_for_p2p (test_pipeline_parallel_fwd_bwd.NcclPipelineParallelForwardBackwardTest) ... Process 1 terminated with exit code 10, terminating remaining processes.
[rank0]:[W603 16:14:15.213860590 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank6]:[W603 16:14:15.229402887 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 6]  using GPU 6 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank4]:[W603 16:14:15.237331454 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 4]  using GPU 4 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank2]:[W603 16:14:15.240780322 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank7]:[W603 16:14:15.246388306 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 7]  using GPU 7 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank1]:[W603 16:14:15.251613170 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank5]:[W603 16:14:15.262122898 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 5]  using GPU 5 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank3]:[W603 16:14:15.264435740 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[dist init] rank = 0, world_size = 8
[dist init] rank = 3, world_size = 8
[dist init] rank = 4, world_size = 8
[dist init] rank = 5, world_size = 8
[dist init] rank = 2, world_size = 8
[dist init] rank = 7, world_size = 8
[dist init] rank = 1, world_size = 8
[dist init] rank = 6, world_size = 8
[rank0]:[W603 16:14:24.075517383 ProcessGroupNCCL.cpp:1553] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
skipped 'Test skipped at subprocess level, look at subprocess log for skip reason'
test_learning_async_pipelining_with_interleaving (test_pipeline_parallel_fwd_bwd.NcclPipelineParallelForwardBackwardTest) ... [rank0]:[W603 16:14:36.025146016 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank3]:[W603 16:14:37.810567302 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank2]:[W603 16:14:37.830866809 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank1]:[W603 16:14:37.831099595 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank4]:[W603 16:14:37.833581789 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 4]  using GPU 4 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank7]:[W603 16:14:37.850227383 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 7]  using GPU 7 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank6]:[W603 16:14:37.852124078 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 6]  using GPU 6 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank5]:[W603 16:14:37.852242875 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 5]  using GPU 5 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[dist init] rank = 4, world_size = 8
 > number of parameters on (tensor, pipeline) model parallel rank (0, 2): 2112
[dist init] rank = 2, world_size = 8
 > number of parameters on (tensor, pipeline) model parallel rank (0, 1): 2112
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/utils.py:81: UserWarning: This function is only for unittest
  warnings.warn("This function is only for unittest")
[dist init] rank = 6, world_size = 8
 > number of parameters on (tensor, pipeline) model parallel rank (0, 3): 2112
[dist init] rank = 0, world_size = 8
 > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 2112
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/common.py:295: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/common.py:295: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/common.py:295: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/common.py:295: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/common.py:295: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/common.py:295: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/common.py:295: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/common.py:295: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(
 > number of parameters on (tensor, pipeline) model parallel rank (0, 1): 2112
 > number of parameters on (tensor, pipeline) model parallel rank (0, 3): 2112
 > number of parameters on (tensor, pipeline) model parallel rank (0, 2): 2112
 > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 2112
 > number of parameters on (tensor, pipeline) model parallel rank (0, 3): 2112
 > number of parameters on (tensor, pipeline) model parallel rank (0, 2): 2112
 > number of parameters on (tensor, pipeline) model parallel rank (0, 1): 2112
 > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 2112
/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py:208: UserWarning: Converting a tensor with requires_grad=True to a scalar may lead to unexpected behavior.
Consider using tensor.detach() first. (Triggered internally at /skishore/github/pytorch/aten/src/ATen/native/Scalar.cpp:22.)
  self.assertEqual(x.item() / microbatch_size, target_loss.item())
 > number of parameters on (tensor, pipeline) model parallel rank (0, 3): 2112
/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py:208: UserWarning: Converting a tensor with requires_grad=True to a scalar may lead to unexpected behavior.
Consider using tensor.detach() first. (Triggered internally at /skishore/github/pytorch/aten/src/ATen/native/Scalar.cpp:22.)
  self.assertEqual(x.item() / microbatch_size, target_loss.item())
 > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 2112
 > number of parameters on (tensor, pipeline) model parallel rank (0, 2): 2112
 > number of parameters on (tensor, pipeline) model parallel rank (0, 1): 2112
/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py:126: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  torch.cuda.amp.GradScaler(init_scale=4.0)
 > number of parameters on (tensor, pipeline) model parallel rank (0, 3): 2112
/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py:126: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  torch.cuda.amp.GradScaler(init_scale=4.0)
/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py:126: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  torch.cuda.amp.GradScaler(init_scale=4.0)
/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py:126: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  torch.cuda.amp.GradScaler(init_scale=4.0)
 > number of parameters on (tensor, pipeline) model parallel rank (0, 1): 2112
/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py:126: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  torch.cuda.amp.GradScaler(init_scale=4.0)
/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py:126: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  torch.cuda.amp.GradScaler(init_scale=4.0)
/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py:126: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  torch.cuda.amp.GradScaler(init_scale=4.0)
 > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 2112
/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py:126: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  torch.cuda.amp.GradScaler(init_scale=4.0)
 > number of parameters on (tensor, pipeline) model parallel rank (0, 2): 2112
 > number of parameters on (tensor, pipeline) model parallel rank (0, 3): 2112
 > number of parameters on (tensor, pipeline) model parallel rank (0, 2): 2112
 > number of parameters on (tensor, pipeline) model parallel rank (0, 1): 2112
 > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 2112
 > number of parameters on (tensor, pipeline) model parallel rank (0, 2): 2112
 > number of parameters on (tensor, pipeline) model parallel rank (0, 3): 2112
 > number of parameters on (tensor, pipeline) model parallel rank (0, 1): 2112
 > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 2112
 > number of parameters on (tensor, pipeline) model parallel rank (0, 3): 2112
 > number of parameters on (tensor, pipeline) model parallel rank (0, 2): 2112
 > number of parameters on (tensor, pipeline) model parallel rank (0, 1): 2112
 > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 2112
[rank0]:E0603 16:16:56.226000 62443 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Caught exception: 
[rank0]:E0603 16:16:56.226000 62443 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Traceback (most recent call last):
[rank0]:E0603 16:16:56.226000 62443 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/testing/_internal/common_distributed.py", line 734, in run_test
[rank0]:E0603 16:16:56.226000 62443 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     getattr(self, test_name)()
[rank0]:E0603 16:16:56.226000 62443 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/testing/_internal/common_distributed.py", line 607, in wrapper
[rank0]:E0603 16:16:56.226000 62443 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     fn()
[rank0]:E0603 16:16:56.226000 62443 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/testing/_internal/common_utils.py", line 3173, in wrapper
[rank0]:E0603 16:16:56.226000 62443 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     method(*args, **kwargs)
[rank0]:E0603 16:16:56.226000 62443 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py", line 272, in test_learning_async_pipelining_with_interleaving
[rank0]:E0603 16:16:56.226000 62443 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     self._forward_backward_test_impl(
[rank0]:E0603 16:16:56.226000 62443 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py", line 180, in _forward_backward_test_impl
[rank0]:E0603 16:16:56.226000 62443 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     loss = fwd_bwd_func(
[rank0]:E0603 16:16:56.226000 62443 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_with_interleaving.py", line 276, in _forward_backward_pipelining_with_interleaving
[rank0]:E0603 16:16:56.226000 62443 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     input_tensor = p2p_communication.send_forward_recv_forward(
[rank0]:E0603 16:16:56.226000 62443 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 515, in send_forward_recv_forward
[rank0]:E0603 16:16:56.226000 62443 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     input_tensor, _ = _communicate(
[rank0]:E0603 16:16:56.226000 62443 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 259, in _communicate
[rank0]:E0603 16:16:56.226000 62443 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     tensor_send_prev_req, tensor_recv_prev_req, tensor_send_next_req, tensor_recv_next_req = _run_p2pops(tensor_send_prev, tensor_send_next, tensor_recv_prev, tensor_recv_next, async_comm=async_comm)
[rank0]:E0603 16:16:56.226000 62443 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 97, in _run_p2pops
[rank0]:E0603 16:16:56.226000 62443 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     reqs = torch.distributed.batch_isend_irecv(ops)
[rank0]:E0603 16:16:56.226000 62443 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/distributed/distributed_c10d.py", line 2717, in batch_isend_irecv
[rank0]:E0603 16:16:56.226000 62443 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     p2p_op.op(
[rank0]:E0603 16:16:56.226000 62443 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/distributed/distributed_c10d.py", line 2366, in isend
[rank0]:E0603 16:16:56.226000 62443 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     return group.send([tensor], group_dst, tag)
[rank0]:E0603 16:16:56.226000 62443 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] torch.distributed.DistBackendError: NCCL error in: /skishore/github/pytorch/torch/csrc/distributed/c10d/NCCLUtils.cpp:77, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.25.1
[rank0]:E0603 16:16:56.226000 62443 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
[rank0]:E0603 16:16:56.226000 62443 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Last error:
[rank0]:E0603 16:16:56.226000 62443 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Error while creating shared memory segment /dev/shm/nccl-BS3aqb (size 8257920), error: No space left on device (28)
[rank0]:E0603 16:16:56.226000 62443 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] 
[rank0]:E0603 16:16:56.226000 62443 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] To execute this test, run the following from the base repo dir:
[rank0]:E0603 16:16:56.226000 62443 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     PYTORCH_TEST_WITH_ROCM=1 python test_pipeline_parallel_fwd_bwd.py NcclPipelineParallelForwardBackwardTest.test_learning_async_pipelining_with_interleaving
[rank0]:E0603 16:16:56.226000 62443 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] 
[rank0]:E0603 16:16:56.226000 62443 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0
[rank0]:E0603 16:16:56.226000 62443 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]  exiting process 0 with exit code: 10
[rank2]:E0603 16:16:56.226000 62445 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Caught exception: 
[rank2]:E0603 16:16:56.226000 62445 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Traceback (most recent call last):
[rank2]:E0603 16:16:56.226000 62445 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/testing/_internal/common_distributed.py", line 734, in run_test
[rank2]:E0603 16:16:56.226000 62445 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     getattr(self, test_name)()
[rank2]:E0603 16:16:56.226000 62445 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/testing/_internal/common_distributed.py", line 607, in wrapper
[rank2]:E0603 16:16:56.226000 62445 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     fn()
[rank2]:E0603 16:16:56.226000 62445 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/testing/_internal/common_utils.py", line 3173, in wrapper
[rank2]:E0603 16:16:56.226000 62445 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     method(*args, **kwargs)
[rank2]:E0603 16:16:56.226000 62445 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py", line 272, in test_learning_async_pipelining_with_interleaving
[rank2]:E0603 16:16:56.226000 62445 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     self._forward_backward_test_impl(
[rank2]:E0603 16:16:56.226000 62445 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py", line 180, in _forward_backward_test_impl
[rank2]:E0603 16:16:56.226000 62445 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     loss = fwd_bwd_func(
[rank2]:E0603 16:16:56.226000 62445 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_with_interleaving.py", line 219, in _forward_backward_pipelining_with_interleaving
[rank2]:E0603 16:16:56.226000 62445 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     p2p_communication.recv_forward(
[rank2]:E0603 16:16:56.226000 62445 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 342, in recv_forward
[rank2]:E0603 16:16:56.226000 62445 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     input_tensor, _ = _communicate(
[rank2]:E0603 16:16:56.226000 62445 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 259, in _communicate
[rank2]:E0603 16:16:56.226000 62445 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     tensor_send_prev_req, tensor_recv_prev_req, tensor_send_next_req, tensor_recv_next_req = _run_p2pops(tensor_send_prev, tensor_send_next, tensor_recv_prev, tensor_recv_next, async_comm=async_comm)
[rank2]:E0603 16:16:56.226000 62445 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 97, in _run_p2pops
[rank2]:E0603 16:16:56.226000 62445 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     reqs = torch.distributed.batch_isend_irecv(ops)
[rank2]:E0603 16:16:56.226000 62445 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/distributed/distributed_c10d.py", line 2717, in batch_isend_irecv
[rank2]:E0603 16:16:56.226000 62445 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     p2p_op.op(
[rank2]:E0603 16:16:56.226000 62445 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/distributed/distributed_c10d.py", line 2411, in irecv
[rank2]:E0603 16:16:56.226000 62445 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     return group.recv([tensor], group_src, tag)
[rank2]:E0603 16:16:56.226000 62445 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] torch.distributed.DistBackendError: NCCL error in: /skishore/github/pytorch/torch/csrc/distributed/c10d/NCCLUtils.cpp:77, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.25.1
[rank2]:E0603 16:16:56.226000 62445 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
[rank2]:E0603 16:16:56.226000 62445 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Last error:
[rank2]:E0603 16:16:56.226000 62445 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Error while creating shared memory segment /dev/shm/nccl-vuRJwT (size 8257920), error: No space left on device (28)
[rank2]:E0603 16:16:56.226000 62445 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] 
[rank2]:E0603 16:16:56.226000 62445 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] To execute this test, run the following from the base repo dir:
[rank2]:E0603 16:16:56.226000 62445 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     PYTORCH_TEST_WITH_ROCM=1 python test_pipeline_parallel_fwd_bwd.py NcclPipelineParallelForwardBackwardTest.test_learning_async_pipelining_with_interleaving
[rank2]:E0603 16:16:56.226000 62445 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] 
[rank2]:E0603 16:16:56.226000 62445 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0
[rank2]:E0603 16:16:56.226000 62445 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]  exiting process 2 with exit code: 10
[rank4]:E0603 16:16:56.226000 62447 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Caught exception: 
[rank4]:E0603 16:16:56.226000 62447 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Traceback (most recent call last):
[rank4]:E0603 16:16:56.226000 62447 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/testing/_internal/common_distributed.py", line 734, in run_test
[rank4]:E0603 16:16:56.226000 62447 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     getattr(self, test_name)()
[rank4]:E0603 16:16:56.226000 62447 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/testing/_internal/common_distributed.py", line 607, in wrapper
[rank4]:E0603 16:16:56.226000 62447 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     fn()
[rank4]:E0603 16:16:56.226000 62447 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/testing/_internal/common_utils.py", line 3173, in wrapper
[rank4]:E0603 16:16:56.226000 62447 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     method(*args, **kwargs)
[rank4]:E0603 16:16:56.226000 62447 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py", line 272, in test_learning_async_pipelining_with_interleaving
[rank4]:E0603 16:16:56.226000 62447 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     self._forward_backward_test_impl(
[rank4]:E0603 16:16:56.226000 62447 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py", line 180, in _forward_backward_test_impl
[rank4]:E0603 16:16:56.226000 62447 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     loss = fwd_bwd_func(
[rank4]:E0603 16:16:56.226000 62447 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_with_interleaving.py", line 219, in _forward_backward_pipelining_with_interleaving
[rank4]:E0603 16:16:56.226000 62447 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     p2p_communication.recv_forward(
[rank4]:E0603 16:16:56.226000 62447 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 342, in recv_forward
[rank4]:E0603 16:16:56.226000 62447 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     input_tensor, _ = _communicate(
[rank4]:E0603 16:16:56.226000 62447 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 259, in _communicate
[rank4]:E0603 16:16:56.226000 62447 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     tensor_send_prev_req, tensor_recv_prev_req, tensor_send_next_req, tensor_recv_next_req = _run_p2pops(tensor_send_prev, tensor_send_next, tensor_recv_prev, tensor_recv_next, async_comm=async_comm)
[rank4]:E0603 16:16:56.226000 62447 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 97, in _run_p2pops
[rank4]:E0603 16:16:56.226000 62447 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     reqs = torch.distributed.batch_isend_irecv(ops)
[rank4]:E0603 16:16:56.226000 62447 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/distributed/distributed_c10d.py", line 2717, in batch_isend_irecv
[rank4]:E0603 16:16:56.226000 62447 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     p2p_op.op(
[rank4]:E0603 16:16:56.226000 62447 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/distributed/distributed_c10d.py", line 2411, in irecv
[rank4]:E0603 16:16:56.226000 62447 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     return group.recv([tensor], group_src, tag)
[rank4]:E0603 16:16:56.226000 62447 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] torch.distributed.DistBackendError: NCCL error in: /skishore/github/pytorch/torch/csrc/distributed/c10d/NCCLUtils.cpp:77, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.25.1
[rank4]:E0603 16:16:56.226000 62447 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
[rank4]:E0603 16:16:56.226000 62447 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Last error:
[rank4]:E0603 16:16:56.226000 62447 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Error while creating shared memory segment /dev/shm/nccl-nVYHSK (size 8257920), error: No space left on device (28)
[rank4]:E0603 16:16:56.226000 62447 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] 
[rank4]:E0603 16:16:56.226000 62447 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] To execute this test, run the following from the base repo dir:
[rank4]:E0603 16:16:56.226000 62447 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     PYTORCH_TEST_WITH_ROCM=1 python test_pipeline_parallel_fwd_bwd.py NcclPipelineParallelForwardBackwardTest.test_learning_async_pipelining_with_interleaving
[rank4]:E0603 16:16:56.226000 62447 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] 
[rank4]:E0603 16:16:56.226000 62447 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0
[rank4]:E0603 16:16:56.226000 62447 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]  exiting process 4 with exit code: 10
[rank6]:E0603 16:16:56.226000 62449 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Caught exception: 
[rank6]:E0603 16:16:56.226000 62449 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Traceback (most recent call last):
[rank6]:E0603 16:16:56.226000 62449 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/testing/_internal/common_distributed.py", line 734, in run_test
[rank6]:E0603 16:16:56.226000 62449 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     getattr(self, test_name)()
[rank6]:E0603 16:16:56.226000 62449 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/testing/_internal/common_distributed.py", line 607, in wrapper
[rank6]:E0603 16:16:56.226000 62449 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     fn()
[rank6]:E0603 16:16:56.226000 62449 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/testing/_internal/common_utils.py", line 3173, in wrapper
[rank6]:E0603 16:16:56.226000 62449 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     method(*args, **kwargs)
[rank6]:E0603 16:16:56.226000 62449 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py", line 272, in test_learning_async_pipelining_with_interleaving
[rank6]:E0603 16:16:56.226000 62449 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     self._forward_backward_test_impl(
[rank6]:E0603 16:16:56.226000 62449 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py", line 180, in _forward_backward_test_impl
[rank6]:E0603 16:16:56.226000 62449 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     loss = fwd_bwd_func(
[rank6]:E0603 16:16:56.226000 62449 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_with_interleaving.py", line 219, in _forward_backward_pipelining_with_interleaving
[rank6]:E0603 16:16:56.226000 62449 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     p2p_communication.recv_forward(
[rank6]:E0603 16:16:56.226000 62449 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 342, in recv_forward
[rank6]:E0603 16:16:56.226000 62449 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     input_tensor, _ = _communicate(
[rank6]:E0603 16:16:56.226000 62449 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 259, in _communicate
[rank6]:E0603 16:16:56.226000 62449 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     tensor_send_prev_req, tensor_recv_prev_req, tensor_send_next_req, tensor_recv_next_req = _run_p2pops(tensor_send_prev, tensor_send_next, tensor_recv_prev, tensor_recv_next, async_comm=async_comm)
[rank6]:E0603 16:16:56.226000 62449 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 97, in _run_p2pops
[rank6]:E0603 16:16:56.226000 62449 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     reqs = torch.distributed.batch_isend_irecv(ops)
[rank6]:E0603 16:16:56.226000 62449 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/distributed/distributed_c10d.py", line 2717, in batch_isend_irecv
[rank6]:E0603 16:16:56.226000 62449 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     p2p_op.op(
[rank6]:E0603 16:16:56.226000 62449 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/distributed/distributed_c10d.py", line 2411, in irecv
[rank6]:E0603 16:16:56.226000 62449 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     return group.recv([tensor], group_src, tag)
[rank6]:E0603 16:16:56.226000 62449 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] torch.distributed.DistBackendError: NCCL error in: /skishore/github/pytorch/torch/csrc/distributed/c10d/NCCLUtils.cpp:77, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.25.1
[rank6]:E0603 16:16:56.226000 62449 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
[rank6]:E0603 16:16:56.226000 62449 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Last error:
[rank6]:E0603 16:16:56.226000 62449 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Error while creating shared memory segment /dev/shm/nccl-kcfrRq (size 8257920), error: No space left on device (28)
[rank6]:E0603 16:16:56.226000 62449 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] 
[rank6]:E0603 16:16:56.226000 62449 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] To execute this test, run the following from the base repo dir:
[rank6]:E0603 16:16:56.226000 62449 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     PYTORCH_TEST_WITH_ROCM=1 python test_pipeline_parallel_fwd_bwd.py NcclPipelineParallelForwardBackwardTest.test_learning_async_pipelining_with_interleaving
[rank6]:E0603 16:16:56.226000 62449 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] 
[rank6]:E0603 16:16:56.226000 62449 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0
[rank6]:E0603 16:16:56.226000 62449 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]  exiting process 6 with exit code: 10
[rank5]:E0603 16:16:56.348000 62448 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Caught exception: 
[rank5]:E0603 16:16:56.348000 62448 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Traceback (most recent call last):
[rank5]:E0603 16:16:56.348000 62448 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/testing/_internal/common_distributed.py", line 734, in run_test
[rank5]:E0603 16:16:56.348000 62448 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     getattr(self, test_name)()
[rank5]:E0603 16:16:56.348000 62448 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/testing/_internal/common_distributed.py", line 607, in wrapper
[rank5]:E0603 16:16:56.348000 62448 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     fn()
[rank5]:E0603 16:16:56.348000 62448 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/testing/_internal/common_utils.py", line 3173, in wrapper
[rank5]:E0603 16:16:56.348000 62448 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     method(*args, **kwargs)
[rank5]:E0603 16:16:56.348000 62448 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py", line 272, in test_learning_async_pipelining_with_interleaving
[rank5]:E0603 16:16:56.348000 62448 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     self._forward_backward_test_impl(
[rank5]:E0603 16:16:56.348000 62448 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py", line 180, in _forward_backward_test_impl
[rank5]:E0603 16:16:56.348000 62448 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     loss = fwd_bwd_func(
[rank5]:E0603 16:16:56.348000 62448 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_with_interleaving.py", line 219, in _forward_backward_pipelining_with_interleaving
[rank5]:E0603 16:16:56.348000 62448 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     p2p_communication.recv_forward(
[rank5]:E0603 16:16:56.348000 62448 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 342, in recv_forward
[rank5]:E0603 16:16:56.348000 62448 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     input_tensor, _ = _communicate(
[rank5]:E0603 16:16:56.348000 62448 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 259, in _communicate
[rank5]:E0603 16:16:56.348000 62448 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     tensor_send_prev_req, tensor_recv_prev_req, tensor_send_next_req, tensor_recv_next_req = _run_p2pops(tensor_send_prev, tensor_send_next, tensor_recv_prev, tensor_recv_next, async_comm=async_comm)
[rank5]:E0603 16:16:56.348000 62448 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 97, in _run_p2pops
[rank5]:E0603 16:16:56.348000 62448 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     reqs = torch.distributed.batch_isend_irecv(ops)
[rank5]:E0603 16:16:56.348000 62448 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/distributed/distributed_c10d.py", line 2717, in batch_isend_irecv
[rank5]:E0603 16:16:56.348000 62448 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     p2p_op.op(
[rank5]:E0603 16:16:56.348000 62448 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/distributed/distributed_c10d.py", line 2411, in irecv
[rank5]:E0603 16:16:56.348000 62448 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     return group.recv([tensor], group_src, tag)
[rank5]:E0603 16:16:56.348000 62448 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] torch.distributed.DistBackendError: NCCL error in: /skishore/github/pytorch/torch/csrc/distributed/c10d/NCCLUtils.cpp:77, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.25.1
[rank5]:E0603 16:16:56.348000 62448 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
[rank5]:E0603 16:16:56.348000 62448 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Last error:
[rank5]:E0603 16:16:56.348000 62448 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Error while creating shared memory segment /dev/shm/nccl-fTb69h (size 8257920), error: No space left on device (28)
[rank5]:E0603 16:16:56.348000 62448 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] 
[rank5]:E0603 16:16:56.348000 62448 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] To execute this test, run the following from the base repo dir:
[rank5]:E0603 16:16:56.348000 62448 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     PYTORCH_TEST_WITH_ROCM=1 python test_pipeline_parallel_fwd_bwd.py NcclPipelineParallelForwardBackwardTest.test_learning_async_pipelining_with_interleaving
[rank5]:E0603 16:16:56.348000 62448 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] 
[rank5]:E0603 16:16:56.348000 62448 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0
[rank5]:E0603 16:16:56.348000 62448 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]  exiting process 5 with exit code: 10
[dist init] rank = 5, world_size = 8
[rank7]:E0603 16:16:56.349000 62450 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Caught exception: 
[rank7]:E0603 16:16:56.349000 62450 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Traceback (most recent call last):
[rank7]:E0603 16:16:56.349000 62450 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/testing/_internal/common_distributed.py", line 734, in run_test
[rank7]:E0603 16:16:56.349000 62450 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     getattr(self, test_name)()
[rank7]:E0603 16:16:56.349000 62450 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/testing/_internal/common_distributed.py", line 607, in wrapper
[rank7]:E0603 16:16:56.349000 62450 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     fn()
[rank7]:E0603 16:16:56.349000 62450 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/testing/_internal/common_utils.py", line 3173, in wrapper
[rank7]:E0603 16:16:56.349000 62450 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     method(*args, **kwargs)
[rank7]:E0603 16:16:56.349000 62450 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py", line 272, in test_learning_async_pipelining_with_interleaving
[rank7]:E0603 16:16:56.349000 62450 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     self._forward_backward_test_impl(
[rank7]:E0603 16:16:56.349000 62450 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py", line 180, in _forward_backward_test_impl
[rank7]:E0603 16:16:56.349000 62450 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     loss = fwd_bwd_func(
[rank7]:E0603 16:16:56.349000 62450 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_with_interleaving.py", line 219, in _forward_backward_pipelining_with_interleaving
[rank7]:E0603 16:16:56.349000 62450 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     p2p_communication.recv_forward(
[rank7]:E0603 16:16:56.349000 62450 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 342, in recv_forward
[rank7]:E0603 16:16:56.349000 62450 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     input_tensor, _ = _communicate(
[rank7]:E0603 16:16:56.349000 62450 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 259, in _communicate
[rank7]:E0603 16:16:56.349000 62450 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     tensor_send_prev_req, tensor_recv_prev_req, tensor_send_next_req, tensor_recv_next_req = _run_p2pops(tensor_send_prev, tensor_send_next, tensor_recv_prev, tensor_recv_next, async_comm=async_comm)
[rank7]:E0603 16:16:56.349000 62450 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 97, in _run_p2pops
[rank7]:E0603 16:16:56.349000 62450 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     reqs = torch.distributed.batch_isend_irecv(ops)
[rank7]:E0603 16:16:56.349000 62450 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/distributed/distributed_c10d.py", line 2717, in batch_isend_irecv
[rank7]:E0603 16:16:56.349000 62450 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     p2p_op.op(
[rank7]:E0603 16:16:56.349000 62450 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/distributed/distributed_c10d.py", line 2411, in irecv
[rank7]:E0603 16:16:56.349000 62450 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     return group.recv([tensor], group_src, tag)
[rank7]:E0603 16:16:56.349000 62450 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] torch.distributed.DistBackendError: NCCL error in: /skishore/github/pytorch/torch/csrc/distributed/c10d/NCCLUtils.cpp:77, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.25.1
[rank7]:E0603 16:16:56.349000 62450 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
[rank7]:E0603 16:16:56.349000 62450 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Last error:
[rank7]:E0603 16:16:56.349000 62450 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Error while creating shared memory segment /dev/shm/nccl-nWTLTe (size 8257920), error: No space left on device (28)
[rank7]:E0603 16:16:56.349000 62450 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] 
[rank7]:E0603 16:16:56.349000 62450 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] To execute this test, run the following from the base repo dir:
[rank7]:E0603 16:16:56.349000 62450 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     PYTORCH_TEST_WITH_ROCM=1 python test_pipeline_parallel_fwd_bwd.py NcclPipelineParallelForwardBackwardTest.test_learning_async_pipelining_with_interleaving
[rank7]:E0603 16:16:56.349000 62450 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] 
[rank7]:E0603 16:16:56.349000 62450 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0
[rank7]:E0603 16:16:56.349000 62450 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]  exiting process 7 with exit code: 10
[dist init] rank = 7, world_size = 8
[rank1]:E0603 16:16:56.349000 62444 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Caught exception: 
[rank1]:E0603 16:16:56.349000 62444 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Traceback (most recent call last):
[rank1]:E0603 16:16:56.349000 62444 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/testing/_internal/common_distributed.py", line 734, in run_test
[rank1]:E0603 16:16:56.349000 62444 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     getattr(self, test_name)()
[rank1]:E0603 16:16:56.349000 62444 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/testing/_internal/common_distributed.py", line 607, in wrapper
[rank1]:E0603 16:16:56.349000 62444 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     fn()
[rank1]:E0603 16:16:56.349000 62444 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/testing/_internal/common_utils.py", line 3173, in wrapper
[rank1]:E0603 16:16:56.349000 62444 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     method(*args, **kwargs)
[rank1]:E0603 16:16:56.349000 62444 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py", line 272, in test_learning_async_pipelining_with_interleaving
[rank1]:E0603 16:16:56.349000 62444 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     self._forward_backward_test_impl(
[rank1]:E0603 16:16:56.349000 62444 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py", line 180, in _forward_backward_test_impl
[rank1]:E0603 16:16:56.349000 62444 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     loss = fwd_bwd_func(
[rank1]:E0603 16:16:56.349000 62444 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_with_interleaving.py", line 276, in _forward_backward_pipelining_with_interleaving
[rank1]:E0603 16:16:56.349000 62444 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     input_tensor = p2p_communication.send_forward_recv_forward(
[rank1]:E0603 16:16:56.349000 62444 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 515, in send_forward_recv_forward
[rank1]:E0603 16:16:56.349000 62444 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     input_tensor, _ = _communicate(
[rank1]:E0603 16:16:56.349000 62444 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 259, in _communicate
[rank1]:E0603 16:16:56.349000 62444 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     tensor_send_prev_req, tensor_recv_prev_req, tensor_send_next_req, tensor_recv_next_req = _run_p2pops(tensor_send_prev, tensor_send_next, tensor_recv_prev, tensor_recv_next, async_comm=async_comm)
[rank1]:E0603 16:16:56.349000 62444 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 97, in _run_p2pops
[rank1]:E0603 16:16:56.349000 62444 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     reqs = torch.distributed.batch_isend_irecv(ops)
[rank1]:E0603 16:16:56.349000 62444 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/distributed/distributed_c10d.py", line 2717, in batch_isend_irecv
[rank1]:E0603 16:16:56.349000 62444 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     p2p_op.op(
[rank1]:E0603 16:16:56.349000 62444 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/distributed/distributed_c10d.py", line 2366, in isend
[rank1]:E0603 16:16:56.349000 62444 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     return group.send([tensor], group_dst, tag)
[rank1]:E0603 16:16:56.349000 62444 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] torch.distributed.DistBackendError: NCCL error in: /skishore/github/pytorch/torch/csrc/distributed/c10d/NCCLUtils.cpp:77, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.25.1
[rank1]:E0603 16:16:56.349000 62444 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
[rank1]:E0603 16:16:56.349000 62444 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Last error:
[rank1]:E0603 16:16:56.349000 62444 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Error while creating shared memory segment /dev/shm/nccl-2Un7eJ (size 8257920), error: No space left on device (28)
[rank1]:E0603 16:16:56.349000 62444 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] 
[rank1]:E0603 16:16:56.349000 62444 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] To execute this test, run the following from the base repo dir:
[rank1]:E0603 16:16:56.349000 62444 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     PYTORCH_TEST_WITH_ROCM=1 python test_pipeline_parallel_fwd_bwd.py NcclPipelineParallelForwardBackwardTest.test_learning_async_pipelining_with_interleaving
[rank1]:E0603 16:16:56.349000 62444 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] 
[rank1]:E0603 16:16:56.349000 62444 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0
[rank1]:E0603 16:16:56.349000 62444 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]  exiting process 1 with exit code: 10
[rank3]:E0603 16:16:56.349000 62446 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Caught exception: 
[rank3]:E0603 16:16:56.349000 62446 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Traceback (most recent call last):
[rank3]:E0603 16:16:56.349000 62446 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/testing/_internal/common_distributed.py", line 734, in run_test
[rank3]:E0603 16:16:56.349000 62446 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     getattr(self, test_name)()
[rank3]:E0603 16:16:56.349000 62446 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/testing/_internal/common_distributed.py", line 607, in wrapper
[rank3]:E0603 16:16:56.349000 62446 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     fn()
[rank3]:E0603 16:16:56.349000 62446 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/testing/_internal/common_utils.py", line 3173, in wrapper
[rank3]:E0603 16:16:56.349000 62446 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     method(*args, **kwargs)
[rank3]:E0603 16:16:56.349000 62446 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py", line 272, in test_learning_async_pipelining_with_interleaving
[rank3]:E0603 16:16:56.349000 62446 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     self._forward_backward_test_impl(
[rank3]:E0603 16:16:56.349000 62446 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py", line 180, in _forward_backward_test_impl
[rank3]:E0603 16:16:56.349000 62446 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     loss = fwd_bwd_func(
[rank3]:E0603 16:16:56.349000 62446 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_with_interleaving.py", line 219, in _forward_backward_pipelining_with_interleaving
[rank3]:E0603 16:16:56.349000 62446 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     p2p_communication.recv_forward(
[rank3]:E0603 16:16:56.349000 62446 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 342, in recv_forward
[rank3]:E0603 16:16:56.349000 62446 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     input_tensor, _ = _communicate(
[rank3]:E0603 16:16:56.349000 62446 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 259, in _communicate
[rank3]:E0603 16:16:56.349000 62446 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     tensor_send_prev_req, tensor_recv_prev_req, tensor_send_next_req, tensor_recv_next_req = _run_p2pops(tensor_send_prev, tensor_send_next, tensor_recv_prev, tensor_recv_next, async_comm=async_comm)
[rank3]:E0603 16:16:56.349000 62446 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 97, in _run_p2pops
[rank3]:E0603 16:16:56.349000 62446 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     reqs = torch.distributed.batch_isend_irecv(ops)
[rank3]:E0603 16:16:56.349000 62446 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/distributed/distributed_c10d.py", line 2717, in batch_isend_irecv
[rank3]:E0603 16:16:56.349000 62446 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     p2p_op.op(
[rank3]:E0603 16:16:56.349000 62446 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/distributed/distributed_c10d.py", line 2411, in irecv
[rank3]:E0603 16:16:56.349000 62446 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     return group.recv([tensor], group_src, tag)
[rank3]:E0603 16:16:56.349000 62446 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] torch.distributed.DistBackendError: NCCL error in: /skishore/github/pytorch/torch/csrc/distributed/c10d/NCCLUtils.cpp:77, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.25.1
[rank3]:E0603 16:16:56.349000 62446 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
[rank3]:E0603 16:16:56.349000 62446 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Last error:
[rank3]:E0603 16:16:56.349000 62446 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Error while creating shared memory segment /dev/shm/nccl-ij6ANE (size 8257920), error: No space left on device (28)
[rank3]:E0603 16:16:56.349000 62446 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] 
[rank3]:E0603 16:16:56.349000 62446 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] To execute this test, run the following from the base repo dir:
[rank3]:E0603 16:16:56.349000 62446 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     PYTORCH_TEST_WITH_ROCM=1 python test_pipeline_parallel_fwd_bwd.py NcclPipelineParallelForwardBackwardTest.test_learning_async_pipelining_with_interleaving
[rank3]:E0603 16:16:56.349000 62446 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] 
[rank3]:E0603 16:16:56.349000 62446 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0
[rank3]:E0603 16:16:56.349000 62446 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]  exiting process 3 with exit code: 10
[dist init] rank = 1, world_size = 8
[dist init] rank = 3, world_size = 8
[rank6]:[W603 16:16:56.562566005 ProcessGroupNCCL.cpp:1553] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank0]:[W603 16:16:56.730181481 ProcessGroupNCCL.cpp:1553] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank4]:[W603 16:16:56.736195096 ProcessGroupNCCL.cpp:1553] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank2]:[W603 16:16:56.754515015 ProcessGroupNCCL.cpp:1553] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank1]:[W603 16:16:57.298347870 ProcessGroupNCCL.cpp:1553] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank3]:[W603 16:16:57.308824739 ProcessGroupNCCL.cpp:1553] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank7]:[W603 16:16:57.335671324 ProcessGroupNCCL.cpp:1553] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank5]:[W603 16:16:57.387429570 ProcessGroupNCCL.cpp:1553] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
ERROR
test_learning_async_pipelining_without_interleaving (test_pipeline_parallel_fwd_bwd.NcclPipelineParallelForwardBackwardTest) ... Process 6 terminated with exit code 10, terminating remaining processes.
[rank6]:[W603 16:17:20.110087992 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 6]  using GPU 6 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank0]:[W603 16:17:20.142174903 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank4]:[W603 16:17:20.152173974 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 4]  using GPU 4 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank5]:[W603 16:17:20.155374203 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 5]  using GPU 5 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank7]:[W603 16:17:20.171188554 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 7]  using GPU 7 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank3]:[W603 16:17:20.175718717 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank1]:[W603 16:17:20.180679259 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank2]:[W603 16:17:20.252079828 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/utils.py:81: UserWarning: This function is only for unittest
  warnings.warn("This function is only for unittest")
[dist init] rank = 6, world_size = 8
 > number of parameters on (tensor, pipeline) model parallel rank (0, 3): 1056
[dist init] rank = 0, world_size = 8
 > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 1056
[dist init] rank = 4, world_size = 8
 > number of parameters on (tensor, pipeline) model parallel rank (0, 2): 1056
[dist init] rank = 2, world_size = 8
 > number of parameters on (tensor, pipeline) model parallel rank (0, 1): 1056
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/common.py:295: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/common.py:295: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/common.py:295: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/common.py:295: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/common.py:295: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/common.py:295: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/common.py:295: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/common.py:295: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(
 > number of parameters on (tensor, pipeline) model parallel rank (0, 2): 1056
 > number of parameters on (tensor, pipeline) model parallel rank (0, 3): 1056
 > number of parameters on (tensor, pipeline) model parallel rank (0, 1): 1056
 > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 1056
 > number of parameters on (tensor, pipeline) model parallel rank (0, 3): 1056
 > number of parameters on (tensor, pipeline) model parallel rank (0, 2): 1056
 > number of parameters on (tensor, pipeline) model parallel rank (0, 1): 1056
 > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 1056
 > number of parameters on (tensor, pipeline) model parallel rank (0, 2): 1056
 > number of parameters on (tensor, pipeline) model parallel rank (0, 1): 1056
/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py:208: UserWarning: Converting a tensor with requires_grad=True to a scalar may lead to unexpected behavior.
Consider using tensor.detach() first. (Triggered internally at /skishore/github/pytorch/aten/src/ATen/native/Scalar.cpp:22.)
  self.assertEqual(x.item() / microbatch_size, target_loss.item())
/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py:208: UserWarning: Converting a tensor with requires_grad=True to a scalar may lead to unexpected behavior.
Consider using tensor.detach() first. (Triggered internally at /skishore/github/pytorch/aten/src/ATen/native/Scalar.cpp:22.)
  self.assertEqual(x.item() / microbatch_size, target_loss.item())
 > number of parameters on (tensor, pipeline) model parallel rank (0, 3): 1056
 > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 1056
/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py:126: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  torch.cuda.amp.GradScaler(init_scale=4.0)
 > number of parameters on (tensor, pipeline) model parallel rank (0, 3): 1056
/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py:126: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  torch.cuda.amp.GradScaler(init_scale=4.0)
/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py:126: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  torch.cuda.amp.GradScaler(init_scale=4.0)
/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py:126: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  torch.cuda.amp.GradScaler(init_scale=4.0)
/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py:126: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  torch.cuda.amp.GradScaler(init_scale=4.0)
 > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 1056
/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py:126: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  torch.cuda.amp.GradScaler(init_scale=4.0)
 > number of parameters on (tensor, pipeline) model parallel rank (0, 2): 1056
/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py:126: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  torch.cuda.amp.GradScaler(init_scale=4.0)
/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py:126: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  torch.cuda.amp.GradScaler(init_scale=4.0)
 > number of parameters on (tensor, pipeline) model parallel rank (0, 1): 1056
 > number of parameters on (tensor, pipeline) model parallel rank (0, 3): 1056
 > number of parameters on (tensor, pipeline) model parallel rank (0, 2): 1056
 > number of parameters on (tensor, pipeline) model parallel rank (0, 1): 1056
 > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 1056
 > number of parameters on (tensor, pipeline) model parallel rank (0, 3): 1056
 > number of parameters on (tensor, pipeline) model parallel rank (0, 2): 1056
 > number of parameters on (tensor, pipeline) model parallel rank (0, 1): 1056
 > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 1056
 > number of parameters on (tensor, pipeline) model parallel rank (0, 3): 1056
 > number of parameters on (tensor, pipeline) model parallel rank (0, 2): 1056
 > number of parameters on (tensor, pipeline) model parallel rank (0, 1): 1056
 > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 1056
[rank2]:E0603 16:19:14.430000 66590 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Caught exception: 
[rank2]:E0603 16:19:14.430000 66590 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Traceback (most recent call last):
[rank2]:E0603 16:19:14.430000 66590 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/testing/_internal/common_distributed.py", line 734, in run_test
[rank2]:E0603 16:19:14.430000 66590 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     getattr(self, test_name)()
[rank2]:E0603 16:19:14.430000 66590 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/testing/_internal/common_distributed.py", line 607, in wrapper
[rank2]:E0603 16:19:14.430000 66590 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     fn()
[rank2]:E0603 16:19:14.430000 66590 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/testing/_internal/common_utils.py", line 3173, in wrapper
[rank2]:E0603 16:19:14.430000 66590 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     method(*args, **kwargs)
[rank2]:E0603 16:19:14.430000 66590 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py", line 249, in test_learning_async_pipelining_without_interleaving
[rank2]:E0603 16:19:14.430000 66590 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     self._forward_backward_test_impl(
[rank2]:E0603 16:19:14.430000 66590 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py", line 180, in _forward_backward_test_impl
[rank2]:E0603 16:19:14.430000 66590 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     loss = fwd_bwd_func(
[rank2]:E0603 16:19:14.430000 66590 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_without_interleaving.py", line 332, in forward_backward_pipelining_without_interleaving
[rank2]:E0603 16:19:14.430000 66590 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     input_tensor = recv_forward(
[rank2]:E0603 16:19:14.430000 66590 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_without_interleaving.py", line 99, in recv_forward
[rank2]:E0603 16:19:14.430000 66590 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     p2p_communication.recv_forward(
[rank2]:E0603 16:19:14.430000 66590 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 342, in recv_forward
[rank2]:E0603 16:19:14.430000 66590 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     input_tensor, _ = _communicate(
[rank2]:E0603 16:19:14.430000 66590 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 259, in _communicate
[rank2]:E0603 16:19:14.430000 66590 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     tensor_send_prev_req, tensor_recv_prev_req, tensor_send_next_req, tensor_recv_next_req = _run_p2pops(tensor_send_prev, tensor_send_next, tensor_recv_prev, tensor_recv_next, async_comm=async_comm)
[rank2]:E0603 16:19:14.430000 66590 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 97, in _run_p2pops
[rank2]:E0603 16:19:14.430000 66590 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     reqs = torch.distributed.batch_isend_irecv(ops)
[rank2]:E0603 16:19:14.430000 66590 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/distributed/distributed_c10d.py", line 2717, in batch_isend_irecv
[rank2]:E0603 16:19:14.430000 66590 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     p2p_op.op(
[rank2]:E0603 16:19:14.430000 66590 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/distributed/distributed_c10d.py", line 2411, in irecv
[rank2]:E0603 16:19:14.430000 66590 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     return group.recv([tensor], group_src, tag)
[rank2]:E0603 16:19:14.430000 66590 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] torch.distributed.DistBackendError: NCCL error in: /skishore/github/pytorch/torch/csrc/distributed/c10d/NCCLUtils.cpp:77, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.25.1
[rank2]:E0603 16:19:14.430000 66590 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
[rank2]:E0603 16:19:14.430000 66590 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Last error:
[rank2]:E0603 16:19:14.430000 66590 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Error while creating shared memory segment /dev/shm/nccl-gwbLVE (size 8257920), error: No space left on device (28)
[rank2]:E0603 16:19:14.430000 66590 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] 
[rank2]:E0603 16:19:14.430000 66590 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] To execute this test, run the following from the base repo dir:
[rank2]:E0603 16:19:14.430000 66590 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     PYTORCH_TEST_WITH_ROCM=1 python test_pipeline_parallel_fwd_bwd.py NcclPipelineParallelForwardBackwardTest.test_learning_async_pipelining_without_interleaving
[rank2]:E0603 16:19:14.430000 66590 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] 
[rank2]:E0603 16:19:14.430000 66590 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0
[rank2]:E0603 16:19:14.430000 66590 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]  exiting process 2 with exit code: 10
[rank4]:E0603 16:19:14.430000 66592 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Caught exception: 
[rank4]:E0603 16:19:14.430000 66592 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Traceback (most recent call last):
[rank4]:E0603 16:19:14.430000 66592 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/testing/_internal/common_distributed.py", line 734, in run_test
[rank4]:E0603 16:19:14.430000 66592 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     getattr(self, test_name)()
[rank4]:E0603 16:19:14.430000 66592 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/testing/_internal/common_distributed.py", line 607, in wrapper
[rank4]:E0603 16:19:14.430000 66592 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     fn()
[rank4]:E0603 16:19:14.430000 66592 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/testing/_internal/common_utils.py", line 3173, in wrapper
[rank4]:E0603 16:19:14.430000 66592 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     method(*args, **kwargs)
[rank4]:E0603 16:19:14.430000 66592 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py", line 249, in test_learning_async_pipelining_without_interleaving
[rank4]:E0603 16:19:14.430000 66592 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     self._forward_backward_test_impl(
[rank4]:E0603 16:19:14.430000 66592 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py", line 180, in _forward_backward_test_impl
[rank4]:E0603 16:19:14.430000 66592 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     loss = fwd_bwd_func(
[rank4]:E0603 16:19:14.430000 66592 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_without_interleaving.py", line 332, in forward_backward_pipelining_without_interleaving
[rank4]:E0603 16:19:14.430000 66592 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     input_tensor = recv_forward(
[rank4]:E0603 16:19:14.430000 66592 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_without_interleaving.py", line 99, in recv_forward
[rank4]:E0603 16:19:14.430000 66592 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     p2p_communication.recv_forward(
[rank4]:E0603 16:19:14.430000 66592 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 342, in recv_forward
[rank4]:E0603 16:19:14.430000 66592 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     input_tensor, _ = _communicate(
[rank4]:E0603 16:19:14.430000 66592 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 259, in _communicate
[rank4]:E0603 16:19:14.430000 66592 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     tensor_send_prev_req, tensor_recv_prev_req, tensor_send_next_req, tensor_recv_next_req = _run_p2pops(tensor_send_prev, tensor_send_next, tensor_recv_prev, tensor_recv_next, async_comm=async_comm)
[rank4]:E0603 16:19:14.430000 66592 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 97, in _run_p2pops
[rank4]:E0603 16:19:14.430000 66592 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     reqs = torch.distributed.batch_isend_irecv(ops)
[rank4]:E0603 16:19:14.430000 66592 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/distributed/distributed_c10d.py", line 2717, in batch_isend_irecv
[rank4]:E0603 16:19:14.430000 66592 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     p2p_op.op(
[rank4]:E0603 16:19:14.430000 66592 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/distributed/distributed_c10d.py", line 2411, in irecv
[rank4]:E0603 16:19:14.430000 66592 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     return group.recv([tensor], group_src, tag)
[rank4]:E0603 16:19:14.430000 66592 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] torch.distributed.DistBackendError: NCCL error in: /skishore/github/pytorch/torch/csrc/distributed/c10d/NCCLUtils.cpp:77, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.25.1
[rank4]:E0603 16:19:14.430000 66592 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
[rank4]:E0603 16:19:14.430000 66592 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Last error:
[rank4]:E0603 16:19:14.430000 66592 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Error while creating shared memory segment /dev/shm/nccl-gP7wMi (size 8257920), error: No space left on device (28)
[rank4]:E0603 16:19:14.430000 66592 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] 
[rank4]:E0603 16:19:14.430000 66592 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] To execute this test, run the following from the base repo dir:
[rank4]:E0603 16:19:14.430000 66592 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     PYTORCH_TEST_WITH_ROCM=1 python test_pipeline_parallel_fwd_bwd.py NcclPipelineParallelForwardBackwardTest.test_learning_async_pipelining_without_interleaving
[rank4]:E0603 16:19:14.430000 66592 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] 
[rank4]:E0603 16:19:14.430000 66592 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0
[rank4]:E0603 16:19:14.430000 66592 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]  exiting process 4 with exit code: 10
[rank5]:E0603 16:19:14.430000 66593 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Caught exception: 
[rank5]:E0603 16:19:14.430000 66593 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Traceback (most recent call last):
[rank5]:E0603 16:19:14.430000 66593 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/testing/_internal/common_distributed.py", line 734, in run_test
[rank5]:E0603 16:19:14.430000 66593 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     getattr(self, test_name)()
[rank5]:E0603 16:19:14.430000 66593 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/testing/_internal/common_distributed.py", line 607, in wrapper
[rank5]:E0603 16:19:14.430000 66593 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     fn()
[rank5]:E0603 16:19:14.430000 66593 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/testing/_internal/common_utils.py", line 3173, in wrapper
[rank5]:E0603 16:19:14.430000 66593 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     method(*args, **kwargs)
[rank5]:E0603 16:19:14.430000 66593 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py", line 249, in test_learning_async_pipelining_without_interleaving
[rank5]:E0603 16:19:14.430000 66593 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     self._forward_backward_test_impl(
[rank5]:E0603 16:19:14.430000 66593 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py", line 180, in _forward_backward_test_impl
[rank5]:E0603 16:19:14.430000 66593 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     loss = fwd_bwd_func(
[rank5]:E0603 16:19:14.430000 66593 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_without_interleaving.py", line 332, in forward_backward_pipelining_without_interleaving
[rank5]:E0603 16:19:14.430000 66593 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     input_tensor = recv_forward(
[rank5]:E0603 16:19:14.430000 66593 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_without_interleaving.py", line 99, in recv_forward
[rank5]:E0603 16:19:14.430000 66593 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     p2p_communication.recv_forward(
[rank5]:E0603 16:19:14.430000 66593 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 342, in recv_forward
[rank5]:E0603 16:19:14.430000 66593 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     input_tensor, _ = _communicate(
[rank5]:E0603 16:19:14.430000 66593 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 259, in _communicate
[rank5]:E0603 16:19:14.430000 66593 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     tensor_send_prev_req, tensor_recv_prev_req, tensor_send_next_req, tensor_recv_next_req = _run_p2pops(tensor_send_prev, tensor_send_next, tensor_recv_prev, tensor_recv_next, async_comm=async_comm)
[rank5]:E0603 16:19:14.430000 66593 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 97, in _run_p2pops
[rank5]:E0603 16:19:14.430000 66593 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     reqs = torch.distributed.batch_isend_irecv(ops)
[rank5]:E0603 16:19:14.430000 66593 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/distributed/distributed_c10d.py", line 2717, in batch_isend_irecv
[rank5]:E0603 16:19:14.430000 66593 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     p2p_op.op(
[rank5]:E0603 16:19:14.430000 66593 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/distributed/distributed_c10d.py", line 2411, in irecv
[rank5]:E0603 16:19:14.430000 66593 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     return group.recv([tensor], group_src, tag)
[rank5]:E0603 16:19:14.430000 66593 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] torch.distributed.DistBackendError: NCCL error in: /skishore/github/pytorch/torch/csrc/distributed/c10d/NCCLUtils.cpp:77, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.25.1
[rank5]:E0603 16:19:14.430000 66593 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
[rank5]:E0603 16:19:14.430000 66593 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Last error:
[rank5]:E0603 16:19:14.430000 66593 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Error while creating shared memory segment /dev/shm/nccl-EwoJJ3 (size 8257920), error: No space left on device (28)
[rank5]:E0603 16:19:14.430000 66593 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] 
[rank5]:E0603 16:19:14.430000 66593 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] To execute this test, run the following from the base repo dir:
[rank5]:E0603 16:19:14.430000 66593 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     PYTORCH_TEST_WITH_ROCM=1 python test_pipeline_parallel_fwd_bwd.py NcclPipelineParallelForwardBackwardTest.test_learning_async_pipelining_without_interleaving
[rank5]:E0603 16:19:14.430000 66593 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] 
[rank5]:E0603 16:19:14.430000 66593 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0
[rank5]:E0603 16:19:14.430000 66593 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]  exiting process 5 with exit code: 10
[dist init] rank = 5, world_size = 8
[rank7]:E0603 16:19:14.431000 66595 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Caught exception: 
[rank7]:E0603 16:19:14.431000 66595 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Traceback (most recent call last):
[rank7]:E0603 16:19:14.431000 66595 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/testing/_internal/common_distributed.py", line 734, in run_test
[rank7]:E0603 16:19:14.431000 66595 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     getattr(self, test_name)()
[rank7]:E0603 16:19:14.431000 66595 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/testing/_internal/common_distributed.py", line 607, in wrapper
[rank7]:E0603 16:19:14.431000 66595 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     fn()
[rank7]:E0603 16:19:14.431000 66595 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/testing/_internal/common_utils.py", line 3173, in wrapper
[rank7]:E0603 16:19:14.431000 66595 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     method(*args, **kwargs)
[rank7]:E0603 16:19:14.431000 66595 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py", line 249, in test_learning_async_pipelining_without_interleaving
[rank7]:E0603 16:19:14.431000 66595 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     self._forward_backward_test_impl(
[rank7]:E0603 16:19:14.431000 66595 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py", line 180, in _forward_backward_test_impl
[rank7]:E0603 16:19:14.431000 66595 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     loss = fwd_bwd_func(
[rank7]:E0603 16:19:14.431000 66595 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_without_interleaving.py", line 367, in forward_backward_pipelining_without_interleaving
[rank7]:E0603 16:19:14.431000 66595 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     input_tensor: List[Union[None, torch.Tensor, FutureTensor]] = recv_forward(tensor_shapes=recv_tensor_shapes, dtype=dtype, async_comm=async_comm)
[rank7]:E0603 16:19:14.431000 66595 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_without_interleaving.py", line 99, in recv_forward
[rank7]:E0603 16:19:14.431000 66595 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     p2p_communication.recv_forward(
[rank7]:E0603 16:19:14.431000 66595 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 342, in recv_forward
[rank7]:E0603 16:19:14.431000 66595 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     input_tensor, _ = _communicate(
[rank7]:E0603 16:19:14.431000 66595 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 259, in _communicate
[rank7]:E0603 16:19:14.431000 66595 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     tensor_send_prev_req, tensor_recv_prev_req, tensor_send_next_req, tensor_recv_next_req = _run_p2pops(tensor_send_prev, tensor_send_next, tensor_recv_prev, tensor_recv_next, async_comm=async_comm)
[rank7]:E0603 16:19:14.431000 66595 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 97, in _run_p2pops
[rank7]:E0603 16:19:14.431000 66595 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     reqs = torch.distributed.batch_isend_irecv(ops)
[rank7]:E0603 16:19:14.431000 66595 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/distributed/distributed_c10d.py", line 2717, in batch_isend_irecv
[rank7]:E0603 16:19:14.431000 66595 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     p2p_op.op(
[rank7]:E0603 16:19:14.431000 66595 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/distributed/distributed_c10d.py", line 2411, in irecv
[rank7]:E0603 16:19:14.431000 66595 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     return group.recv([tensor], group_src, tag)
[rank7]:E0603 16:19:14.431000 66595 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] torch.distributed.DistBackendError: NCCL error in: /skishore/github/pytorch/torch/csrc/distributed/c10d/NCCLUtils.cpp:77, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.25.1
[rank7]:E0603 16:19:14.431000 66595 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
[rank7]:E0603 16:19:14.431000 66595 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Last error:
[rank7]:E0603 16:19:14.431000 66595 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Error while creating shared memory segment /dev/shm/nccl-bTPujE (size 8257920), error: No space left on device (28)
[rank7]:E0603 16:19:14.431000 66595 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] 
[rank7]:E0603 16:19:14.431000 66595 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] To execute this test, run the following from the base repo dir:
[rank7]:E0603 16:19:14.431000 66595 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     PYTORCH_TEST_WITH_ROCM=1 python test_pipeline_parallel_fwd_bwd.py NcclPipelineParallelForwardBackwardTest.test_learning_async_pipelining_without_interleaving
[rank7]:E0603 16:19:14.431000 66595 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] 
[rank7]:E0603 16:19:14.431000 66595 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0
[rank7]:E0603 16:19:14.431000 66595 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]  exiting process 7 with exit code: 10
[rank1]:E0603 16:19:14.431000 66589 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Caught exception: 
[rank1]:E0603 16:19:14.431000 66589 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Traceback (most recent call last):
[rank1]:E0603 16:19:14.431000 66589 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/testing/_internal/common_distributed.py", line 734, in run_test
[rank1]:E0603 16:19:14.431000 66589 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     getattr(self, test_name)()
[rank1]:E0603 16:19:14.431000 66589 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/testing/_internal/common_distributed.py", line 607, in wrapper
[rank1]:E0603 16:19:14.431000 66589 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     fn()
[rank1]:E0603 16:19:14.431000 66589 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/testing/_internal/common_utils.py", line 3173, in wrapper
[rank1]:E0603 16:19:14.431000 66589 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     method(*args, **kwargs)
[rank1]:E0603 16:19:14.431000 66589 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py", line 249, in test_learning_async_pipelining_without_interleaving
[rank1]:E0603 16:19:14.431000 66589 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     self._forward_backward_test_impl(
[rank1]:E0603 16:19:14.431000 66589 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py", line 180, in _forward_backward_test_impl
[rank1]:E0603 16:19:14.431000 66589 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     loss = fwd_bwd_func(
[rank1]:E0603 16:19:14.431000 66589 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_without_interleaving.py", line 349, in forward_backward_pipelining_without_interleaving
[rank1]:E0603 16:19:14.431000 66589 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     send_forward(
[rank1]:E0603 16:19:14.431000 66589 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_without_interleaving.py", line 145, in send_forward
[rank1]:E0603 16:19:14.431000 66589 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     p2p_communication.send_forward(
[rank1]:E0603 16:19:14.431000 66589 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 401, in send_forward
[rank1]:E0603 16:19:14.431000 66589 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     _communicate(
[rank1]:E0603 16:19:14.431000 66589 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 259, in _communicate
[rank1]:E0603 16:19:14.431000 66589 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     tensor_send_prev_req, tensor_recv_prev_req, tensor_send_next_req, tensor_recv_next_req = _run_p2pops(tensor_send_prev, tensor_send_next, tensor_recv_prev, tensor_recv_next, async_comm=async_comm)
[rank1]:E0603 16:19:14.431000 66589 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 97, in _run_p2pops
[rank1]:E0603 16:19:14.431000 66589 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     reqs = torch.distributed.batch_isend_irecv(ops)
[rank1]:E0603 16:19:14.431000 66589 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/distributed/distributed_c10d.py", line 2717, in batch_isend_irecv
[rank1]:E0603 16:19:14.431000 66589 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     p2p_op.op(
[rank1]:E0603 16:19:14.431000 66589 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/distributed/distributed_c10d.py", line 2366, in isend
[rank1]:E0603 16:19:14.431000 66589 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     return group.send([tensor], group_dst, tag)
[rank1]:E0603 16:19:14.431000 66589 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] torch.distributed.DistBackendError: NCCL error in: /skishore/github/pytorch/torch/csrc/distributed/c10d/NCCLUtils.cpp:77, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.25.1
[rank1]:E0603 16:19:14.431000 66589 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
[rank1]:E0603 16:19:14.431000 66589 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Last error:
[rank1]:E0603 16:19:14.431000 66589 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Error while creating shared memory segment /dev/shm/nccl-OSTKDz (size 8257920), error: No space left on device (28)
[rank1]:E0603 16:19:14.431000 66589 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] 
[rank1]:E0603 16:19:14.431000 66589 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] To execute this test, run the following from the base repo dir:
[rank1]:E0603 16:19:14.431000 66589 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     PYTORCH_TEST_WITH_ROCM=1 python test_pipeline_parallel_fwd_bwd.py NcclPipelineParallelForwardBackwardTest.test_learning_async_pipelining_without_interleaving
[rank1]:E0603 16:19:14.431000 66589 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] 
[rank1]:E0603 16:19:14.431000 66589 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0
[rank1]:E0603 16:19:14.431000 66589 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]  exiting process 1 with exit code: 10
[rank3]:E0603 16:19:14.432000 66591 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Caught exception: 
[rank3]:E0603 16:19:14.432000 66591 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Traceback (most recent call last):
[rank3]:E0603 16:19:14.432000 66591 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/testing/_internal/common_distributed.py", line 734, in run_test
[rank3]:E0603 16:19:14.432000 66591 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     getattr(self, test_name)()
[rank3]:E0603 16:19:14.432000 66591 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/testing/_internal/common_distributed.py", line 607, in wrapper
[rank3]:E0603 16:19:14.432000 66591 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     fn()
[rank3]:E0603 16:19:14.432000 66591 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/testing/_internal/common_utils.py", line 3173, in wrapper
[rank3]:E0603 16:19:14.432000 66591 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     method(*args, **kwargs)
[rank3]:E0603 16:19:14.432000 66591 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py", line 249, in test_learning_async_pipelining_without_interleaving
[rank3]:E0603 16:19:14.432000 66591 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     self._forward_backward_test_impl(
[rank3]:E0603 16:19:14.432000 66591 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py", line 180, in _forward_backward_test_impl
[rank3]:E0603 16:19:14.432000 66591 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     loss = fwd_bwd_func(
[rank3]:E0603 16:19:14.432000 66591 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_without_interleaving.py", line 332, in forward_backward_pipelining_without_interleaving
[rank3]:E0603 16:19:14.432000 66591 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     input_tensor = recv_forward(
[rank3]:E0603 16:19:14.432000 66591 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_without_interleaving.py", line 99, in recv_forward
[rank3]:E0603 16:19:14.432000 66591 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     p2p_communication.recv_forward(
[rank3]:E0603 16:19:14.432000 66591 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 342, in recv_forward
[rank3]:E0603 16:19:14.432000 66591 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     input_tensor, _ = _communicate(
[rank3]:E0603 16:19:14.432000 66591 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 259, in _communicate
[rank3]:E0603 16:19:14.432000 66591 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     tensor_send_prev_req, tensor_recv_prev_req, tensor_send_next_req, tensor_recv_next_req = _run_p2pops(tensor_send_prev, tensor_send_next, tensor_recv_prev, tensor_recv_next, async_comm=async_comm)
[rank3]:E0603 16:19:14.432000 66591 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 97, in _run_p2pops
[rank3]:E0603 16:19:14.432000 66591 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     reqs = torch.distributed.batch_isend_irecv(ops)
[rank3]:E0603 16:19:14.432000 66591 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/distributed/distributed_c10d.py", line 2717, in batch_isend_irecv
[rank3]:E0603 16:19:14.432000 66591 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     p2p_op.op(
[rank3]:E0603 16:19:14.432000 66591 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/distributed/distributed_c10d.py", line 2411, in irecv
[rank3]:E0603 16:19:14.432000 66591 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     return group.recv([tensor], group_src, tag)
[rank3]:E0603 16:19:14.432000 66591 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] torch.distributed.DistBackendError: NCCL error in: /skishore/github/pytorch/torch/csrc/distributed/c10d/NCCLUtils.cpp:77, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.25.1
[rank3]:E0603 16:19:14.432000 66591 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
[rank3]:E0603 16:19:14.432000 66591 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Last error:
[rank3]:E0603 16:19:14.432000 66591 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Error while creating shared memory segment /dev/shm/nccl-IVeBCr (size 8257920), error: No space left on device (28)
[rank3]:E0603 16:19:14.432000 66591 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] 
[rank3]:E0603 16:19:14.432000 66591 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] To execute this test, run the following from the base repo dir:
[rank3]:E0603 16:19:14.432000 66591 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     PYTORCH_TEST_WITH_ROCM=1 python test_pipeline_parallel_fwd_bwd.py NcclPipelineParallelForwardBackwardTest.test_learning_async_pipelining_without_interleaving
[rank3]:E0603 16:19:14.432000 66591 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] 
[rank3]:E0603 16:19:14.432000 66591 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0
[rank3]:E0603 16:19:14.432000 66591 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]  exiting process 3 with exit code: 10
[rank6]:E0603 16:19:14.432000 66594 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Caught exception: 
[rank6]:E0603 16:19:14.432000 66594 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Traceback (most recent call last):
[rank6]:E0603 16:19:14.432000 66594 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/testing/_internal/common_distributed.py", line 734, in run_test
[rank6]:E0603 16:19:14.432000 66594 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     getattr(self, test_name)()
[rank6]:E0603 16:19:14.432000 66594 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/testing/_internal/common_distributed.py", line 607, in wrapper
[rank6]:E0603 16:19:14.432000 66594 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     fn()
[rank6]:E0603 16:19:14.432000 66594 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/testing/_internal/common_utils.py", line 3173, in wrapper
[rank6]:E0603 16:19:14.432000 66594 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     method(*args, **kwargs)
[rank6]:E0603 16:19:14.432000 66594 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py", line 249, in test_learning_async_pipelining_without_interleaving
[rank6]:E0603 16:19:14.432000 66594 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     self._forward_backward_test_impl(
[rank6]:E0603 16:19:14.432000 66594 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py", line 180, in _forward_backward_test_impl
[rank6]:E0603 16:19:14.432000 66594 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     loss = fwd_bwd_func(
[rank6]:E0603 16:19:14.432000 66594 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_without_interleaving.py", line 367, in forward_backward_pipelining_without_interleaving
[rank6]:E0603 16:19:14.432000 66594 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     input_tensor: List[Union[None, torch.Tensor, FutureTensor]] = recv_forward(tensor_shapes=recv_tensor_shapes, dtype=dtype, async_comm=async_comm)
[rank6]:E0603 16:19:14.432000 66594 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_without_interleaving.py", line 99, in recv_forward
[rank6]:E0603 16:19:14.432000 66594 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     p2p_communication.recv_forward(
[rank6]:E0603 16:19:14.432000 66594 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 342, in recv_forward
[rank6]:E0603 16:19:14.432000 66594 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     input_tensor, _ = _communicate(
[rank6]:E0603 16:19:14.432000 66594 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 259, in _communicate
[rank6]:E0603 16:19:14.432000 66594 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     tensor_send_prev_req, tensor_recv_prev_req, tensor_send_next_req, tensor_recv_next_req = _run_p2pops(tensor_send_prev, tensor_send_next, tensor_recv_prev, tensor_recv_next, async_comm=async_comm)
[rank6]:E0603 16:19:14.432000 66594 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 97, in _run_p2pops
[rank6]:E0603 16:19:14.432000 66594 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     reqs = torch.distributed.batch_isend_irecv(ops)
[rank6]:E0603 16:19:14.432000 66594 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/distributed/distributed_c10d.py", line 2717, in batch_isend_irecv
[rank6]:E0603 16:19:14.432000 66594 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     p2p_op.op(
[rank6]:E0603 16:19:14.432000 66594 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/distributed/distributed_c10d.py", line 2411, in irecv
[rank6]:E0603 16:19:14.432000 66594 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     return group.recv([tensor], group_src, tag)
[rank6]:E0603 16:19:14.432000 66594 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] torch.distributed.DistBackendError: NCCL error in: /skishore/github/pytorch/torch/csrc/distributed/c10d/NCCLUtils.cpp:77, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.25.1
[rank6]:E0603 16:19:14.432000 66594 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
[rank6]:E0603 16:19:14.432000 66594 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Last error:
[rank6]:E0603 16:19:14.432000 66594 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Error while creating shared memory segment /dev/shm/nccl-DLYrhm (size 8257920), error: No space left on device (28)
[rank6]:E0603 16:19:14.432000 66594 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] 
[rank6]:E0603 16:19:14.432000 66594 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] To execute this test, run the following from the base repo dir:
[rank6]:E0603 16:19:14.432000 66594 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     PYTORCH_TEST_WITH_ROCM=1 python test_pipeline_parallel_fwd_bwd.py NcclPipelineParallelForwardBackwardTest.test_learning_async_pipelining_without_interleaving
[rank6]:E0603 16:19:14.432000 66594 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] 
[rank6]:E0603 16:19:14.432000 66594 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0
[rank6]:E0603 16:19:14.432000 66594 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]  exiting process 6 with exit code: 10
[rank0]:E0603 16:19:14.432000 66588 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Caught exception: 
[rank0]:E0603 16:19:14.432000 66588 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Traceback (most recent call last):
[rank0]:E0603 16:19:14.432000 66588 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/testing/_internal/common_distributed.py", line 734, in run_test
[rank0]:E0603 16:19:14.432000 66588 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     getattr(self, test_name)()
[rank0]:E0603 16:19:14.432000 66588 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/testing/_internal/common_distributed.py", line 607, in wrapper
[rank0]:E0603 16:19:14.432000 66588 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     fn()
[rank0]:E0603 16:19:14.432000 66588 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/testing/_internal/common_utils.py", line 3173, in wrapper
[rank0]:E0603 16:19:14.432000 66588 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     method(*args, **kwargs)
[rank0]:E0603 16:19:14.432000 66588 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py", line 249, in test_learning_async_pipelining_without_interleaving
[rank0]:E0603 16:19:14.432000 66588 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     self._forward_backward_test_impl(
[rank0]:E0603 16:19:14.432000 66588 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py", line 180, in _forward_backward_test_impl
[rank0]:E0603 16:19:14.432000 66588 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     loss = fwd_bwd_func(
[rank0]:E0603 16:19:14.432000 66588 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_without_interleaving.py", line 349, in forward_backward_pipelining_without_interleaving
[rank0]:E0603 16:19:14.432000 66588 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     send_forward(
[rank0]:E0603 16:19:14.432000 66588 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_without_interleaving.py", line 145, in send_forward
[rank0]:E0603 16:19:14.432000 66588 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     p2p_communication.send_forward(
[rank0]:E0603 16:19:14.432000 66588 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 401, in send_forward
[rank0]:E0603 16:19:14.432000 66588 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     _communicate(
[rank0]:E0603 16:19:14.432000 66588 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 259, in _communicate
[rank0]:E0603 16:19:14.432000 66588 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     tensor_send_prev_req, tensor_recv_prev_req, tensor_send_next_req, tensor_recv_next_req = _run_p2pops(tensor_send_prev, tensor_send_next, tensor_recv_prev, tensor_recv_next, async_comm=async_comm)
[rank0]:E0603 16:19:14.432000 66588 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 97, in _run_p2pops
[rank0]:E0603 16:19:14.432000 66588 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     reqs = torch.distributed.batch_isend_irecv(ops)
[rank0]:E0603 16:19:14.432000 66588 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/distributed/distributed_c10d.py", line 2717, in batch_isend_irecv
[rank0]:E0603 16:19:14.432000 66588 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     p2p_op.op(
[rank0]:E0603 16:19:14.432000 66588 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/distributed/distributed_c10d.py", line 2366, in isend
[rank0]:E0603 16:19:14.432000 66588 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     return group.send([tensor], group_dst, tag)
[rank0]:E0603 16:19:14.432000 66588 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] torch.distributed.DistBackendError: NCCL error in: /skishore/github/pytorch/torch/csrc/distributed/c10d/NCCLUtils.cpp:77, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.25.1
[rank0]:E0603 16:19:14.432000 66588 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
[rank0]:E0603 16:19:14.432000 66588 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Last error:
[rank0]:E0603 16:19:14.432000 66588 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Error while creating shared memory segment /dev/shm/nccl-5FNsgR (size 8257920), error: No space left on device (28)
[rank0]:E0603 16:19:14.432000 66588 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] 
[rank0]:E0603 16:19:14.432000 66588 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] To execute this test, run the following from the base repo dir:
[rank0]:E0603 16:19:14.432000 66588 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     PYTORCH_TEST_WITH_ROCM=1 python test_pipeline_parallel_fwd_bwd.py NcclPipelineParallelForwardBackwardTest.test_learning_async_pipelining_without_interleaving
[rank0]:E0603 16:19:14.432000 66588 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] 
[rank0]:E0603 16:19:14.432000 66588 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0
[rank0]:E0603 16:19:14.432000 66588 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]  exiting process 0 with exit code: 10
[dist init] rank = 1, world_size = 8
[dist init] rank = 7, world_size = 8
[dist init] rank = 3, world_size = 8
[rank6]:[W603 16:19:14.744460635 ProcessGroupNCCL.cpp:1553] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank2]:[W603 16:19:14.746772096 ProcessGroupNCCL.cpp:1553] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank4]:[W603 16:19:14.760300394 ProcessGroupNCCL.cpp:1553] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank0]:[W603 16:19:14.772900364 ProcessGroupNCCL.cpp:1553] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank1]:[W603 16:19:15.356409455 ProcessGroupNCCL.cpp:1553] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank3]:[W603 16:19:15.363179882 ProcessGroupNCCL.cpp:1553] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank7]:[W603 16:19:15.494206214 ProcessGroupNCCL.cpp:1553] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank5]:[W603 16:19:15.494956748 ProcessGroupNCCL.cpp:1553] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
ERROR
test_learning_no_pipelining (test_pipeline_parallel_fwd_bwd.NcclPipelineParallelForwardBackwardTest) ... Process 1 terminated with exit code 10, terminating remaining processes.
[rank5]:[W603 16:19:37.713920562 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 5]  using GPU 5 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank1]:[W603 16:19:37.722140622 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank4]:[W603 16:19:37.724227198 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 4]  using GPU 4 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank7]:[W603 16:19:37.730407901 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 7]  using GPU 7 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank3]:[W603 16:19:37.734493395 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank0]:[W603 16:19:37.907402606 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank6]:[W603 16:19:37.929812315 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 6]  using GPU 6 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank2]:[W603 16:19:37.930026984 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/utils.py:81: UserWarning: This function is only for unittest
  warnings.warn("This function is only for unittest")
[dist init] rank = 0, world_size = 8
 > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 1056
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/common.py:295: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/common.py:295: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/common.py:295: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/common.py:295: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/common.py:295: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/common.py:295: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/common.py:295: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/common.py:295: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(
 > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 1056
 > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 1056
/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py:208: UserWarning: Converting a tensor with requires_grad=True to a scalar may lead to unexpected behavior.
Consider using tensor.detach() first. (Triggered internally at /skishore/github/pytorch/aten/src/ATen/native/Scalar.cpp:22.)
  self.assertEqual(x.item() / microbatch_size, target_loss.item())
/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py:208: UserWarning: Converting a tensor with requires_grad=True to a scalar may lead to unexpected behavior.
Consider using tensor.detach() first. (Triggered internally at /skishore/github/pytorch/aten/src/ATen/native/Scalar.cpp:22.)
  self.assertEqual(x.item() / microbatch_size, target_loss.item())
/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py:208: UserWarning: Converting a tensor with requires_grad=True to a scalar may lead to unexpected behavior.
Consider using tensor.detach() first. (Triggered internally at /skishore/github/pytorch/aten/src/ATen/native/Scalar.cpp:22.)
  self.assertEqual(x.item() / microbatch_size, target_loss.item())
/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py:208: UserWarning: Converting a tensor with requires_grad=True to a scalar may lead to unexpected behavior.
Consider using tensor.detach() first. (Triggered internally at /skishore/github/pytorch/aten/src/ATen/native/Scalar.cpp:22.)
  self.assertEqual(x.item() / microbatch_size, target_loss.item())
/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py:208: UserWarning: Converting a tensor with requires_grad=True to a scalar may lead to unexpected behavior.
Consider using tensor.detach() first. (Triggered internally at /skishore/github/pytorch/aten/src/ATen/native/Scalar.cpp:22.)
  self.assertEqual(x.item() / microbatch_size, target_loss.item())
/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py:208: UserWarning: Converting a tensor with requires_grad=True to a scalar may lead to unexpected behavior.
Consider using tensor.detach() first. (Triggered internally at /skishore/github/pytorch/aten/src/ATen/native/Scalar.cpp:22.)
  self.assertEqual(x.item() / microbatch_size, target_loss.item())
/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py:208: UserWarning: Converting a tensor with requires_grad=True to a scalar may lead to unexpected behavior.
Consider using tensor.detach() first. (Triggered internally at /skishore/github/pytorch/aten/src/ATen/native/Scalar.cpp:22.)
  self.assertEqual(x.item() / microbatch_size, target_loss.item())
/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py:208: UserWarning: Converting a tensor with requires_grad=True to a scalar may lead to unexpected behavior.
Consider using tensor.detach() first. (Triggered internally at /skishore/github/pytorch/aten/src/ATen/native/Scalar.cpp:22.)
  self.assertEqual(x.item() / microbatch_size, target_loss.item())
 > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 1056
/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py:126: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  torch.cuda.amp.GradScaler(init_scale=4.0)
/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py:126: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  torch.cuda.amp.GradScaler(init_scale=4.0)
/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py:126: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  torch.cuda.amp.GradScaler(init_scale=4.0)
/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py:126: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  torch.cuda.amp.GradScaler(init_scale=4.0)
/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py:126: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  torch.cuda.amp.GradScaler(init_scale=4.0)
 > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 1056
/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py:126: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  torch.cuda.amp.GradScaler(init_scale=4.0)
/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py:126: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  torch.cuda.amp.GradScaler(init_scale=4.0)
/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py:126: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  torch.cuda.amp.GradScaler(init_scale=4.0)
 > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 1056
 > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 1056
 > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 1056
[dist init] rank = 7, world_size = 8
[dist init] rank = 6, world_size = 8
[dist init] rank = 2, world_size = 8
[dist init] rank = 5, world_size = 8
[dist init] rank = 4, world_size = 8
[dist init] rank = 1, world_size = 8
[dist init] rank = 3, world_size = 8
ok
test_learning_pipelining_with_interleaving (test_pipeline_parallel_fwd_bwd.NcclPipelineParallelForwardBackwardTest) ... [rank1]:[W603 16:21:13.240201491 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank2]:[W603 16:21:13.249418440 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank7]:[W603 16:21:13.249418210 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 7]  using GPU 7 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank5]:[W603 16:21:13.273189500 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 5]  using GPU 5 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank0]:[W603 16:21:13.275414472 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank6]:[W603 16:21:13.276441496 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 6]  using GPU 6 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank3]:[W603 16:21:13.295509256 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank4]:[W603 16:21:13.323185761 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 4]  using GPU 4 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[dist init] rank = 4, world_size = 8
 > number of parameters on (tensor, pipeline) model parallel rank (0, 2): 2112
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/utils.py:81: UserWarning: This function is only for unittest
  warnings.warn("This function is only for unittest")
[dist init] rank = 2, world_size = 8
 > number of parameters on (tensor, pipeline) model parallel rank (0, 1): 2112
[dist init] rank = 6, world_size = 8
 > number of parameters on (tensor, pipeline) model parallel rank (0, 3): 2112
[dist init] rank = 0, world_size = 8
 > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 2112
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/common.py:295: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/common.py:295: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/common.py:295: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/common.py:295: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/common.py:295: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/common.py:295: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/common.py:295: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/common.py:295: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(
 > number of parameters on (tensor, pipeline) model parallel rank (0, 2): 2112
 > number of parameters on (tensor, pipeline) model parallel rank (0, 1): 2112
 > number of parameters on (tensor, pipeline) model parallel rank (0, 3): 2112
 > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 2112
 > number of parameters on (tensor, pipeline) model parallel rank (0, 2): 2112
 > number of parameters on (tensor, pipeline) model parallel rank (0, 3): 2112
 > number of parameters on (tensor, pipeline) model parallel rank (0, 1): 2112
 > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 2112
 > number of parameters on (tensor, pipeline) model parallel rank (0, 2): 2112
/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py:208: UserWarning: Converting a tensor with requires_grad=True to a scalar may lead to unexpected behavior.
Consider using tensor.detach() first. (Triggered internally at /skishore/github/pytorch/aten/src/ATen/native/Scalar.cpp:22.)
  self.assertEqual(x.item() / microbatch_size, target_loss.item())
 > number of parameters on (tensor, pipeline) model parallel rank (0, 3): 2112
/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py:208: UserWarning: Converting a tensor with requires_grad=True to a scalar may lead to unexpected behavior.
Consider using tensor.detach() first. (Triggered internally at /skishore/github/pytorch/aten/src/ATen/native/Scalar.cpp:22.)
  self.assertEqual(x.item() / microbatch_size, target_loss.item())
 > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 2112
 > number of parameters on (tensor, pipeline) model parallel rank (0, 1): 2112
/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py:126: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  torch.cuda.amp.GradScaler(init_scale=4.0)
/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py:126: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  torch.cuda.amp.GradScaler(init_scale=4.0)
 > number of parameters on (tensor, pipeline) model parallel rank (0, 2): 2112
/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py:126: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  torch.cuda.amp.GradScaler(init_scale=4.0)
/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py:126: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  torch.cuda.amp.GradScaler(init_scale=4.0)
 > number of parameters on (tensor, pipeline) model parallel rank (0, 1): 2112
/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py:126: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  torch.cuda.amp.GradScaler(init_scale=4.0)
 > number of parameters on (tensor, pipeline) model parallel rank (0, 3): 2112
/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py:126: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  torch.cuda.amp.GradScaler(init_scale=4.0)
/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py:126: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  torch.cuda.amp.GradScaler(init_scale=4.0)
 > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 2112
/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py:126: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  torch.cuda.amp.GradScaler(init_scale=4.0)
 > number of parameters on (tensor, pipeline) model parallel rank (0, 3): 2112
 > number of parameters on (tensor, pipeline) model parallel rank (0, 2): 2112
 > number of parameters on (tensor, pipeline) model parallel rank (0, 1): 2112
 > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 2112
 > number of parameters on (tensor, pipeline) model parallel rank (0, 3): 2112
 > number of parameters on (tensor, pipeline) model parallel rank (0, 2): 2112
 > number of parameters on (tensor, pipeline) model parallel rank (0, 1): 2112
 > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 2112
 > number of parameters on (tensor, pipeline) model parallel rank (0, 3): 2112
 > number of parameters on (tensor, pipeline) model parallel rank (0, 2): 2112
 > number of parameters on (tensor, pipeline) model parallel rank (0, 1): 2112
 > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 2112
[rank7]:E0603 16:23:37.606000 74341 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Caught exception: 
[rank7]:E0603 16:23:37.606000 74341 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Traceback (most recent call last):
[rank7]:E0603 16:23:37.606000 74341 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/testing/_internal/common_distributed.py", line 734, in run_test
[rank7]:E0603 16:23:37.606000 74341 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     getattr(self, test_name)()
[rank7]:E0603 16:23:37.606000 74341 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/testing/_internal/common_distributed.py", line 607, in wrapper
[rank7]:E0603 16:23:37.606000 74341 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     fn()
[rank7]:E0603 16:23:37.606000 74341 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/testing/_internal/common_utils.py", line 3173, in wrapper
[rank7]:E0603 16:23:37.606000 74341 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     method(*args, **kwargs)
[rank7]:E0603 16:23:37.606000 74341 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py", line 260, in test_learning_pipelining_with_interleaving
[rank7]:E0603 16:23:37.606000 74341 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     self._forward_backward_test_impl(
[rank7]:E0603 16:23:37.606000 74341 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py", line 180, in _forward_backward_test_impl
[rank7]:E0603 16:23:37.606000 74341 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     loss = fwd_bwd_func(
[rank7]:E0603 16:23:37.606000 74341 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_with_interleaving.py", line 219, in _forward_backward_pipelining_with_interleaving
[rank7]:E0603 16:23:37.606000 74341 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     p2p_communication.recv_forward(
[rank7]:E0603 16:23:37.606000 74341 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 342, in recv_forward
[rank7]:E0603 16:23:37.606000 74341 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     input_tensor, _ = _communicate(
[rank7]:E0603 16:23:37.606000 74341 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 259, in _communicate
[rank7]:E0603 16:23:37.606000 74341 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     tensor_send_prev_req, tensor_recv_prev_req, tensor_send_next_req, tensor_recv_next_req = _run_p2pops(tensor_send_prev, tensor_send_next, tensor_recv_prev, tensor_recv_next, async_comm=async_comm)
[rank7]:E0603 16:23:37.606000 74341 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 97, in _run_p2pops
[rank7]:E0603 16:23:37.606000 74341 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     reqs = torch.distributed.batch_isend_irecv(ops)
[rank7]:E0603 16:23:37.606000 74341 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/distributed/distributed_c10d.py", line 2717, in batch_isend_irecv
[rank7]:E0603 16:23:37.606000 74341 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     p2p_op.op(
[rank7]:E0603 16:23:37.606000 74341 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/distributed/distributed_c10d.py", line 2411, in irecv
[rank7]:E0603 16:23:37.606000 74341 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     return group.recv([tensor], group_src, tag)
[rank7]:E0603 16:23:37.606000 74341 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] torch.distributed.DistBackendError: NCCL error in: /skishore/github/pytorch/torch/csrc/distributed/c10d/NCCLUtils.cpp:77, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.25.1
[rank7]:E0603 16:23:37.606000 74341 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
[rank7]:E0603 16:23:37.606000 74341 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Last error:
[rank7]:E0603 16:23:37.606000 74341 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Error while creating shared memory segment /dev/shm/nccl-g8m58H (size 8257920), error: No space left on device (28)
[rank7]:E0603 16:23:37.606000 74341 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] 
[rank7]:E0603 16:23:37.606000 74341 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] To execute this test, run the following from the base repo dir:
[rank7]:E0603 16:23:37.606000 74341 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     PYTORCH_TEST_WITH_ROCM=1 python test_pipeline_parallel_fwd_bwd.py NcclPipelineParallelForwardBackwardTest.test_learning_pipelining_with_interleaving
[rank7]:E0603 16:23:37.606000 74341 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] 
[rank7]:E0603 16:23:37.606000 74341 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0
[rank7]:E0603 16:23:37.606000 74341 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]  exiting process 7 with exit code: 10
[dist init] rank = 7, world_size = 8
[rank1]:E0603 16:23:37.607000 74335 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Caught exception: 
[rank1]:E0603 16:23:37.607000 74335 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Traceback (most recent call last):
[rank1]:E0603 16:23:37.607000 74335 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/testing/_internal/common_distributed.py", line 734, in run_test
[rank1]:E0603 16:23:37.607000 74335 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     getattr(self, test_name)()
[rank1]:E0603 16:23:37.607000 74335 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/testing/_internal/common_distributed.py", line 607, in wrapper
[rank1]:E0603 16:23:37.607000 74335 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     fn()
[rank1]:E0603 16:23:37.607000 74335 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/testing/_internal/common_utils.py", line 3173, in wrapper
[rank1]:E0603 16:23:37.607000 74335 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     method(*args, **kwargs)
[rank1]:E0603 16:23:37.607000 74335 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py", line 260, in test_learning_pipelining_with_interleaving
[rank1]:E0603 16:23:37.607000 74335 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     self._forward_backward_test_impl(
[rank1]:E0603 16:23:37.607000 74335 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py", line 180, in _forward_backward_test_impl
[rank1]:E0603 16:23:37.607000 74335 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     loss = fwd_bwd_func(
[rank1]:E0603 16:23:37.607000 74335 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_with_interleaving.py", line 276, in _forward_backward_pipelining_with_interleaving
[rank1]:E0603 16:23:37.607000 74335 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     input_tensor = p2p_communication.send_forward_recv_forward(
[rank1]:E0603 16:23:37.607000 74335 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 515, in send_forward_recv_forward
[rank1]:E0603 16:23:37.607000 74335 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     input_tensor, _ = _communicate(
[rank1]:E0603 16:23:37.607000 74335 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 259, in _communicate
[rank1]:E0603 16:23:37.607000 74335 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     tensor_send_prev_req, tensor_recv_prev_req, tensor_send_next_req, tensor_recv_next_req = _run_p2pops(tensor_send_prev, tensor_send_next, tensor_recv_prev, tensor_recv_next, async_comm=async_comm)
[rank1]:E0603 16:23:37.607000 74335 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 97, in _run_p2pops
[rank1]:E0603 16:23:37.607000 74335 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     reqs = torch.distributed.batch_isend_irecv(ops)
[rank1]:E0603 16:23:37.607000 74335 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/distributed/distributed_c10d.py", line 2717, in batch_isend_irecv
[rank1]:E0603 16:23:37.607000 74335 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     p2p_op.op(
[rank1]:E0603 16:23:37.607000 74335 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/distributed/distributed_c10d.py", line 2366, in isend
[rank1]:E0603 16:23:37.607000 74335 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     return group.send([tensor], group_dst, tag)
[rank1]:E0603 16:23:37.607000 74335 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] torch.distributed.DistBackendError: NCCL error in: /skishore/github/pytorch/torch/csrc/distributed/c10d/NCCLUtils.cpp:77, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.25.1
[rank1]:E0603 16:23:37.607000 74335 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
[rank1]:E0603 16:23:37.607000 74335 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Last error:
[rank1]:E0603 16:23:37.607000 74335 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Error while creating shared memory segment /dev/shm/nccl-ANcY0t (size 8257920), error: No space left on device (28)
[rank1]:E0603 16:23:37.607000 74335 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] 
[rank1]:E0603 16:23:37.607000 74335 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] To execute this test, run the following from the base repo dir:
[rank1]:E0603 16:23:37.607000 74335 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     PYTORCH_TEST_WITH_ROCM=1 python test_pipeline_parallel_fwd_bwd.py NcclPipelineParallelForwardBackwardTest.test_learning_pipelining_with_interleaving
[rank1]:E0603 16:23:37.607000 74335 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] 
[rank1]:E0603 16:23:37.607000 74335 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0
[rank1]:E0603 16:23:37.607000 74335 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]  exiting process 1 with exit code: 10
[rank3]:E0603 16:23:37.608000 74337 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Caught exception: 
[rank3]:E0603 16:23:37.608000 74337 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Traceback (most recent call last):
[rank3]:E0603 16:23:37.608000 74337 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/testing/_internal/common_distributed.py", line 734, in run_test
[rank3]:E0603 16:23:37.608000 74337 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     getattr(self, test_name)()
[rank3]:E0603 16:23:37.608000 74337 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/testing/_internal/common_distributed.py", line 607, in wrapper
[rank3]:E0603 16:23:37.608000 74337 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     fn()
[rank3]:E0603 16:23:37.608000 74337 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/testing/_internal/common_utils.py", line 3173, in wrapper
[rank3]:E0603 16:23:37.608000 74337 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     method(*args, **kwargs)
[rank3]:E0603 16:23:37.608000 74337 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py", line 260, in test_learning_pipelining_with_interleaving
[rank3]:E0603 16:23:37.608000 74337 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     self._forward_backward_test_impl(
[rank3]:E0603 16:23:37.608000 74337 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py", line 180, in _forward_backward_test_impl
[rank3]:E0603 16:23:37.608000 74337 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     loss = fwd_bwd_func(
[rank3]:E0603 16:23:37.608000 74337 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_with_interleaving.py", line 219, in _forward_backward_pipelining_with_interleaving
[rank3]:E0603 16:23:37.608000 74337 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     p2p_communication.recv_forward(
[rank3]:E0603 16:23:37.608000 74337 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 342, in recv_forward
[rank3]:E0603 16:23:37.608000 74337 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     input_tensor, _ = _communicate(
[rank3]:E0603 16:23:37.608000 74337 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 259, in _communicate
[rank3]:E0603 16:23:37.608000 74337 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     tensor_send_prev_req, tensor_recv_prev_req, tensor_send_next_req, tensor_recv_next_req = _run_p2pops(tensor_send_prev, tensor_send_next, tensor_recv_prev, tensor_recv_next, async_comm=async_comm)
[rank3]:E0603 16:23:37.608000 74337 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 97, in _run_p2pops
[rank3]:E0603 16:23:37.608000 74337 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     reqs = torch.distributed.batch_isend_irecv(ops)
[rank3]:E0603 16:23:37.608000 74337 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/distributed/distributed_c10d.py", line 2717, in batch_isend_irecv
[rank3]:E0603 16:23:37.608000 74337 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     p2p_op.op(
[rank3]:E0603 16:23:37.608000 74337 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/distributed/distributed_c10d.py", line 2411, in irecv
[rank3]:E0603 16:23:37.608000 74337 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     return group.recv([tensor], group_src, tag)
[rank3]:E0603 16:23:37.608000 74337 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] torch.distributed.DistBackendError: NCCL error in: /skishore/github/pytorch/torch/csrc/distributed/c10d/NCCLUtils.cpp:77, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.25.1
[rank3]:E0603 16:23:37.608000 74337 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
[rank3]:E0603 16:23:37.608000 74337 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Last error:
[rank3]:E0603 16:23:37.608000 74337 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Error while creating shared memory segment /dev/shm/nccl-UrziMF (size 8257920), error: No space left on device (28)
[rank3]:E0603 16:23:37.608000 74337 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] 
[rank3]:E0603 16:23:37.608000 74337 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] To execute this test, run the following from the base repo dir:
[rank3]:E0603 16:23:37.608000 74337 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     PYTORCH_TEST_WITH_ROCM=1 python test_pipeline_parallel_fwd_bwd.py NcclPipelineParallelForwardBackwardTest.test_learning_pipelining_with_interleaving
[rank3]:E0603 16:23:37.608000 74337 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] 
[rank3]:E0603 16:23:37.608000 74337 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0
[rank3]:E0603 16:23:37.608000 74337 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]  exiting process 3 with exit code: 10
[rank5]:E0603 16:23:37.608000 74339 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Caught exception: 
[rank5]:E0603 16:23:37.608000 74339 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Traceback (most recent call last):
[rank5]:E0603 16:23:37.608000 74339 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/testing/_internal/common_distributed.py", line 734, in run_test
[rank5]:E0603 16:23:37.608000 74339 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     getattr(self, test_name)()
[rank5]:E0603 16:23:37.608000 74339 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/testing/_internal/common_distributed.py", line 607, in wrapper
[rank5]:E0603 16:23:37.608000 74339 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     fn()
[rank5]:E0603 16:23:37.608000 74339 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/testing/_internal/common_utils.py", line 3173, in wrapper
[rank5]:E0603 16:23:37.608000 74339 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     method(*args, **kwargs)
[rank5]:E0603 16:23:37.608000 74339 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py", line 260, in test_learning_pipelining_with_interleaving
[rank5]:E0603 16:23:37.608000 74339 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     self._forward_backward_test_impl(
[rank5]:E0603 16:23:37.608000 74339 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py", line 180, in _forward_backward_test_impl
[rank5]:E0603 16:23:37.608000 74339 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     loss = fwd_bwd_func(
[rank5]:E0603 16:23:37.608000 74339 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_with_interleaving.py", line 219, in _forward_backward_pipelining_with_interleaving
[rank5]:E0603 16:23:37.608000 74339 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     p2p_communication.recv_forward(
[rank5]:E0603 16:23:37.608000 74339 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 342, in recv_forward
[rank5]:E0603 16:23:37.608000 74339 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     input_tensor, _ = _communicate(
[rank5]:E0603 16:23:37.608000 74339 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 259, in _communicate
[rank5]:E0603 16:23:37.608000 74339 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     tensor_send_prev_req, tensor_recv_prev_req, tensor_send_next_req, tensor_recv_next_req = _run_p2pops(tensor_send_prev, tensor_send_next, tensor_recv_prev, tensor_recv_next, async_comm=async_comm)
[rank5]:E0603 16:23:37.608000 74339 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 97, in _run_p2pops
[rank5]:E0603 16:23:37.608000 74339 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     reqs = torch.distributed.batch_isend_irecv(ops)
[rank5]:E0603 16:23:37.608000 74339 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/distributed/distributed_c10d.py", line 2717, in batch_isend_irecv
[rank5]:E0603 16:23:37.608000 74339 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     p2p_op.op(
[rank5]:E0603 16:23:37.608000 74339 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/distributed/distributed_c10d.py", line 2411, in irecv
[rank5]:E0603 16:23:37.608000 74339 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     return group.recv([tensor], group_src, tag)
[rank5]:E0603 16:23:37.608000 74339 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] torch.distributed.DistBackendError: NCCL error in: /skishore/github/pytorch/torch/csrc/distributed/c10d/NCCLUtils.cpp:77, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.25.1
[rank5]:E0603 16:23:37.608000 74339 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
[rank5]:E0603 16:23:37.608000 74339 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Last error:
[rank5]:E0603 16:23:37.608000 74339 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Error while creating shared memory segment /dev/shm/nccl-jPvPWC (size 8257920), error: No space left on device (28)
[rank5]:E0603 16:23:37.608000 74339 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] 
[rank5]:E0603 16:23:37.608000 74339 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] To execute this test, run the following from the base repo dir:
[rank5]:E0603 16:23:37.608000 74339 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     PYTORCH_TEST_WITH_ROCM=1 python test_pipeline_parallel_fwd_bwd.py NcclPipelineParallelForwardBackwardTest.test_learning_pipelining_with_interleaving
[rank5]:E0603 16:23:37.608000 74339 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] 
[rank5]:E0603 16:23:37.608000 74339 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0
[rank5]:E0603 16:23:37.608000 74339 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]  exiting process 5 with exit code: 10
[dist init] rank = 1, world_size = 8
[dist init] rank = 3, world_size = 8
[dist init] rank = 5, world_size = 8
[rank0]:E0603 16:23:37.656000 74334 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Caught exception: 
[rank0]:E0603 16:23:37.656000 74334 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Traceback (most recent call last):
[rank0]:E0603 16:23:37.656000 74334 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/testing/_internal/common_distributed.py", line 734, in run_test
[rank0]:E0603 16:23:37.656000 74334 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     getattr(self, test_name)()
[rank0]:E0603 16:23:37.656000 74334 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/testing/_internal/common_distributed.py", line 607, in wrapper
[rank0]:E0603 16:23:37.656000 74334 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     fn()
[rank0]:E0603 16:23:37.656000 74334 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/testing/_internal/common_utils.py", line 3173, in wrapper
[rank0]:E0603 16:23:37.656000 74334 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     method(*args, **kwargs)
[rank0]:E0603 16:23:37.656000 74334 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py", line 260, in test_learning_pipelining_with_interleaving
[rank0]:E0603 16:23:37.656000 74334 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     self._forward_backward_test_impl(
[rank0]:E0603 16:23:37.656000 74334 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py", line 180, in _forward_backward_test_impl
[rank0]:E0603 16:23:37.656000 74334 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     loss = fwd_bwd_func(
[rank0]:E0603 16:23:37.656000 74334 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_with_interleaving.py", line 276, in _forward_backward_pipelining_with_interleaving
[rank0]:E0603 16:23:37.656000 74334 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     input_tensor = p2p_communication.send_forward_recv_forward(
[rank0]:E0603 16:23:37.656000 74334 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 515, in send_forward_recv_forward
[rank0]:E0603 16:23:37.656000 74334 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     input_tensor, _ = _communicate(
[rank0]:E0603 16:23:37.656000 74334 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 259, in _communicate
[rank0]:E0603 16:23:37.656000 74334 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     tensor_send_prev_req, tensor_recv_prev_req, tensor_send_next_req, tensor_recv_next_req = _run_p2pops(tensor_send_prev, tensor_send_next, tensor_recv_prev, tensor_recv_next, async_comm=async_comm)
[rank0]:E0603 16:23:37.656000 74334 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 97, in _run_p2pops
[rank0]:E0603 16:23:37.656000 74334 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     reqs = torch.distributed.batch_isend_irecv(ops)
[rank0]:E0603 16:23:37.656000 74334 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/distributed/distributed_c10d.py", line 2717, in batch_isend_irecv
[rank0]:E0603 16:23:37.656000 74334 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     p2p_op.op(
[rank0]:E0603 16:23:37.656000 74334 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/distributed/distributed_c10d.py", line 2366, in isend
[rank0]:E0603 16:23:37.656000 74334 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     return group.send([tensor], group_dst, tag)
[rank0]:E0603 16:23:37.656000 74334 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] torch.distributed.DistBackendError: NCCL error in: /skishore/github/pytorch/torch/csrc/distributed/c10d/NCCLUtils.cpp:77, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.25.1
[rank0]:E0603 16:23:37.656000 74334 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
[rank0]:E0603 16:23:37.656000 74334 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Last error:
[rank0]:E0603 16:23:37.656000 74334 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Error while creating shared memory segment /dev/shm/nccl-jjObkm (size 8257920), error: No space left on device (28)
[rank0]:E0603 16:23:37.656000 74334 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] 
[rank0]:E0603 16:23:37.656000 74334 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] To execute this test, run the following from the base repo dir:
[rank0]:E0603 16:23:37.656000 74334 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     PYTORCH_TEST_WITH_ROCM=1 python test_pipeline_parallel_fwd_bwd.py NcclPipelineParallelForwardBackwardTest.test_learning_pipelining_with_interleaving
[rank0]:E0603 16:23:37.656000 74334 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] 
[rank0]:E0603 16:23:37.656000 74334 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0
[rank0]:E0603 16:23:37.656000 74334 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]  exiting process 0 with exit code: 10
[rank4]:E0603 16:23:37.657000 74338 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Caught exception: 
[rank4]:E0603 16:23:37.657000 74338 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Traceback (most recent call last):
[rank4]:E0603 16:23:37.657000 74338 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/testing/_internal/common_distributed.py", line 734, in run_test
[rank4]:E0603 16:23:37.657000 74338 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     getattr(self, test_name)()
[rank4]:E0603 16:23:37.657000 74338 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/testing/_internal/common_distributed.py", line 607, in wrapper
[rank4]:E0603 16:23:37.657000 74338 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     fn()
[rank4]:E0603 16:23:37.657000 74338 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/testing/_internal/common_utils.py", line 3173, in wrapper
[rank4]:E0603 16:23:37.657000 74338 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     method(*args, **kwargs)
[rank4]:E0603 16:23:37.657000 74338 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py", line 260, in test_learning_pipelining_with_interleaving
[rank4]:E0603 16:23:37.657000 74338 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     self._forward_backward_test_impl(
[rank4]:E0603 16:23:37.657000 74338 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py", line 180, in _forward_backward_test_impl
[rank4]:E0603 16:23:37.657000 74338 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     loss = fwd_bwd_func(
[rank4]:E0603 16:23:37.657000 74338 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_with_interleaving.py", line 219, in _forward_backward_pipelining_with_interleaving
[rank4]:E0603 16:23:37.657000 74338 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     p2p_communication.recv_forward(
[rank4]:E0603 16:23:37.657000 74338 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 342, in recv_forward
[rank4]:E0603 16:23:37.657000 74338 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     input_tensor, _ = _communicate(
[rank4]:E0603 16:23:37.657000 74338 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 259, in _communicate
[rank4]:E0603 16:23:37.657000 74338 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     tensor_send_prev_req, tensor_recv_prev_req, tensor_send_next_req, tensor_recv_next_req = _run_p2pops(tensor_send_prev, tensor_send_next, tensor_recv_prev, tensor_recv_next, async_comm=async_comm)
[rank4]:E0603 16:23:37.657000 74338 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 97, in _run_p2pops
[rank4]:E0603 16:23:37.657000 74338 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     reqs = torch.distributed.batch_isend_irecv(ops)
[rank4]:E0603 16:23:37.657000 74338 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/distributed/distributed_c10d.py", line 2717, in batch_isend_irecv
[rank4]:E0603 16:23:37.657000 74338 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     p2p_op.op(
[rank4]:E0603 16:23:37.657000 74338 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/distributed/distributed_c10d.py", line 2411, in irecv
[rank4]:E0603 16:23:37.657000 74338 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     return group.recv([tensor], group_src, tag)
[rank4]:E0603 16:23:37.657000 74338 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] torch.distributed.DistBackendError: NCCL error in: /skishore/github/pytorch/torch/csrc/distributed/c10d/NCCLUtils.cpp:77, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.25.1
[rank4]:E0603 16:23:37.657000 74338 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
[rank4]:E0603 16:23:37.657000 74338 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Last error:
[rank4]:E0603 16:23:37.657000 74338 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Error while creating shared memory segment /dev/shm/nccl-IoM3re (size 8257920), error: No space left on device (28)
[rank4]:E0603 16:23:37.657000 74338 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] 
[rank4]:E0603 16:23:37.657000 74338 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] To execute this test, run the following from the base repo dir:
[rank4]:E0603 16:23:37.657000 74338 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     PYTORCH_TEST_WITH_ROCM=1 python test_pipeline_parallel_fwd_bwd.py NcclPipelineParallelForwardBackwardTest.test_learning_pipelining_with_interleaving
[rank4]:E0603 16:23:37.657000 74338 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] 
[rank4]:E0603 16:23:37.657000 74338 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0
[rank4]:E0603 16:23:37.657000 74338 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]  exiting process 4 with exit code: 10
[rank2]:E0603 16:23:37.657000 74336 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Caught exception: 
[rank2]:E0603 16:23:37.657000 74336 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Traceback (most recent call last):
[rank2]:E0603 16:23:37.657000 74336 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/testing/_internal/common_distributed.py", line 734, in run_test
[rank2]:E0603 16:23:37.657000 74336 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     getattr(self, test_name)()
[rank2]:E0603 16:23:37.657000 74336 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/testing/_internal/common_distributed.py", line 607, in wrapper
[rank2]:E0603 16:23:37.657000 74336 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     fn()
[rank2]:E0603 16:23:37.657000 74336 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/testing/_internal/common_utils.py", line 3173, in wrapper
[rank2]:E0603 16:23:37.657000 74336 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     method(*args, **kwargs)
[rank2]:E0603 16:23:37.657000 74336 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py", line 260, in test_learning_pipelining_with_interleaving
[rank2]:E0603 16:23:37.657000 74336 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     self._forward_backward_test_impl(
[rank2]:E0603 16:23:37.657000 74336 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py", line 180, in _forward_backward_test_impl
[rank2]:E0603 16:23:37.657000 74336 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     loss = fwd_bwd_func(
[rank2]:E0603 16:23:37.657000 74336 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_with_interleaving.py", line 219, in _forward_backward_pipelining_with_interleaving
[rank2]:E0603 16:23:37.657000 74336 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     p2p_communication.recv_forward(
[rank2]:E0603 16:23:37.657000 74336 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 342, in recv_forward
[rank2]:E0603 16:23:37.657000 74336 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     input_tensor, _ = _communicate(
[rank2]:E0603 16:23:37.657000 74336 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 259, in _communicate
[rank2]:E0603 16:23:37.657000 74336 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     tensor_send_prev_req, tensor_recv_prev_req, tensor_send_next_req, tensor_recv_next_req = _run_p2pops(tensor_send_prev, tensor_send_next, tensor_recv_prev, tensor_recv_next, async_comm=async_comm)
[rank2]:E0603 16:23:37.657000 74336 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 97, in _run_p2pops
[rank2]:E0603 16:23:37.657000 74336 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     reqs = torch.distributed.batch_isend_irecv(ops)
[rank2]:E0603 16:23:37.657000 74336 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/distributed/distributed_c10d.py", line 2717, in batch_isend_irecv
[rank2]:E0603 16:23:37.657000 74336 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     p2p_op.op(
[rank2]:E0603 16:23:37.657000 74336 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/distributed/distributed_c10d.py", line 2411, in irecv
[rank2]:E0603 16:23:37.657000 74336 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     return group.recv([tensor], group_src, tag)
[rank2]:E0603 16:23:37.657000 74336 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] torch.distributed.DistBackendError: NCCL error in: /skishore/github/pytorch/torch/csrc/distributed/c10d/NCCLUtils.cpp:77, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.25.1
[rank2]:E0603 16:23:37.657000 74336 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
[rank2]:E0603 16:23:37.657000 74336 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Last error:
[rank2]:E0603 16:23:37.657000 74336 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Error while creating shared memory segment /dev/shm/nccl-tJxE3b (size 8257920), error: No space left on device (28)
[rank2]:E0603 16:23:37.657000 74336 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] 
[rank2]:E0603 16:23:37.657000 74336 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] To execute this test, run the following from the base repo dir:
[rank2]:E0603 16:23:37.657000 74336 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     PYTORCH_TEST_WITH_ROCM=1 python test_pipeline_parallel_fwd_bwd.py NcclPipelineParallelForwardBackwardTest.test_learning_pipelining_with_interleaving
[rank2]:E0603 16:23:37.657000 74336 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] 
[rank2]:E0603 16:23:37.657000 74336 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0
[rank2]:E0603 16:23:37.657000 74336 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]  exiting process 2 with exit code: 10
[rank6]:E0603 16:23:37.659000 74340 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Caught exception: 
[rank6]:E0603 16:23:37.659000 74340 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Traceback (most recent call last):
[rank6]:E0603 16:23:37.659000 74340 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/testing/_internal/common_distributed.py", line 734, in run_test
[rank6]:E0603 16:23:37.659000 74340 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     getattr(self, test_name)()
[rank6]:E0603 16:23:37.659000 74340 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/testing/_internal/common_distributed.py", line 607, in wrapper
[rank6]:E0603 16:23:37.659000 74340 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     fn()
[rank6]:E0603 16:23:37.659000 74340 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/testing/_internal/common_utils.py", line 3173, in wrapper
[rank6]:E0603 16:23:37.659000 74340 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     method(*args, **kwargs)
[rank6]:E0603 16:23:37.659000 74340 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py", line 260, in test_learning_pipelining_with_interleaving
[rank6]:E0603 16:23:37.659000 74340 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     self._forward_backward_test_impl(
[rank6]:E0603 16:23:37.659000 74340 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py", line 180, in _forward_backward_test_impl
[rank6]:E0603 16:23:37.659000 74340 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     loss = fwd_bwd_func(
[rank6]:E0603 16:23:37.659000 74340 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_with_interleaving.py", line 219, in _forward_backward_pipelining_with_interleaving
[rank6]:E0603 16:23:37.659000 74340 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     p2p_communication.recv_forward(
[rank6]:E0603 16:23:37.659000 74340 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 342, in recv_forward
[rank6]:E0603 16:23:37.659000 74340 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     input_tensor, _ = _communicate(
[rank6]:E0603 16:23:37.659000 74340 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 259, in _communicate
[rank6]:E0603 16:23:37.659000 74340 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     tensor_send_prev_req, tensor_recv_prev_req, tensor_send_next_req, tensor_recv_next_req = _run_p2pops(tensor_send_prev, tensor_send_next, tensor_recv_prev, tensor_recv_next, async_comm=async_comm)
[rank6]:E0603 16:23:37.659000 74340 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 97, in _run_p2pops
[rank6]:E0603 16:23:37.659000 74340 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     reqs = torch.distributed.batch_isend_irecv(ops)
[rank6]:E0603 16:23:37.659000 74340 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/distributed/distributed_c10d.py", line 2717, in batch_isend_irecv
[rank6]:E0603 16:23:37.659000 74340 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     p2p_op.op(
[rank6]:E0603 16:23:37.659000 74340 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/distributed/distributed_c10d.py", line 2411, in irecv
[rank6]:E0603 16:23:37.659000 74340 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     return group.recv([tensor], group_src, tag)
[rank6]:E0603 16:23:37.659000 74340 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] torch.distributed.DistBackendError: NCCL error in: /skishore/github/pytorch/torch/csrc/distributed/c10d/NCCLUtils.cpp:77, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.25.1
[rank6]:E0603 16:23:37.659000 74340 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
[rank6]:E0603 16:23:37.659000 74340 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Last error:
[rank6]:E0603 16:23:37.659000 74340 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Error while creating shared memory segment /dev/shm/nccl-4zQyZf (size 8257920), error: No space left on device (28)
[rank6]:E0603 16:23:37.659000 74340 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] 
[rank6]:E0603 16:23:37.659000 74340 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] To execute this test, run the following from the base repo dir:
[rank6]:E0603 16:23:37.659000 74340 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     PYTORCH_TEST_WITH_ROCM=1 python test_pipeline_parallel_fwd_bwd.py NcclPipelineParallelForwardBackwardTest.test_learning_pipelining_with_interleaving
[rank6]:E0603 16:23:37.659000 74340 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] 
[rank6]:E0603 16:23:37.659000 74340 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0
[rank6]:E0603 16:23:37.659000 74340 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]  exiting process 6 with exit code: 10
[rank2]:[W603 16:23:37.958873085 ProcessGroupNCCL.cpp:1553] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank0]:[W603 16:23:37.997044803 ProcessGroupNCCL.cpp:1553] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank4]:[W603 16:23:37.998627199 ProcessGroupNCCL.cpp:1553] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank6]:[W603 16:23:38.116992402 ProcessGroupNCCL.cpp:1553] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank3]:[W603 16:23:38.516222369 ProcessGroupNCCL.cpp:1553] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank5]:[W603 16:23:38.541465240 ProcessGroupNCCL.cpp:1553] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank1]:[W603 16:23:38.748065229 ProcessGroupNCCL.cpp:1553] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank7]:[W603 16:23:38.794640305 ProcessGroupNCCL.cpp:1553] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
ERROR
test_learning_pipelining_without_interleaving (test_pipeline_parallel_fwd_bwd.NcclPipelineParallelForwardBackwardTest) ... Process 3 terminated with exit code 10, terminating remaining processes.
[rank0]:[W603 16:24:01.300922058 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank3]:[W603 16:24:01.310179142 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank4]:[W603 16:24:01.313455880 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 4]  using GPU 4 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank1]:[W603 16:24:02.201775634 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank6]:[W603 16:24:02.208507283 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 6]  using GPU 6 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank2]:[W603 16:24:02.208962478 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank7]:[W603 16:24:02.209742284 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 7]  using GPU 7 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank5]:[W603 16:24:02.219031365 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 5]  using GPU 5 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/utils.py:81: UserWarning: This function is only for unittest
  warnings.warn("This function is only for unittest")
[dist init] rank = 4, world_size = 8
 > number of parameters on (tensor, pipeline) model parallel rank (0, 2): 1056
[dist init] rank = 0, world_size = 8
 > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 1056
[dist init] rank = 2, world_size = 8
 > number of parameters on (tensor, pipeline) model parallel rank (0, 1): 1056
[dist init] rank = 6, world_size = 8
 > number of parameters on (tensor, pipeline) model parallel rank (0, 3): 1056
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/common.py:295: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/common.py:295: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/common.py:295: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/common.py:295: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/common.py:295: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/common.py:295: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/common.py:295: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/common.py:295: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(
 > number of parameters on (tensor, pipeline) model parallel rank (0, 3): 1056
 > number of parameters on (tensor, pipeline) model parallel rank (0, 2): 1056
 > number of parameters on (tensor, pipeline) model parallel rank (0, 1): 1056
 > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 1056
 > number of parameters on (tensor, pipeline) model parallel rank (0, 3): 1056
 > number of parameters on (tensor, pipeline) model parallel rank (0, 2): 1056
 > number of parameters on (tensor, pipeline) model parallel rank (0, 1): 1056
 > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 1056
/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py:208: UserWarning: Converting a tensor with requires_grad=True to a scalar may lead to unexpected behavior.
Consider using tensor.detach() first. (Triggered internally at /skishore/github/pytorch/aten/src/ATen/native/Scalar.cpp:22.)
  self.assertEqual(x.item() / microbatch_size, target_loss.item())
/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py:208: UserWarning: Converting a tensor with requires_grad=True to a scalar may lead to unexpected behavior.
Consider using tensor.detach() first. (Triggered internally at /skishore/github/pytorch/aten/src/ATen/native/Scalar.cpp:22.)
  self.assertEqual(x.item() / microbatch_size, target_loss.item())
 > number of parameters on (tensor, pipeline) model parallel rank (0, 2): 1056
 > number of parameters on (tensor, pipeline) model parallel rank (0, 3): 1056
 > number of parameters on (tensor, pipeline) model parallel rank (0, 1): 1056
 > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 1056
/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py:126: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  torch.cuda.amp.GradScaler(init_scale=4.0)
 > number of parameters on (tensor, pipeline) model parallel rank (0, 2): 1056
/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py:126: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  torch.cuda.amp.GradScaler(init_scale=4.0)
 > number of parameters on (tensor, pipeline) model parallel rank (0, 3): 1056
/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py:126: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  torch.cuda.amp.GradScaler(init_scale=4.0)
/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py:126: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  torch.cuda.amp.GradScaler(init_scale=4.0)
/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py:126: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  torch.cuda.amp.GradScaler(init_scale=4.0)
 > number of parameters on (tensor, pipeline) model parallel rank (0, 1): 1056
/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py:126: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  torch.cuda.amp.GradScaler(init_scale=4.0)
/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py:126: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  torch.cuda.amp.GradScaler(init_scale=4.0)
/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py:126: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  torch.cuda.amp.GradScaler(init_scale=4.0)
 > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 1056
 > number of parameters on (tensor, pipeline) model parallel rank (0, 3): 1056
 > number of parameters on (tensor, pipeline) model parallel rank (0, 2): 1056
 > number of parameters on (tensor, pipeline) model parallel rank (0, 1): 1056
 > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 1056
 > number of parameters on (tensor, pipeline) model parallel rank (0, 3): 1056
 > number of parameters on (tensor, pipeline) model parallel rank (0, 2): 1056
 > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 1056
 > number of parameters on (tensor, pipeline) model parallel rank (0, 1): 1056
 > number of parameters on (tensor, pipeline) model parallel rank (0, 3): 1056
 > number of parameters on (tensor, pipeline) model parallel rank (0, 2): 1056
 > number of parameters on (tensor, pipeline) model parallel rank (0, 1): 1056
 > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 1056
[rank5]:E0603 16:26:09.377000 78484 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Caught exception: 
[rank5]:E0603 16:26:09.377000 78484 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Traceback (most recent call last):
[rank5]:E0603 16:26:09.377000 78484 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/testing/_internal/common_distributed.py", line 734, in run_test
[rank5]:E0603 16:26:09.377000 78484 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     getattr(self, test_name)()
[rank5]:E0603 16:26:09.377000 78484 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/testing/_internal/common_distributed.py", line 607, in wrapper
[rank5]:E0603 16:26:09.377000 78484 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     fn()
[rank5]:E0603 16:26:09.377000 78484 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/testing/_internal/common_utils.py", line 3173, in wrapper
[rank5]:E0603 16:26:09.377000 78484 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     method(*args, **kwargs)
[rank5]:E0603 16:26:09.377000 78484 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py", line 239, in test_learning_pipelining_without_interleaving
[rank5]:E0603 16:26:09.377000 78484 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     self._forward_backward_test_impl(
[rank5]:E0603 16:26:09.377000 78484 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py", line 180, in _forward_backward_test_impl
[rank5]:E0603 16:26:09.377000 78484 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     loss = fwd_bwd_func(
[rank5]:E0603 16:26:09.377000 78484 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_without_interleaving.py", line 332, in forward_backward_pipelining_without_interleaving
[rank5]:E0603 16:26:09.377000 78484 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     input_tensor = recv_forward(
[rank5]:E0603 16:26:09.377000 78484 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_without_interleaving.py", line 99, in recv_forward
[rank5]:E0603 16:26:09.377000 78484 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     p2p_communication.recv_forward(
[rank5]:E0603 16:26:09.377000 78484 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 342, in recv_forward
[rank5]:E0603 16:26:09.377000 78484 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     input_tensor, _ = _communicate(
[rank5]:E0603 16:26:09.377000 78484 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 259, in _communicate
[rank5]:E0603 16:26:09.377000 78484 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     tensor_send_prev_req, tensor_recv_prev_req, tensor_send_next_req, tensor_recv_next_req = _run_p2pops(tensor_send_prev, tensor_send_next, tensor_recv_prev, tensor_recv_next, async_comm=async_comm)
[rank5]:E0603 16:26:09.377000 78484 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 97, in _run_p2pops
[rank5]:E0603 16:26:09.377000 78484 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     reqs = torch.distributed.batch_isend_irecv(ops)
[rank5]:E0603 16:26:09.377000 78484 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/distributed/distributed_c10d.py", line 2717, in batch_isend_irecv
[rank5]:E0603 16:26:09.377000 78484 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     p2p_op.op(
[rank5]:E0603 16:26:09.377000 78484 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/distributed/distributed_c10d.py", line 2411, in irecv
[rank5]:E0603 16:26:09.377000 78484 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     return group.recv([tensor], group_src, tag)
[rank5]:E0603 16:26:09.377000 78484 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] torch.distributed.DistBackendError: NCCL error in: /skishore/github/pytorch/torch/csrc/distributed/c10d/NCCLUtils.cpp:77, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.25.1
[rank5]:E0603 16:26:09.377000 78484 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
[rank5]:E0603 16:26:09.377000 78484 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Last error:
[rank5]:E0603 16:26:09.377000 78484 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Error while creating shared memory segment /dev/shm/nccl-2fZGs1 (size 8257920), error: No space left on device (28)
[rank5]:E0603 16:26:09.377000 78484 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] 
[rank5]:E0603 16:26:09.377000 78484 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] To execute this test, run the following from the base repo dir:
[rank5]:E0603 16:26:09.377000 78484 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     PYTORCH_TEST_WITH_ROCM=1 python test_pipeline_parallel_fwd_bwd.py NcclPipelineParallelForwardBackwardTest.test_learning_pipelining_without_interleaving
[rank5]:E0603 16:26:09.377000 78484 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] 
[rank5]:E0603 16:26:09.377000 78484 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0
[rank5]:E0603 16:26:09.377000 78484 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]  exiting process 5 with exit code: 10
[dist init] rank = 5, world_size = 8
[rank3]:E0603 16:26:09.378000 78482 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Caught exception: 
[rank3]:E0603 16:26:09.378000 78482 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Traceback (most recent call last):
[rank3]:E0603 16:26:09.378000 78482 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/testing/_internal/common_distributed.py", line 734, in run_test
[rank3]:E0603 16:26:09.378000 78482 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     getattr(self, test_name)()
[rank3]:E0603 16:26:09.378000 78482 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/testing/_internal/common_distributed.py", line 607, in wrapper
[rank3]:E0603 16:26:09.378000 78482 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     fn()
[rank3]:E0603 16:26:09.378000 78482 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/testing/_internal/common_utils.py", line 3173, in wrapper
[rank3]:E0603 16:26:09.378000 78482 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     method(*args, **kwargs)
[rank3]:E0603 16:26:09.378000 78482 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py", line 239, in test_learning_pipelining_without_interleaving
[rank3]:E0603 16:26:09.378000 78482 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     self._forward_backward_test_impl(
[rank3]:E0603 16:26:09.378000 78482 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py", line 180, in _forward_backward_test_impl
[rank3]:E0603 16:26:09.378000 78482 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     loss = fwd_bwd_func(
[rank3]:E0603 16:26:09.378000 78482 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_without_interleaving.py", line 332, in forward_backward_pipelining_without_interleaving
[rank3]:E0603 16:26:09.378000 78482 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     input_tensor = recv_forward(
[rank3]:E0603 16:26:09.378000 78482 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_without_interleaving.py", line 99, in recv_forward
[rank3]:E0603 16:26:09.378000 78482 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     p2p_communication.recv_forward(
[rank3]:E0603 16:26:09.378000 78482 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 342, in recv_forward
[rank3]:E0603 16:26:09.378000 78482 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     input_tensor, _ = _communicate(
[rank3]:E0603 16:26:09.378000 78482 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 259, in _communicate
[rank3]:E0603 16:26:09.378000 78482 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     tensor_send_prev_req, tensor_recv_prev_req, tensor_send_next_req, tensor_recv_next_req = _run_p2pops(tensor_send_prev, tensor_send_next, tensor_recv_prev, tensor_recv_next, async_comm=async_comm)
[rank3]:E0603 16:26:09.378000 78482 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 97, in _run_p2pops
[rank3]:E0603 16:26:09.378000 78482 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     reqs = torch.distributed.batch_isend_irecv(ops)
[rank3]:E0603 16:26:09.378000 78482 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/distributed/distributed_c10d.py", line 2717, in batch_isend_irecv
[rank3]:E0603 16:26:09.378000 78482 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     p2p_op.op(
[rank3]:E0603 16:26:09.378000 78482 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/distributed/distributed_c10d.py", line 2411, in irecv
[rank3]:E0603 16:26:09.378000 78482 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     return group.recv([tensor], group_src, tag)
[rank3]:E0603 16:26:09.378000 78482 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] torch.distributed.DistBackendError: NCCL error in: /skishore/github/pytorch/torch/csrc/distributed/c10d/NCCLUtils.cpp:77, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.25.1
[rank3]:E0603 16:26:09.378000 78482 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
[rank3]:E0603 16:26:09.378000 78482 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Last error:
[rank3]:E0603 16:26:09.378000 78482 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Error while creating shared memory segment /dev/shm/nccl-GPa5Ic (size 8257920), error: No space left on device (28)
[rank3]:E0603 16:26:09.378000 78482 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] 
[rank3]:E0603 16:26:09.378000 78482 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] To execute this test, run the following from the base repo dir:
[rank3]:E0603 16:26:09.378000 78482 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     PYTORCH_TEST_WITH_ROCM=1 python test_pipeline_parallel_fwd_bwd.py NcclPipelineParallelForwardBackwardTest.test_learning_pipelining_without_interleaving
[rank3]:E0603 16:26:09.378000 78482 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] 
[rank3]:E0603 16:26:09.378000 78482 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0
[rank3]:E0603 16:26:09.378000 78482 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]  exiting process 3 with exit code: 10
[rank1]:E0603 16:26:09.378000 78480 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Caught exception: 
[rank1]:E0603 16:26:09.378000 78480 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Traceback (most recent call last):
[rank1]:E0603 16:26:09.378000 78480 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/testing/_internal/common_distributed.py", line 734, in run_test
[rank1]:E0603 16:26:09.378000 78480 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     getattr(self, test_name)()
[rank1]:E0603 16:26:09.378000 78480 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/testing/_internal/common_distributed.py", line 607, in wrapper
[rank1]:E0603 16:26:09.378000 78480 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     fn()
[rank1]:E0603 16:26:09.378000 78480 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/testing/_internal/common_utils.py", line 3173, in wrapper
[rank1]:E0603 16:26:09.378000 78480 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     method(*args, **kwargs)
[rank1]:E0603 16:26:09.378000 78480 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py", line 239, in test_learning_pipelining_without_interleaving
[rank1]:E0603 16:26:09.378000 78480 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     self._forward_backward_test_impl(
[rank1]:E0603 16:26:09.378000 78480 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py", line 180, in _forward_backward_test_impl
[rank1]:E0603 16:26:09.378000 78480 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     loss = fwd_bwd_func(
[rank1]:E0603 16:26:09.378000 78480 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_without_interleaving.py", line 349, in forward_backward_pipelining_without_interleaving
[rank1]:E0603 16:26:09.378000 78480 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     send_forward(
[rank1]:E0603 16:26:09.378000 78480 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_without_interleaving.py", line 145, in send_forward
[rank1]:E0603 16:26:09.378000 78480 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     p2p_communication.send_forward(
[rank1]:E0603 16:26:09.378000 78480 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 401, in send_forward
[rank1]:E0603 16:26:09.378000 78480 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     _communicate(
[rank1]:E0603 16:26:09.378000 78480 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 259, in _communicate
[rank1]:E0603 16:26:09.378000 78480 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     tensor_send_prev_req, tensor_recv_prev_req, tensor_send_next_req, tensor_recv_next_req = _run_p2pops(tensor_send_prev, tensor_send_next, tensor_recv_prev, tensor_recv_next, async_comm=async_comm)
[rank1]:E0603 16:26:09.378000 78480 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 97, in _run_p2pops
[rank1]:E0603 16:26:09.378000 78480 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     reqs = torch.distributed.batch_isend_irecv(ops)
[rank1]:E0603 16:26:09.378000 78480 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/distributed/distributed_c10d.py", line 2717, in batch_isend_irecv
[rank1]:E0603 16:26:09.378000 78480 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     p2p_op.op(
[rank1]:E0603 16:26:09.378000 78480 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/distributed/distributed_c10d.py", line 2366, in isend
[rank1]:E0603 16:26:09.378000 78480 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     return group.send([tensor], group_dst, tag)
[rank1]:E0603 16:26:09.378000 78480 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] torch.distributed.DistBackendError: NCCL error in: /skishore/github/pytorch/torch/csrc/distributed/c10d/NCCLUtils.cpp:77, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.25.1
[rank1]:E0603 16:26:09.378000 78480 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
[rank1]:E0603 16:26:09.378000 78480 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Last error:
[rank1]:E0603 16:26:09.378000 78480 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Error while creating shared memory segment /dev/shm/nccl-abqPSD (size 8257920), error: No space left on device (28)
[rank1]:E0603 16:26:09.378000 78480 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] 
[rank1]:E0603 16:26:09.378000 78480 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] To execute this test, run the following from the base repo dir:
[rank1]:E0603 16:26:09.378000 78480 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     PYTORCH_TEST_WITH_ROCM=1 python test_pipeline_parallel_fwd_bwd.py NcclPipelineParallelForwardBackwardTest.test_learning_pipelining_without_interleaving
[rank1]:E0603 16:26:09.378000 78480 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] 
[rank1]:E0603 16:26:09.378000 78480 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0
[rank1]:E0603 16:26:09.378000 78480 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]  exiting process 1 with exit code: 10
[rank7]:E0603 16:26:09.378000 78486 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Caught exception: 
[rank7]:E0603 16:26:09.378000 78486 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Traceback (most recent call last):
[rank7]:E0603 16:26:09.378000 78486 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/testing/_internal/common_distributed.py", line 734, in run_test
[rank7]:E0603 16:26:09.378000 78486 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     getattr(self, test_name)()
[rank7]:E0603 16:26:09.378000 78486 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/testing/_internal/common_distributed.py", line 607, in wrapper
[rank7]:E0603 16:26:09.378000 78486 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     fn()
[rank7]:E0603 16:26:09.378000 78486 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/testing/_internal/common_utils.py", line 3173, in wrapper
[rank7]:E0603 16:26:09.378000 78486 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     method(*args, **kwargs)
[rank7]:E0603 16:26:09.378000 78486 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py", line 239, in test_learning_pipelining_without_interleaving
[rank7]:E0603 16:26:09.378000 78486 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     self._forward_backward_test_impl(
[rank7]:E0603 16:26:09.378000 78486 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py", line 180, in _forward_backward_test_impl
[rank7]:E0603 16:26:09.378000 78486 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     loss = fwd_bwd_func(
[rank7]:E0603 16:26:09.378000 78486 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_without_interleaving.py", line 367, in forward_backward_pipelining_without_interleaving
[rank7]:E0603 16:26:09.378000 78486 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     input_tensor: List[Union[None, torch.Tensor, FutureTensor]] = recv_forward(tensor_shapes=recv_tensor_shapes, dtype=dtype, async_comm=async_comm)
[rank7]:E0603 16:26:09.378000 78486 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_without_interleaving.py", line 99, in recv_forward
[rank7]:E0603 16:26:09.378000 78486 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     p2p_communication.recv_forward(
[rank7]:E0603 16:26:09.378000 78486 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 342, in recv_forward
[rank7]:E0603 16:26:09.378000 78486 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     input_tensor, _ = _communicate(
[rank7]:E0603 16:26:09.378000 78486 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 259, in _communicate
[rank7]:E0603 16:26:09.378000 78486 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     tensor_send_prev_req, tensor_recv_prev_req, tensor_send_next_req, tensor_recv_next_req = _run_p2pops(tensor_send_prev, tensor_send_next, tensor_recv_prev, tensor_recv_next, async_comm=async_comm)
[rank7]:E0603 16:26:09.378000 78486 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 97, in _run_p2pops
[rank7]:E0603 16:26:09.378000 78486 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     reqs = torch.distributed.batch_isend_irecv(ops)
[rank7]:E0603 16:26:09.378000 78486 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/distributed/distributed_c10d.py", line 2717, in batch_isend_irecv
[rank7]:E0603 16:26:09.378000 78486 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     p2p_op.op(
[rank7]:E0603 16:26:09.378000 78486 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/distributed/distributed_c10d.py", line 2411, in irecv
[rank7]:E0603 16:26:09.378000 78486 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     return group.recv([tensor], group_src, tag)
[rank7]:E0603 16:26:09.378000 78486 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] torch.distributed.DistBackendError: NCCL error in: /skishore/github/pytorch/torch/csrc/distributed/c10d/NCCLUtils.cpp:77, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.25.1
[rank7]:E0603 16:26:09.378000 78486 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
[rank7]:E0603 16:26:09.378000 78486 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Last error:
[rank7]:E0603 16:26:09.378000 78486 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Error while creating shared memory segment /dev/shm/nccl-PAeuzA (size 8257920), error: No space left on device (28)
[rank7]:E0603 16:26:09.378000 78486 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] 
[rank7]:E0603 16:26:09.378000 78486 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] To execute this test, run the following from the base repo dir:
[rank7]:E0603 16:26:09.378000 78486 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     PYTORCH_TEST_WITH_ROCM=1 python test_pipeline_parallel_fwd_bwd.py NcclPipelineParallelForwardBackwardTest.test_learning_pipelining_without_interleaving
[rank7]:E0603 16:26:09.378000 78486 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] 
[rank7]:E0603 16:26:09.378000 78486 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0
[rank7]:E0603 16:26:09.378000 78486 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]  exiting process 7 with exit code: 10
[dist init] rank = 1, world_size = 8
[dist init] rank = 3, world_size = 8
[dist init] rank = 7, world_size = 8
[rank6]:E0603 16:26:10.117000 78485 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Caught exception: 
[rank6]:E0603 16:26:10.117000 78485 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Traceback (most recent call last):
[rank6]:E0603 16:26:10.117000 78485 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/testing/_internal/common_distributed.py", line 734, in run_test
[rank6]:E0603 16:26:10.117000 78485 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     getattr(self, test_name)()
[rank6]:E0603 16:26:10.117000 78485 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/testing/_internal/common_distributed.py", line 607, in wrapper
[rank6]:E0603 16:26:10.117000 78485 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     fn()
[rank6]:E0603 16:26:10.117000 78485 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/testing/_internal/common_utils.py", line 3173, in wrapper
[rank6]:E0603 16:26:10.117000 78485 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     method(*args, **kwargs)
[rank6]:E0603 16:26:10.117000 78485 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py", line 239, in test_learning_pipelining_without_interleaving
[rank6]:E0603 16:26:10.117000 78485 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     self._forward_backward_test_impl(
[rank6]:E0603 16:26:10.117000 78485 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py", line 180, in _forward_backward_test_impl
[rank6]:E0603 16:26:10.117000 78485 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     loss = fwd_bwd_func(
[rank6]:E0603 16:26:10.117000 78485 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_without_interleaving.py", line 367, in forward_backward_pipelining_without_interleaving
[rank6]:E0603 16:26:10.117000 78485 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     input_tensor: List[Union[None, torch.Tensor, FutureTensor]] = recv_forward(tensor_shapes=recv_tensor_shapes, dtype=dtype, async_comm=async_comm)
[rank6]:E0603 16:26:10.117000 78485 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_without_interleaving.py", line 99, in recv_forward
[rank6]:E0603 16:26:10.117000 78485 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     p2p_communication.recv_forward(
[rank6]:E0603 16:26:10.117000 78485 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 342, in recv_forward
[rank6]:E0603 16:26:10.117000 78485 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     input_tensor, _ = _communicate(
[rank6]:E0603 16:26:10.117000 78485 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 259, in _communicate
[rank6]:E0603 16:26:10.117000 78485 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     tensor_send_prev_req, tensor_recv_prev_req, tensor_send_next_req, tensor_recv_next_req = _run_p2pops(tensor_send_prev, tensor_send_next, tensor_recv_prev, tensor_recv_next, async_comm=async_comm)
[rank6]:E0603 16:26:10.117000 78485 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 97, in _run_p2pops
[rank6]:E0603 16:26:10.117000 78485 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     reqs = torch.distributed.batch_isend_irecv(ops)
[rank6]:E0603 16:26:10.117000 78485 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/distributed/distributed_c10d.py", line 2717, in batch_isend_irecv
[rank6]:E0603 16:26:10.117000 78485 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     p2p_op.op(
[rank6]:E0603 16:26:10.117000 78485 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/distributed/distributed_c10d.py", line 2411, in irecv
[rank6]:E0603 16:26:10.117000 78485 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     return group.recv([tensor], group_src, tag)
[rank6]:E0603 16:26:10.117000 78485 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] torch.distributed.DistBackendError: NCCL error in: /skishore/github/pytorch/torch/csrc/distributed/c10d/NCCLUtils.cpp:77, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.25.1
[rank6]:E0603 16:26:10.117000 78485 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
[rank6]:E0603 16:26:10.117000 78485 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Last error:
[rank6]:E0603 16:26:10.117000 78485 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Error while creating shared memory segment /dev/shm/nccl-UuuwZX (size 8257920), error: No space left on device (28)
[rank6]:E0603 16:26:10.117000 78485 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] 
[rank6]:E0603 16:26:10.117000 78485 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] To execute this test, run the following from the base repo dir:
[rank6]:E0603 16:26:10.117000 78485 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     PYTORCH_TEST_WITH_ROCM=1 python test_pipeline_parallel_fwd_bwd.py NcclPipelineParallelForwardBackwardTest.test_learning_pipelining_without_interleaving
[rank6]:E0603 16:26:10.117000 78485 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] 
[rank6]:E0603 16:26:10.117000 78485 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0
[rank6]:E0603 16:26:10.117000 78485 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]  exiting process 6 with exit code: 10
[rank2]:E0603 16:26:10.118000 78481 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Caught exception: 
[rank2]:E0603 16:26:10.118000 78481 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Traceback (most recent call last):
[rank2]:E0603 16:26:10.118000 78481 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/testing/_internal/common_distributed.py", line 734, in run_test
[rank2]:E0603 16:26:10.118000 78481 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     getattr(self, test_name)()
[rank2]:E0603 16:26:10.118000 78481 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/testing/_internal/common_distributed.py", line 607, in wrapper
[rank2]:E0603 16:26:10.118000 78481 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     fn()
[rank2]:E0603 16:26:10.118000 78481 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/testing/_internal/common_utils.py", line 3173, in wrapper
[rank2]:E0603 16:26:10.118000 78481 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     method(*args, **kwargs)
[rank2]:E0603 16:26:10.118000 78481 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py", line 239, in test_learning_pipelining_without_interleaving
[rank2]:E0603 16:26:10.118000 78481 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     self._forward_backward_test_impl(
[rank2]:E0603 16:26:10.118000 78481 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py", line 180, in _forward_backward_test_impl
[rank2]:E0603 16:26:10.118000 78481 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     loss = fwd_bwd_func(
[rank2]:E0603 16:26:10.118000 78481 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_without_interleaving.py", line 332, in forward_backward_pipelining_without_interleaving
[rank2]:E0603 16:26:10.118000 78481 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     input_tensor = recv_forward(
[rank2]:E0603 16:26:10.118000 78481 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_without_interleaving.py", line 99, in recv_forward
[rank2]:E0603 16:26:10.118000 78481 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     p2p_communication.recv_forward(
[rank2]:E0603 16:26:10.118000 78481 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 342, in recv_forward
[rank2]:E0603 16:26:10.118000 78481 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     input_tensor, _ = _communicate(
[rank2]:E0603 16:26:10.118000 78481 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 259, in _communicate
[rank2]:E0603 16:26:10.118000 78481 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     tensor_send_prev_req, tensor_recv_prev_req, tensor_send_next_req, tensor_recv_next_req = _run_p2pops(tensor_send_prev, tensor_send_next, tensor_recv_prev, tensor_recv_next, async_comm=async_comm)
[rank2]:E0603 16:26:10.118000 78481 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 97, in _run_p2pops
[rank2]:E0603 16:26:10.118000 78481 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     reqs = torch.distributed.batch_isend_irecv(ops)
[rank2]:E0603 16:26:10.118000 78481 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/distributed/distributed_c10d.py", line 2717, in batch_isend_irecv
[rank2]:E0603 16:26:10.118000 78481 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     p2p_op.op(
[rank2]:E0603 16:26:10.118000 78481 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/distributed/distributed_c10d.py", line 2411, in irecv
[rank2]:E0603 16:26:10.118000 78481 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     return group.recv([tensor], group_src, tag)
[rank2]:E0603 16:26:10.118000 78481 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] torch.distributed.DistBackendError: NCCL error in: /skishore/github/pytorch/torch/csrc/distributed/c10d/NCCLUtils.cpp:77, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.25.1
[rank2]:E0603 16:26:10.118000 78481 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
[rank2]:E0603 16:26:10.118000 78481 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Last error:
[rank2]:E0603 16:26:10.118000 78481 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Error while creating shared memory segment /dev/shm/nccl-jvUOv9 (size 8257920), error: No space left on device (28)
[rank2]:E0603 16:26:10.118000 78481 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] 
[rank2]:E0603 16:26:10.118000 78481 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] To execute this test, run the following from the base repo dir:
[rank2]:E0603 16:26:10.118000 78481 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     PYTORCH_TEST_WITH_ROCM=1 python test_pipeline_parallel_fwd_bwd.py NcclPipelineParallelForwardBackwardTest.test_learning_pipelining_without_interleaving
[rank2]:E0603 16:26:10.118000 78481 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] 
[rank2]:E0603 16:26:10.118000 78481 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0
[rank2]:E0603 16:26:10.118000 78481 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]  exiting process 2 with exit code: 10
[rank4]:E0603 16:26:10.118000 78483 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Caught exception: 
[rank4]:E0603 16:26:10.118000 78483 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Traceback (most recent call last):
[rank4]:E0603 16:26:10.118000 78483 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/testing/_internal/common_distributed.py", line 734, in run_test
[rank4]:E0603 16:26:10.118000 78483 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     getattr(self, test_name)()
[rank4]:E0603 16:26:10.118000 78483 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/testing/_internal/common_distributed.py", line 607, in wrapper
[rank4]:E0603 16:26:10.118000 78483 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     fn()
[rank4]:E0603 16:26:10.118000 78483 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/testing/_internal/common_utils.py", line 3173, in wrapper
[rank4]:E0603 16:26:10.118000 78483 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     method(*args, **kwargs)
[rank4]:E0603 16:26:10.118000 78483 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py", line 239, in test_learning_pipelining_without_interleaving
[rank4]:E0603 16:26:10.118000 78483 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     self._forward_backward_test_impl(
[rank4]:E0603 16:26:10.118000 78483 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py", line 180, in _forward_backward_test_impl
[rank4]:E0603 16:26:10.118000 78483 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     loss = fwd_bwd_func(
[rank4]:E0603 16:26:10.118000 78483 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_without_interleaving.py", line 332, in forward_backward_pipelining_without_interleaving
[rank4]:E0603 16:26:10.118000 78483 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     input_tensor = recv_forward(
[rank4]:E0603 16:26:10.118000 78483 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_without_interleaving.py", line 99, in recv_forward
[rank4]:E0603 16:26:10.118000 78483 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     p2p_communication.recv_forward(
[rank4]:E0603 16:26:10.118000 78483 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 342, in recv_forward
[rank4]:E0603 16:26:10.118000 78483 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     input_tensor, _ = _communicate(
[rank4]:E0603 16:26:10.118000 78483 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 259, in _communicate
[rank4]:E0603 16:26:10.118000 78483 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     tensor_send_prev_req, tensor_recv_prev_req, tensor_send_next_req, tensor_recv_next_req = _run_p2pops(tensor_send_prev, tensor_send_next, tensor_recv_prev, tensor_recv_next, async_comm=async_comm)
[rank4]:E0603 16:26:10.118000 78483 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 97, in _run_p2pops
[rank4]:E0603 16:26:10.118000 78483 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     reqs = torch.distributed.batch_isend_irecv(ops)
[rank4]:E0603 16:26:10.118000 78483 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/distributed/distributed_c10d.py", line 2717, in batch_isend_irecv
[rank4]:E0603 16:26:10.118000 78483 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     p2p_op.op(
[rank4]:E0603 16:26:10.118000 78483 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/distributed/distributed_c10d.py", line 2411, in irecv
[rank4]:E0603 16:26:10.118000 78483 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     return group.recv([tensor], group_src, tag)
[rank4]:E0603 16:26:10.118000 78483 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] torch.distributed.DistBackendError: NCCL error in: /skishore/github/pytorch/torch/csrc/distributed/c10d/NCCLUtils.cpp:77, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.25.1
[rank4]:E0603 16:26:10.118000 78483 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
[rank4]:E0603 16:26:10.118000 78483 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Last error:
[rank4]:E0603 16:26:10.118000 78483 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Error while creating shared memory segment /dev/shm/nccl-auyogM (size 8257920), error: No space left on device (28)
[rank4]:E0603 16:26:10.118000 78483 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] 
[rank4]:E0603 16:26:10.118000 78483 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] To execute this test, run the following from the base repo dir:
[rank4]:E0603 16:26:10.118000 78483 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     PYTORCH_TEST_WITH_ROCM=1 python test_pipeline_parallel_fwd_bwd.py NcclPipelineParallelForwardBackwardTest.test_learning_pipelining_without_interleaving
[rank4]:E0603 16:26:10.118000 78483 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] 
[rank4]:E0603 16:26:10.118000 78483 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0
[rank4]:E0603 16:26:10.118000 78483 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]  exiting process 4 with exit code: 10
[rank0]:E0603 16:26:10.120000 78479 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Caught exception: 
[rank0]:E0603 16:26:10.120000 78479 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Traceback (most recent call last):
[rank0]:E0603 16:26:10.120000 78479 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/testing/_internal/common_distributed.py", line 734, in run_test
[rank0]:E0603 16:26:10.120000 78479 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     getattr(self, test_name)()
[rank0]:E0603 16:26:10.120000 78479 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/testing/_internal/common_distributed.py", line 607, in wrapper
[rank0]:E0603 16:26:10.120000 78479 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     fn()
[rank0]:E0603 16:26:10.120000 78479 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/testing/_internal/common_utils.py", line 3173, in wrapper
[rank0]:E0603 16:26:10.120000 78479 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     method(*args, **kwargs)
[rank0]:E0603 16:26:10.120000 78479 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py", line 239, in test_learning_pipelining_without_interleaving
[rank0]:E0603 16:26:10.120000 78479 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     self._forward_backward_test_impl(
[rank0]:E0603 16:26:10.120000 78479 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py", line 180, in _forward_backward_test_impl
[rank0]:E0603 16:26:10.120000 78479 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     loss = fwd_bwd_func(
[rank0]:E0603 16:26:10.120000 78479 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_without_interleaving.py", line 349, in forward_backward_pipelining_without_interleaving
[rank0]:E0603 16:26:10.120000 78479 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     send_forward(
[rank0]:E0603 16:26:10.120000 78479 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_without_interleaving.py", line 145, in send_forward
[rank0]:E0603 16:26:10.120000 78479 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     p2p_communication.send_forward(
[rank0]:E0603 16:26:10.120000 78479 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 401, in send_forward
[rank0]:E0603 16:26:10.120000 78479 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     _communicate(
[rank0]:E0603 16:26:10.120000 78479 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 259, in _communicate
[rank0]:E0603 16:26:10.120000 78479 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     tensor_send_prev_req, tensor_recv_prev_req, tensor_send_next_req, tensor_recv_next_req = _run_p2pops(tensor_send_prev, tensor_send_next, tensor_recv_prev, tensor_recv_next, async_comm=async_comm)
[rank0]:E0603 16:26:10.120000 78479 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 97, in _run_p2pops
[rank0]:E0603 16:26:10.120000 78479 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     reqs = torch.distributed.batch_isend_irecv(ops)
[rank0]:E0603 16:26:10.120000 78479 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/distributed/distributed_c10d.py", line 2717, in batch_isend_irecv
[rank0]:E0603 16:26:10.120000 78479 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     p2p_op.op(
[rank0]:E0603 16:26:10.120000 78479 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]   File "/skishore/github/pytorch/torch/distributed/distributed_c10d.py", line 2366, in isend
[rank0]:E0603 16:26:10.120000 78479 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     return group.send([tensor], group_dst, tag)
[rank0]:E0603 16:26:10.120000 78479 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] torch.distributed.DistBackendError: NCCL error in: /skishore/github/pytorch/torch/csrc/distributed/c10d/NCCLUtils.cpp:77, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.25.1
[rank0]:E0603 16:26:10.120000 78479 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
[rank0]:E0603 16:26:10.120000 78479 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Last error:
[rank0]:E0603 16:26:10.120000 78479 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] Error while creating shared memory segment /dev/shm/nccl-bYNB7m (size 8257920), error: No space left on device (28)
[rank0]:E0603 16:26:10.120000 78479 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] 
[rank0]:E0603 16:26:10.120000 78479 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] To execute this test, run the following from the base repo dir:
[rank0]:E0603 16:26:10.120000 78479 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]     PYTORCH_TEST_WITH_ROCM=1 python test_pipeline_parallel_fwd_bwd.py NcclPipelineParallelForwardBackwardTest.test_learning_pipelining_without_interleaving
[rank0]:E0603 16:26:10.120000 78479 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] 
[rank0]:E0603 16:26:10.120000 78479 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741] This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0
[rank0]:E0603 16:26:10.120000 78479 /skishore/github/pytorch/torch/testing/_internal/common_distributed.py:741]  exiting process 0 with exit code: 10
[rank5]:[W603 16:26:10.298413670 ProcessGroupNCCL.cpp:1553] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank7]:[W603 16:26:10.307117426 ProcessGroupNCCL.cpp:1553] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank3]:[W603 16:26:10.349196783 ProcessGroupNCCL.cpp:1553] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank1]:[W603 16:26:10.354651742 ProcessGroupNCCL.cpp:1553] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank6]:[W603 16:26:10.420917412 ProcessGroupNCCL.cpp:1553] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank2]:[W603 16:26:10.432103319 ProcessGroupNCCL.cpp:1553] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank4]:[W603 16:26:10.444288648 ProcessGroupNCCL.cpp:1553] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank0]:[W603 16:26:10.468277920 ProcessGroupNCCL.cpp:1553] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
ERROR
test_learning_pipelining_without_interleaving_ucc_for_p2p (test_pipeline_parallel_fwd_bwd.NcclPipelineParallelForwardBackwardTest) ... Process 1 terminated with exit code 10, terminating remaining processes.
[rank3]:[W603 16:26:32.370430829 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank1]:[W603 16:26:32.374126929 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank0]:[W603 16:26:32.374865154 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank7]:[W603 16:26:33.111700547 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 7]  using GPU 7 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank4]:[W603 16:26:33.222152363 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 4]  using GPU 4 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank2]:[W603 16:26:33.230871453 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank5]:[W603 16:26:33.234719208 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 5]  using GPU 5 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank6]:[W603 16:26:33.237089914 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 6]  using GPU 6 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[dist init] rank = 7, world_size = 8
[dist init] rank = 5, world_size = 8
[dist init] rank = 1, world_size = 8
[dist init] rank = 2, world_size = 8
[dist init] rank = 6, world_size = 8
[dist init] rank = 3, world_size = 8
[dist init] rank = 4, world_size = 8
[dist init] rank = 0, world_size = 8
[rank0]:[W603 16:26:45.905335399 ProcessGroupNCCL.cpp:1553] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
skipped 'Test skipped at subprocess level, look at subprocess log for skip reason'
test_pipelining_without_interleaving_encoder_and_decoder (test_pipeline_parallel_fwd_bwd.NcclPipelineParallelWithToyParallelMLP) ... [rank1]:[W603 16:26:59.114654676 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank0]:[W603 16:26:59.138041697 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank2]:[W603 16:26:59.151866578 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank5]:[W603 16:26:59.153898992 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 5]  using GPU 5 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank6]:[W603 16:26:59.155885838 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 6]  using GPU 6 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank7]:[W603 16:26:59.156481882 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 7]  using GPU 7 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank4]:[W603 16:26:59.159510874 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 4]  using GPU 4 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank3]:[W603 16:26:59.164376950 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/utils.py:81: UserWarning: This function is only for unittest
  warnings.warn("This function is only for unittest")
[dist init] rank = 5, world_size = 8
 > number of parameters on (tensor, pipeline) model parallel rank (1, 2): 16576
[dist init] rank = 3, world_size = 8
 > number of parameters on (tensor, pipeline) model parallel rank (1, 1): 16576
[dist init] rank = 7, world_size = 8
 > number of parameters on (tensor, pipeline) model parallel rank (1, 3): 16576
[dist init] rank = 4, world_size = 8
 > number of parameters on (tensor, pipeline) model parallel rank (0, 2): 16576
[dist init] rank = 6, world_size = 8
 > number of parameters on (tensor, pipeline) model parallel rank (0, 3): 16576
[dist init] rank = 0, world_size = 8
 > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 16576
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/common.py:295: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(
[dist init] rank = 2, world_size = 8
 > number of parameters on (tensor, pipeline) model parallel rank (0, 1): 16576
[dist init] rank = 1, world_size = 8
 > number of parameters on (tensor, pipeline) model parallel rank (1, 0): 16576
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/common.py:295: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/common.py:295: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/common.py:295: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/common.py:295: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/common.py:295: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/common.py:295: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/common.py:295: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(
ok
test_pipelining_without_interleaving_encoder_or_decoder (test_pipeline_parallel_fwd_bwd.NcclPipelineParallelWithToyParallelMLP) ... [rank6]:[W603 16:28:16.813641713 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 6]  using GPU 6 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank7]:[W603 16:28:16.822877040 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 7]  using GPU 7 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank3]:[W603 16:28:16.822896689 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank0]:[W603 16:28:16.826799597 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank2]:[W603 16:28:16.830679000 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank1]:[W603 16:28:16.831445467 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank5]:[W603 16:28:16.831964727 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 5]  using GPU 5 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank4]:[W603 16:28:16.832049082 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 4]  using GPU 4 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/utils.py:81: UserWarning: This function is only for unittest
  warnings.warn("This function is only for unittest")
[dist init] rank = 7, world_size = 8
 > number of parameters on (tensor, pipeline) model parallel rank (1, 3): 16576
[dist init] rank = 3, world_size = 8
 > number of parameters on (tensor, pipeline) model parallel rank (1, 1): 16576
[dist init] rank = 0, world_size = 8
 > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 16576
[dist init] rank = 4, world_size = 8
 > number of parameters on (tensor, pipeline) model parallel rank (0, 2): 16576
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/common.py:295: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(
[dist init] rank = 1, world_size = 8
 > number of parameters on (tensor, pipeline) model parallel rank (1, 0): 16576
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/common.py:295: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(
[dist init] rank = 6, world_size = 8
 > number of parameters on (tensor, pipeline) model parallel rank (0, 3): 16576
[dist init] rank = 2, world_size = 8
 > number of parameters on (tensor, pipeline) model parallel rank (0, 1): 16576
[dist init] rank = 5, world_size = 8
 > number of parameters on (tensor, pipeline) model parallel rank (1, 2): 16576
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/common.py:295: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/common.py:295: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/common.py:295: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/common.py:295: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/common.py:295: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/common.py:295: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(
ok
test_pipelining_without_interleaving_inferenc_encoder_and_decoder (test_pipeline_parallel_fwd_bwd.NcclPipelineParallelWithToyParallelMLP) ... [rank5]:[W603 16:29:58.144385859 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 5]  using GPU 5 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank0]:[W603 16:29:58.153746813 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank4]:[W603 16:29:58.154014020 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 4]  using GPU 4 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank1]:[W603 16:29:58.166112505 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank7]:[W603 16:29:58.167303591 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 7]  using GPU 7 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank3]:[W603 16:29:58.244365810 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank2]:[W603 16:29:58.249077668 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank6]:[W603 16:29:58.250782958 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 6]  using GPU 6 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/utils.py:81: UserWarning: This function is only for unittest
  warnings.warn("This function is only for unittest")
[dist init] rank = 6, world_size = 8
 > number of parameters on (tensor, pipeline) model parallel rank (0, 3): 16576
[dist init] rank = 7, world_size = 8
 > number of parameters on (tensor, pipeline) model parallel rank (1, 3): 16576
[dist init] rank = 5, world_size = 8
 > number of parameters on (tensor, pipeline) model parallel rank (1, 2): 16576
[dist init] rank = 1, world_size = 8
 > number of parameters on (tensor, pipeline) model parallel rank (1, 0): 16576
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/common.py:295: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(
[dist init] rank = 2, world_size = 8
 > number of parameters on (tensor, pipeline) model parallel rank (0, 1): 16576
[dist init] rank = 3, world_size = 8
 > number of parameters on (tensor, pipeline) model parallel rank (1, 1): 16576
[dist init] rank = 0, world_size = 8
 > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 16576
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/common.py:295: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(
[dist init] rank = 4, world_size = 8
 > number of parameters on (tensor, pipeline) model parallel rank (0, 2): 16576
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/common.py:295: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/common.py:295: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/common.py:295: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/common.py:295: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/common.py:295: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/common.py:295: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(
ok
test_pipelining_without_interleaving_inference_sequence_paralle_encoder_and_decoder (test_pipeline_parallel_fwd_bwd.NcclPipelineParallelWithToyParallelMLP) ... [rank5]:[W603 16:30:36.818185801 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 5]  using GPU 5 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank2]:[W603 16:30:36.820888440 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank4]:[W603 16:30:36.822907925 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 4]  using GPU 4 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank1]:[W603 16:30:36.834983897 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank6]:[W603 16:30:36.911624183 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 6]  using GPU 6 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank7]:[W603 16:30:36.919864365 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 7]  using GPU 7 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank3]:[W603 16:30:37.098846840 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank0]:[W603 16:30:37.101206801 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/utils.py:81: UserWarning: This function is only for unittest
  warnings.warn("This function is only for unittest")
[dist init] rank = 5, world_size = 8
 > number of parameters on (tensor, pipeline) model parallel rank (1, 2): 16576
[dist init] rank = 6, world_size = 8
 > number of parameters on (tensor, pipeline) model parallel rank (0, 3): 16576
[dist init] rank = 4, world_size = 8
 > number of parameters on (tensor, pipeline) model parallel rank (0, 2): 16576
[dist init] rank = 0, world_size = 8
 > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 16576
[dist init] rank = 7, world_size = 8
 > number of parameters on (tensor, pipeline) model parallel rank (1, 3): 16576
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/common.py:295: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(
[dist init] rank = 3, world_size = 8
 > number of parameters on (tensor, pipeline) model parallel rank (1, 1): 16576
[dist init] rank = 1, world_size = 8
 > number of parameters on (tensor, pipeline) model parallel rank (1, 0): 16576
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/common.py:295: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(
[dist init] rank = 2, world_size = 8
 > number of parameters on (tensor, pipeline) model parallel rank (0, 1): 16576
/skishore/github/pytorch/torch/distributed/c10d_logger.py:81: FutureWarning: `torch.distributed._all_gather_base` is a private function and will be deprecated. Please use `torch.distributed.all_gather_into_tensor` instead.
  return func(*args, **kwargs)
/skishore/github/pytorch/torch/distributed/c10d_logger.py:81: FutureWarning: `torch.distributed._all_gather_base` is a private function and will be deprecated. Please use `torch.distributed.all_gather_into_tensor` instead.
  return func(*args, **kwargs)
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/tensor_parallel/mappings.py:125: FutureWarning: `torch.distributed._reduce_scatter_base` is a private function and will be deprecated. Please use `torch.distributed.reduce_scatter_tensor` instead.
  torch.distributed._reduce_scatter_base(
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/tensor_parallel/mappings.py:125: FutureWarning: `torch.distributed._reduce_scatter_base` is a private function and will be deprecated. Please use `torch.distributed.reduce_scatter_tensor` instead.
  torch.distributed._reduce_scatter_base(
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/common.py:295: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(
/skishore/github/pytorch/torch/distributed/c10d_logger.py:81: FutureWarning: `torch.distributed._all_gather_base` is a private function and will be deprecated. Please use `torch.distributed.all_gather_into_tensor` instead.
  return func(*args, **kwargs)
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/common.py:295: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(
/skishore/github/pytorch/torch/distributed/c10d_logger.py:81: FutureWarning: `torch.distributed._all_gather_base` is a private function and will be deprecated. Please use `torch.distributed.all_gather_into_tensor` instead.
  return func(*args, **kwargs)
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/tensor_parallel/mappings.py:125: FutureWarning: `torch.distributed._reduce_scatter_base` is a private function and will be deprecated. Please use `torch.distributed.reduce_scatter_tensor` instead.
  torch.distributed._reduce_scatter_base(
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/tensor_parallel/mappings.py:125: FutureWarning: `torch.distributed._reduce_scatter_base` is a private function and will be deprecated. Please use `torch.distributed.reduce_scatter_tensor` instead.
  torch.distributed._reduce_scatter_base(
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/common.py:295: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(
/skishore/github/pytorch/torch/distributed/c10d_logger.py:81: FutureWarning: `torch.distributed._all_gather_base` is a private function and will be deprecated. Please use `torch.distributed.all_gather_into_tensor` instead.
  return func(*args, **kwargs)
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/common.py:295: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(
/skishore/github/pytorch/torch/distributed/c10d_logger.py:81: FutureWarning: `torch.distributed._all_gather_base` is a private function and will be deprecated. Please use `torch.distributed.all_gather_into_tensor` instead.
  return func(*args, **kwargs)
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/tensor_parallel/mappings.py:125: FutureWarning: `torch.distributed._reduce_scatter_base` is a private function and will be deprecated. Please use `torch.distributed.reduce_scatter_tensor` instead.
  torch.distributed._reduce_scatter_base(
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/tensor_parallel/mappings.py:125: FutureWarning: `torch.distributed._reduce_scatter_base` is a private function and will be deprecated. Please use `torch.distributed.reduce_scatter_tensor` instead.
  torch.distributed._reduce_scatter_base(
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/common.py:295: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(
/skishore/github/pytorch/torch/distributed/c10d_logger.py:81: FutureWarning: `torch.distributed._all_gather_base` is a private function and will be deprecated. Please use `torch.distributed.all_gather_into_tensor` instead.
  return func(*args, **kwargs)
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/common.py:295: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(
/skishore/github/pytorch/torch/distributed/c10d_logger.py:81: FutureWarning: `torch.distributed._all_gather_base` is a private function and will be deprecated. Please use `torch.distributed.all_gather_into_tensor` instead.
  return func(*args, **kwargs)
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/tensor_parallel/mappings.py:125: FutureWarning: `torch.distributed._reduce_scatter_base` is a private function and will be deprecated. Please use `torch.distributed.reduce_scatter_tensor` instead.
  torch.distributed._reduce_scatter_base(
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/tensor_parallel/mappings.py:125: FutureWarning: `torch.distributed._reduce_scatter_base` is a private function and will be deprecated. Please use `torch.distributed.reduce_scatter_tensor` instead.
  torch.distributed._reduce_scatter_base(
ok
test_pipelining_without_interleaving_sequence_paralle_encoder_and_decoder (test_pipeline_parallel_fwd_bwd.NcclPipelineParallelWithToyParallelMLP) ... [rank7]:[W603 16:31:14.282970134 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 7]  using GPU 7 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank2]:[W603 16:31:14.397246689 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank4]:[W603 16:31:14.401349996 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 4]  using GPU 4 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank1]:[W603 16:31:14.402028883 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank5]:[W603 16:31:14.403024530 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 5]  using GPU 5 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank3]:[W603 16:31:14.405795309 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank0]:[W603 16:31:14.472123643 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank6]:[W603 16:31:14.485535778 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 6]  using GPU 6 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/utils.py:81: UserWarning: This function is only for unittest
  warnings.warn("This function is only for unittest")
[dist init] rank = 6, world_size = 8
 > number of parameters on (tensor, pipeline) model parallel rank (0, 3): 16576
[dist init] rank = 4, world_size = 8
 > number of parameters on (tensor, pipeline) model parallel rank (0, 2): 16576
[dist init] rank = 7, world_size = 8
 > number of parameters on (tensor, pipeline) model parallel rank (1, 3): 16576
[dist init] rank = 0, world_size = 8
 > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 16576
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/common.py:295: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(
[dist init] rank = 2, world_size = 8
 > number of parameters on (tensor, pipeline) model parallel rank (0, 1): 16576
[dist init] rank = 1, world_size = 8
 > number of parameters on (tensor, pipeline) model parallel rank (1, 0): 16576
[dist init] rank = 3, world_size = 8
 > number of parameters on (tensor, pipeline) model parallel rank (1, 1): 16576
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/common.py:295: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(
[dist init] rank = 5, world_size = 8
 > number of parameters on (tensor, pipeline) model parallel rank (1, 2): 16576
/skishore/github/pytorch/torch/distributed/c10d_logger.py:81: FutureWarning: `torch.distributed._all_gather_base` is a private function and will be deprecated. Please use `torch.distributed.all_gather_into_tensor` instead.
  return func(*args, **kwargs)
/skishore/github/pytorch/torch/distributed/c10d_logger.py:81: FutureWarning: `torch.distributed._all_gather_base` is a private function and will be deprecated. Please use `torch.distributed.all_gather_into_tensor` instead.
  return func(*args, **kwargs)
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/tensor_parallel/mappings.py:125: FutureWarning: `torch.distributed._reduce_scatter_base` is a private function and will be deprecated. Please use `torch.distributed.reduce_scatter_tensor` instead.
  torch.distributed._reduce_scatter_base(
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/tensor_parallel/mappings.py:125: FutureWarning: `torch.distributed._reduce_scatter_base` is a private function and will be deprecated. Please use `torch.distributed.reduce_scatter_tensor` instead.
  torch.distributed._reduce_scatter_base(
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/common.py:295: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(
/skishore/github/pytorch/torch/distributed/c10d_logger.py:81: FutureWarning: `torch.distributed._all_gather_base` is a private function and will be deprecated. Please use `torch.distributed.all_gather_into_tensor` instead.
  return func(*args, **kwargs)
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/common.py:295: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(
/skishore/github/pytorch/torch/distributed/c10d_logger.py:81: FutureWarning: `torch.distributed._all_gather_base` is a private function and will be deprecated. Please use `torch.distributed.all_gather_into_tensor` instead.
  return func(*args, **kwargs)
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/tensor_parallel/mappings.py:125: FutureWarning: `torch.distributed._reduce_scatter_base` is a private function and will be deprecated. Please use `torch.distributed.reduce_scatter_tensor` instead.
  torch.distributed._reduce_scatter_base(
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/tensor_parallel/mappings.py:125: FutureWarning: `torch.distributed._reduce_scatter_base` is a private function and will be deprecated. Please use `torch.distributed.reduce_scatter_tensor` instead.
  torch.distributed._reduce_scatter_base(
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/common.py:295: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(
/skishore/github/pytorch/torch/distributed/c10d_logger.py:81: FutureWarning: `torch.distributed._all_gather_base` is a private function and will be deprecated. Please use `torch.distributed.all_gather_into_tensor` instead.
  return func(*args, **kwargs)
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/common.py:295: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(
/skishore/github/pytorch/torch/distributed/c10d_logger.py:81: FutureWarning: `torch.distributed._all_gather_base` is a private function and will be deprecated. Please use `torch.distributed.all_gather_into_tensor` instead.
  return func(*args, **kwargs)
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/tensor_parallel/mappings.py:125: FutureWarning: `torch.distributed._reduce_scatter_base` is a private function and will be deprecated. Please use `torch.distributed.reduce_scatter_tensor` instead.
  torch.distributed._reduce_scatter_base(
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/tensor_parallel/mappings.py:125: FutureWarning: `torch.distributed._reduce_scatter_base` is a private function and will be deprecated. Please use `torch.distributed.reduce_scatter_tensor` instead.
  torch.distributed._reduce_scatter_base(
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/common.py:295: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(
/skishore/github/pytorch/torch/distributed/c10d_logger.py:81: FutureWarning: `torch.distributed._all_gather_base` is a private function and will be deprecated. Please use `torch.distributed.all_gather_into_tensor` instead.
  return func(*args, **kwargs)
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/common.py:295: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(
/skishore/github/pytorch/torch/distributed/c10d_logger.py:81: FutureWarning: `torch.distributed._all_gather_base` is a private function and will be deprecated. Please use `torch.distributed.all_gather_into_tensor` instead.
  return func(*args, **kwargs)
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/tensor_parallel/mappings.py:125: FutureWarning: `torch.distributed._reduce_scatter_base` is a private function and will be deprecated. Please use `torch.distributed.reduce_scatter_tensor` instead.
  torch.distributed._reduce_scatter_base(
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/tensor_parallel/mappings.py:125: FutureWarning: `torch.distributed._reduce_scatter_base` is a private function and will be deprecated. Please use `torch.distributed.reduce_scatter_tensor` instead.
  torch.distributed._reduce_scatter_base(
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/tensor_parallel/layers.py:358: FutureWarning: `torch.distributed._reduce_scatter_base` is a private function and will be deprecated. Please use `torch.distributed.reduce_scatter_tensor` instead.
  handle = torch.distributed._reduce_scatter_base(
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/tensor_parallel/layers.py:358: FutureWarning: `torch.distributed._reduce_scatter_base` is a private function and will be deprecated. Please use `torch.distributed.reduce_scatter_tensor` instead.
  handle = torch.distributed._reduce_scatter_base(
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/tensor_parallel/layers.py:358: FutureWarning: `torch.distributed._reduce_scatter_base` is a private function and will be deprecated. Please use `torch.distributed.reduce_scatter_tensor` instead.
  handle = torch.distributed._reduce_scatter_base(
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/tensor_parallel/layers.py:358: FutureWarning: `torch.distributed._reduce_scatter_base` is a private function and will be deprecated. Please use `torch.distributed.reduce_scatter_tensor` instead.
  handle = torch.distributed._reduce_scatter_base(
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/tensor_parallel/layers.py:358: FutureWarning: `torch.distributed._reduce_scatter_base` is a private function and will be deprecated. Please use `torch.distributed.reduce_scatter_tensor` instead.
  handle = torch.distributed._reduce_scatter_base(
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/tensor_parallel/layers.py:358: FutureWarning: `torch.distributed._reduce_scatter_base` is a private function and will be deprecated. Please use `torch.distributed.reduce_scatter_tensor` instead.
  handle = torch.distributed._reduce_scatter_base(
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/tensor_parallel/layers.py:358: FutureWarning: `torch.distributed._reduce_scatter_base` is a private function and will be deprecated. Please use `torch.distributed.reduce_scatter_tensor` instead.
  handle = torch.distributed._reduce_scatter_base(
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/tensor_parallel/layers.py:358: FutureWarning: `torch.distributed._reduce_scatter_base` is a private function and will be deprecated. Please use `torch.distributed.reduce_scatter_tensor` instead.
  handle = torch.distributed._reduce_scatter_base(
ok
test_pipelining_without_interleaving_sequence_parallel_encoder_or_decoder (test_pipeline_parallel_fwd_bwd.NcclPipelineParallelWithToyParallelMLP) ... [rank0]:[W603 16:32:27.723335566 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank3]:[W603 16:32:27.726510125 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank1]:[W603 16:32:27.735554250 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank2]:[W603 16:32:27.735853684 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank4]:[W603 16:32:27.739658569 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 4]  using GPU 4 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank5]:[W603 16:32:27.747217020 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 5]  using GPU 5 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank7]:[W603 16:32:27.755008258 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 7]  using GPU 7 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank6]:[W603 16:32:27.755425907 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 6]  using GPU 6 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/utils.py:81: UserWarning: This function is only for unittest
  warnings.warn("This function is only for unittest")
[dist init] rank = 1, world_size = 8
 > number of parameters on (tensor, pipeline) model parallel rank (1, 0): 16576
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/common.py:295: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(
[dist init] rank = 7, world_size = 8
 > number of parameters on (tensor, pipeline) model parallel rank (1, 3): 16576
[dist init] rank = 5, world_size = 8
 > number of parameters on (tensor, pipeline) model parallel rank (1, 2): 16576
[dist init] rank = 2, world_size = 8
 > number of parameters on (tensor, pipeline) model parallel rank (0, 1): 16576
[dist init] rank = 4, world_size = 8
 > number of parameters on (tensor, pipeline) model parallel rank (0, 2): 16576
[dist init] rank = 6, world_size = 8
 > number of parameters on (tensor, pipeline) model parallel rank (0, 3): 16576
[dist init] rank = 0, world_size = 8
 > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 16576
[dist init] rank = 3, world_size = 8
 > number of parameters on (tensor, pipeline) model parallel rank (1, 1): 16576
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/common.py:295: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(
/skishore/github/pytorch/torch/distributed/c10d_logger.py:81: FutureWarning: `torch.distributed._all_gather_base` is a private function and will be deprecated. Please use `torch.distributed.all_gather_into_tensor` instead.
  return func(*args, **kwargs)
/skishore/github/pytorch/torch/distributed/c10d_logger.py:81: FutureWarning: `torch.distributed._all_gather_base` is a private function and will be deprecated. Please use `torch.distributed.all_gather_into_tensor` instead.
  return func(*args, **kwargs)
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/tensor_parallel/mappings.py:125: FutureWarning: `torch.distributed._reduce_scatter_base` is a private function and will be deprecated. Please use `torch.distributed.reduce_scatter_tensor` instead.
  torch.distributed._reduce_scatter_base(
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/tensor_parallel/mappings.py:125: FutureWarning: `torch.distributed._reduce_scatter_base` is a private function and will be deprecated. Please use `torch.distributed.reduce_scatter_tensor` instead.
  torch.distributed._reduce_scatter_base(
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/common.py:295: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(
/skishore/github/pytorch/torch/distributed/c10d_logger.py:81: FutureWarning: `torch.distributed._all_gather_base` is a private function and will be deprecated. Please use `torch.distributed.all_gather_into_tensor` instead.
  return func(*args, **kwargs)
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/common.py:295: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(
/skishore/github/pytorch/torch/distributed/c10d_logger.py:81: FutureWarning: `torch.distributed._all_gather_base` is a private function and will be deprecated. Please use `torch.distributed.all_gather_into_tensor` instead.
  return func(*args, **kwargs)
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/tensor_parallel/mappings.py:125: FutureWarning: `torch.distributed._reduce_scatter_base` is a private function and will be deprecated. Please use `torch.distributed.reduce_scatter_tensor` instead.
  torch.distributed._reduce_scatter_base(
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/tensor_parallel/mappings.py:125: FutureWarning: `torch.distributed._reduce_scatter_base` is a private function and will be deprecated. Please use `torch.distributed.reduce_scatter_tensor` instead.
  torch.distributed._reduce_scatter_base(
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/common.py:295: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(
/skishore/github/pytorch/torch/distributed/c10d_logger.py:81: FutureWarning: `torch.distributed._all_gather_base` is a private function and will be deprecated. Please use `torch.distributed.all_gather_into_tensor` instead.
  return func(*args, **kwargs)
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/common.py:295: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(
/skishore/github/pytorch/torch/distributed/c10d_logger.py:81: FutureWarning: `torch.distributed._all_gather_base` is a private function and will be deprecated. Please use `torch.distributed.all_gather_into_tensor` instead.
  return func(*args, **kwargs)
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/tensor_parallel/mappings.py:125: FutureWarning: `torch.distributed._reduce_scatter_base` is a private function and will be deprecated. Please use `torch.distributed.reduce_scatter_tensor` instead.
  torch.distributed._reduce_scatter_base(
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/tensor_parallel/mappings.py:125: FutureWarning: `torch.distributed._reduce_scatter_base` is a private function and will be deprecated. Please use `torch.distributed.reduce_scatter_tensor` instead.
  torch.distributed._reduce_scatter_base(
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/common.py:295: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(
/skishore/github/pytorch/torch/distributed/c10d_logger.py:81: FutureWarning: `torch.distributed._all_gather_base` is a private function and will be deprecated. Please use `torch.distributed.all_gather_into_tensor` instead.
  return func(*args, **kwargs)
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/common.py:295: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(
/skishore/github/pytorch/torch/distributed/c10d_logger.py:81: FutureWarning: `torch.distributed._all_gather_base` is a private function and will be deprecated. Please use `torch.distributed.all_gather_into_tensor` instead.
  return func(*args, **kwargs)
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/tensor_parallel/mappings.py:125: FutureWarning: `torch.distributed._reduce_scatter_base` is a private function and will be deprecated. Please use `torch.distributed.reduce_scatter_tensor` instead.
  torch.distributed._reduce_scatter_base(
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/tensor_parallel/mappings.py:125: FutureWarning: `torch.distributed._reduce_scatter_base` is a private function and will be deprecated. Please use `torch.distributed.reduce_scatter_tensor` instead.
  torch.distributed._reduce_scatter_base(
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/tensor_parallel/layers.py:358: FutureWarning: `torch.distributed._reduce_scatter_base` is a private function and will be deprecated. Please use `torch.distributed.reduce_scatter_tensor` instead.
  handle = torch.distributed._reduce_scatter_base(
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/tensor_parallel/layers.py:358: FutureWarning: `torch.distributed._reduce_scatter_base` is a private function and will be deprecated. Please use `torch.distributed.reduce_scatter_tensor` instead.
  handle = torch.distributed._reduce_scatter_base(
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/tensor_parallel/layers.py:358: FutureWarning: `torch.distributed._reduce_scatter_base` is a private function and will be deprecated. Please use `torch.distributed.reduce_scatter_tensor` instead.
  handle = torch.distributed._reduce_scatter_base(
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/tensor_parallel/layers.py:358: FutureWarning: `torch.distributed._reduce_scatter_base` is a private function and will be deprecated. Please use `torch.distributed.reduce_scatter_tensor` instead.
  handle = torch.distributed._reduce_scatter_base(
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/tensor_parallel/layers.py:358: FutureWarning: `torch.distributed._reduce_scatter_base` is a private function and will be deprecated. Please use `torch.distributed.reduce_scatter_tensor` instead.
  handle = torch.distributed._reduce_scatter_base(
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/tensor_parallel/layers.py:358: FutureWarning: `torch.distributed._reduce_scatter_base` is a private function and will be deprecated. Please use `torch.distributed.reduce_scatter_tensor` instead.
  handle = torch.distributed._reduce_scatter_base(
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/tensor_parallel/layers.py:358: FutureWarning: `torch.distributed._reduce_scatter_base` is a private function and will be deprecated. Please use `torch.distributed.reduce_scatter_tensor` instead.
  handle = torch.distributed._reduce_scatter_base(
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/tensor_parallel/layers.py:358: FutureWarning: `torch.distributed._reduce_scatter_base` is a private function and will be deprecated. Please use `torch.distributed.reduce_scatter_tensor` instead.
  handle = torch.distributed._reduce_scatter_base(
ok
test_pipelining_without_interleaving_sequence_parallel_encoder_or_decoder_half (test_pipeline_parallel_fwd_bwd.NcclPipelineParallelWithToyParallelMLP) ... [rank2]:[W603 16:33:41.746748332 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank4]:[W603 16:33:41.767579754 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 4]  using GPU 4 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank0]:[W603 16:33:41.769077596 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank6]:[W603 16:33:41.770638701 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 6]  using GPU 6 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank1]:[W603 16:33:42.345720967 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank7]:[W603 16:33:42.362124551 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 7]  using GPU 7 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank5]:[W603 16:33:42.378914949 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 5]  using GPU 5 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank3]:[W603 16:33:42.380083462 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/utils.py:81: UserWarning: This function is only for unittest
  warnings.warn("This function is only for unittest")
[dist init] rank = 6, world_size = 8
 > number of parameters on (tensor, pipeline) model parallel rank (0, 3): 16576
[dist init] rank = 4, world_size = 8
 > number of parameters on (tensor, pipeline) model parallel rank (0, 2): 16576
[dist init] rank = 0, world_size = 8
 > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 16576
[dist init] rank = 7, world_size = 8
 > number of parameters on (tensor, pipeline) model parallel rank (1, 3): 16576
[dist init] rank = 2, world_size = 8
 > number of parameters on (tensor, pipeline) model parallel rank (0, 1): 16576
[dist init] rank = 3, world_size = 8
 > number of parameters on (tensor, pipeline) model parallel rank (1, 1): 16576
[dist init] rank = 5, world_size = 8
 > number of parameters on (tensor, pipeline) model parallel rank (1, 2): 16576
[dist init] rank = 1, world_size = 8
 > number of parameters on (tensor, pipeline) model parallel rank (1, 0): 16576
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/common.py:295: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/_autocast_utils.py:26: FutureWarning: `torch.cuda.amp.autocast_mode._cast(value, dtype)` is deprecated. Please use `torch.amp.autocast_mode._cast(value, 'cuda', dtype)` instead.
  return torch.cuda.amp.autocast_mode._cast(args, torch.get_autocast_gpu_dtype())
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/common.py:295: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/_autocast_utils.py:26: FutureWarning: `torch.cuda.amp.autocast_mode._cast(value, dtype)` is deprecated. Please use `torch.amp.autocast_mode._cast(value, 'cuda', dtype)` instead.
  return torch.cuda.amp.autocast_mode._cast(args, torch.get_autocast_gpu_dtype())
/skishore/github/pytorch/torch/distributed/c10d_logger.py:81: FutureWarning: `torch.distributed._all_gather_base` is a private function and will be deprecated. Please use `torch.distributed.all_gather_into_tensor` instead.
  return func(*args, **kwargs)
/skishore/github/pytorch/torch/distributed/c10d_logger.py:81: FutureWarning: `torch.distributed._all_gather_base` is a private function and will be deprecated. Please use `torch.distributed.all_gather_into_tensor` instead.
  return func(*args, **kwargs)
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/tensor_parallel/mappings.py:125: FutureWarning: `torch.distributed._reduce_scatter_base` is a private function and will be deprecated. Please use `torch.distributed.reduce_scatter_tensor` instead.
  torch.distributed._reduce_scatter_base(
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/tensor_parallel/mappings.py:125: FutureWarning: `torch.distributed._reduce_scatter_base` is a private function and will be deprecated. Please use `torch.distributed.reduce_scatter_tensor` instead.
  torch.distributed._reduce_scatter_base(
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/common.py:295: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/_autocast_utils.py:26: FutureWarning: `torch.cuda.amp.autocast_mode._cast(value, dtype)` is deprecated. Please use `torch.amp.autocast_mode._cast(value, 'cuda', dtype)` instead.
  return torch.cuda.amp.autocast_mode._cast(args, torch.get_autocast_gpu_dtype())
/skishore/github/pytorch/torch/distributed/c10d_logger.py:81: FutureWarning: `torch.distributed._all_gather_base` is a private function and will be deprecated. Please use `torch.distributed.all_gather_into_tensor` instead.
  return func(*args, **kwargs)
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/common.py:295: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/_autocast_utils.py:26: FutureWarning: `torch.cuda.amp.autocast_mode._cast(value, dtype)` is deprecated. Please use `torch.amp.autocast_mode._cast(value, 'cuda', dtype)` instead.
  return torch.cuda.amp.autocast_mode._cast(args, torch.get_autocast_gpu_dtype())
/skishore/github/pytorch/torch/distributed/c10d_logger.py:81: FutureWarning: `torch.distributed._all_gather_base` is a private function and will be deprecated. Please use `torch.distributed.all_gather_into_tensor` instead.
  return func(*args, **kwargs)
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/tensor_parallel/mappings.py:125: FutureWarning: `torch.distributed._reduce_scatter_base` is a private function and will be deprecated. Please use `torch.distributed.reduce_scatter_tensor` instead.
  torch.distributed._reduce_scatter_base(
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/tensor_parallel/mappings.py:125: FutureWarning: `torch.distributed._reduce_scatter_base` is a private function and will be deprecated. Please use `torch.distributed.reduce_scatter_tensor` instead.
  torch.distributed._reduce_scatter_base(
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/common.py:295: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/_autocast_utils.py:26: FutureWarning: `torch.cuda.amp.autocast_mode._cast(value, dtype)` is deprecated. Please use `torch.amp.autocast_mode._cast(value, 'cuda', dtype)` instead.
  return torch.cuda.amp.autocast_mode._cast(args, torch.get_autocast_gpu_dtype())
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/common.py:295: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/_autocast_utils.py:26: FutureWarning: `torch.cuda.amp.autocast_mode._cast(value, dtype)` is deprecated. Please use `torch.amp.autocast_mode._cast(value, 'cuda', dtype)` instead.
  return torch.cuda.amp.autocast_mode._cast(args, torch.get_autocast_gpu_dtype())
/skishore/github/pytorch/torch/distributed/c10d_logger.py:81: FutureWarning: `torch.distributed._all_gather_base` is a private function and will be deprecated. Please use `torch.distributed.all_gather_into_tensor` instead.
  return func(*args, **kwargs)
/skishore/github/pytorch/torch/distributed/c10d_logger.py:81: FutureWarning: `torch.distributed._all_gather_base` is a private function and will be deprecated. Please use `torch.distributed.all_gather_into_tensor` instead.
  return func(*args, **kwargs)
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/tensor_parallel/mappings.py:125: FutureWarning: `torch.distributed._reduce_scatter_base` is a private function and will be deprecated. Please use `torch.distributed.reduce_scatter_tensor` instead.
  torch.distributed._reduce_scatter_base(
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/tensor_parallel/mappings.py:125: FutureWarning: `torch.distributed._reduce_scatter_base` is a private function and will be deprecated. Please use `torch.distributed.reduce_scatter_tensor` instead.
  torch.distributed._reduce_scatter_base(
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/common.py:295: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/_autocast_utils.py:26: FutureWarning: `torch.cuda.amp.autocast_mode._cast(value, dtype)` is deprecated. Please use `torch.amp.autocast_mode._cast(value, 'cuda', dtype)` instead.
  return torch.cuda.amp.autocast_mode._cast(args, torch.get_autocast_gpu_dtype())
/skishore/github/pytorch/torch/distributed/c10d_logger.py:81: FutureWarning: `torch.distributed._all_gather_base` is a private function and will be deprecated. Please use `torch.distributed.all_gather_into_tensor` instead.
  return func(*args, **kwargs)
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/common.py:295: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/_autocast_utils.py:26: FutureWarning: `torch.cuda.amp.autocast_mode._cast(value, dtype)` is deprecated. Please use `torch.amp.autocast_mode._cast(value, 'cuda', dtype)` instead.
  return torch.cuda.amp.autocast_mode._cast(args, torch.get_autocast_gpu_dtype())
/skishore/github/pytorch/torch/distributed/c10d_logger.py:81: FutureWarning: `torch.distributed._all_gather_base` is a private function and will be deprecated. Please use `torch.distributed.all_gather_into_tensor` instead.
  return func(*args, **kwargs)
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/tensor_parallel/mappings.py:125: FutureWarning: `torch.distributed._reduce_scatter_base` is a private function and will be deprecated. Please use `torch.distributed.reduce_scatter_tensor` instead.
  torch.distributed._reduce_scatter_base(
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/tensor_parallel/mappings.py:125: FutureWarning: `torch.distributed._reduce_scatter_base` is a private function and will be deprecated. Please use `torch.distributed.reduce_scatter_tensor` instead.
  torch.distributed._reduce_scatter_base(
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/tensor_parallel/layers.py:358: FutureWarning: `torch.distributed._reduce_scatter_base` is a private function and will be deprecated. Please use `torch.distributed.reduce_scatter_tensor` instead.
  handle = torch.distributed._reduce_scatter_base(
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/tensor_parallel/layers.py:358: FutureWarning: `torch.distributed._reduce_scatter_base` is a private function and will be deprecated. Please use `torch.distributed.reduce_scatter_tensor` instead.
  handle = torch.distributed._reduce_scatter_base(
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/tensor_parallel/layers.py:358: FutureWarning: `torch.distributed._reduce_scatter_base` is a private function and will be deprecated. Please use `torch.distributed.reduce_scatter_tensor` instead.
  handle = torch.distributed._reduce_scatter_base(
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/tensor_parallel/layers.py:358: FutureWarning: `torch.distributed._reduce_scatter_base` is a private function and will be deprecated. Please use `torch.distributed.reduce_scatter_tensor` instead.
  handle = torch.distributed._reduce_scatter_base(
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/tensor_parallel/layers.py:358: FutureWarning: `torch.distributed._reduce_scatter_base` is a private function and will be deprecated. Please use `torch.distributed.reduce_scatter_tensor` instead.
  handle = torch.distributed._reduce_scatter_base(
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/tensor_parallel/layers.py:358: FutureWarning: `torch.distributed._reduce_scatter_base` is a private function and will be deprecated. Please use `torch.distributed.reduce_scatter_tensor` instead.
  handle = torch.distributed._reduce_scatter_base(
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/tensor_parallel/layers.py:358: FutureWarning: `torch.distributed._reduce_scatter_base` is a private function and will be deprecated. Please use `torch.distributed.reduce_scatter_tensor` instead.
  handle = torch.distributed._reduce_scatter_base(
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/tensor_parallel/layers.py:358: FutureWarning: `torch.distributed._reduce_scatter_base` is a private function and will be deprecated. Please use `torch.distributed.reduce_scatter_tensor` instead.
  handle = torch.distributed._reduce_scatter_base(
ok
test_inference_async_pipelining_with_interleaving (test_pipeline_parallel_fwd_bwd.UccPipelineParallelForwardBackwardTest) ... skipped 'Requires [`torch_ucc`](https://github.com/facebookresearch/torch_ucc)'
test_inference_async_pipelining_without_interleaving (test_pipeline_parallel_fwd_bwd.UccPipelineParallelForwardBackwardTest) ... skipped 'Requires [`torch_ucc`](https://github.com/facebookresearch/torch_ucc)'
test_inference_no_pipelining (test_pipeline_parallel_fwd_bwd.UccPipelineParallelForwardBackwardTest) ... skipped 'Requires [`torch_ucc`](https://github.com/facebookresearch/torch_ucc)'
test_inference_pipelining_with_interleaving (test_pipeline_parallel_fwd_bwd.UccPipelineParallelForwardBackwardTest) ... skipped 'Requires [`torch_ucc`](https://github.com/facebookresearch/torch_ucc)'
test_inference_pipelining_without_interleaving (test_pipeline_parallel_fwd_bwd.UccPipelineParallelForwardBackwardTest) ... skipped 'Requires [`torch_ucc`](https://github.com/facebookresearch/torch_ucc)'
test_learning_async_pipelining_with_interleaving (test_pipeline_parallel_fwd_bwd.UccPipelineParallelForwardBackwardTest) ... skipped 'Requires [`torch_ucc`](https://github.com/facebookresearch/torch_ucc)'
test_learning_async_pipelining_without_interleaving (test_pipeline_parallel_fwd_bwd.UccPipelineParallelForwardBackwardTest) ... skipped 'Requires [`torch_ucc`](https://github.com/facebookresearch/torch_ucc)'
test_learning_no_pipelining (test_pipeline_parallel_fwd_bwd.UccPipelineParallelForwardBackwardTest) ... skipped 'Requires [`torch_ucc`](https://github.com/facebookresearch/torch_ucc)'
test_learning_pipelining_with_interleaving (test_pipeline_parallel_fwd_bwd.UccPipelineParallelForwardBackwardTest) ... skipped 'Requires [`torch_ucc`](https://github.com/facebookresearch/torch_ucc)'
test_learning_pipelining_without_interleaving (test_pipeline_parallel_fwd_bwd.UccPipelineParallelForwardBackwardTest) ... skipped 'Requires [`torch_ucc`](https://github.com/facebookresearch/torch_ucc)'
test_cuda_rng_tracker (test_random.NcclTransformerRandomTest) ... [rank0]:[W603 16:34:39.248877851 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank2]:[W603 16:34:39.259966901 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank3]:[W603 16:34:39.268915315 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank1]:[W603 16:34:39.272702934 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
/skishore/github/apex/tests/L0/run_transformer/test_random.py:73: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /skishore/github/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
  tensor = torch.cuda.FloatTensor(size)
/skishore/github/apex/tests/L0/run_transformer/test_random.py:73: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /skishore/github/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
  tensor = torch.cuda.FloatTensor(size)
/skishore/github/apex/tests/L0/run_transformer/test_random.py:73: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /skishore/github/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
  tensor = torch.cuda.FloatTensor(size)
/skishore/github/apex/tests/L0/run_transformer/test_random.py:73: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /skishore/github/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
  tensor = torch.cuda.FloatTensor(size)
[dist init] rank = 0, world_size = 4
[dist init] rank = 1, world_size = 4
[dist init] rank = 3, world_size = 4
[dist init] rank = 2, world_size = 4
ok
test_set_cuda_rng_state (test_random.NcclTransformerRandomTest) ... [rank1]:[W603 16:34:54.482892089 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank0]:[W603 16:34:54.488903577 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank3]:[W603 16:34:54.490081995 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank2]:[W603 16:34:54.970732545 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
/skishore/github/apex/tests/L0/run_transformer/test_random.py:30: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /skishore/github/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
  tensor = torch.cuda.FloatTensor(size)
/skishore/github/apex/tests/L0/run_transformer/test_random.py:30: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /skishore/github/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
  tensor = torch.cuda.FloatTensor(size)
/skishore/github/apex/tests/L0/run_transformer/test_random.py:30: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /skishore/github/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
  tensor = torch.cuda.FloatTensor(size)
/skishore/github/apex/tests/L0/run_transformer/test_random.py:30: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /skishore/github/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
  tensor = torch.cuda.FloatTensor(size)
[dist init] rank = 0, world_size = 4
[dist init] rank = 3, world_size = 4
[dist init] rank = 1, world_size = 4
[dist init] rank = 2, world_size = 4
ok
test_cuda_rng_tracker (test_random.UccTransformerRandomTest) ... skipped 'Requires [`torch_ucc`](https://github.com/facebookresearch/torch_ucc)'
test_set_cuda_rng_state (test_random.UccTransformerRandomTest) ... skipped 'Requires [`torch_ucc`](https://github.com/facebookresearch/torch_ucc)'
test_transformer (test_transformer_module.TestTransformer) ... 
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/common.py:295: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/common.py:295: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/common.py:295: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/common.py:295: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/common.py:295: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/common.py:295: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/common.py:295: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/common.py:295: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(
/skishore/github/pytorch/torch/autograd/graph.py:824: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /skishore/github/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:62.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/skishore/github/pytorch/torch/autograd/graph.py:824: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /skishore/github/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:62.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/skishore/github/pytorch/torch/autograd/graph.py:824: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /skishore/github/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:62.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/skishore/github/pytorch/torch/autograd/graph.py:824: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /skishore/github/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:62.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank7]:[W603 16:37:20.454816579 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 7]  using GPU 7 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank6]:[W603 16:37:20.459721136 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 6]  using GPU 6 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank4]:[W603 16:37:20.467136577 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 4]  using GPU 4 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank5]:[W603 16:37:20.469727852 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 5]  using GPU 5 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank3]:[W603 16:37:20.475567385 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank2]:[W603 16:37:20.479225452 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank1]:[W603 16:37:20.489246729 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank0]:[W603 16:37:20.490170971 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank0]:[W603 16:37:35.197418010 ProcessGroupNCCL.cpp:1553] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank3]:[W603 16:37:35.202581912 ProcessGroupNCCL.cpp:1553] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank1]:[W603 16:37:35.214856702 ProcessGroupNCCL.cpp:1553] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank2]:[W603 16:37:35.232038583 ProcessGroupNCCL.cpp:1553] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank4]:[W603 16:37:35.235840664 ProcessGroupNCCL.cpp:1553] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank5]:[W603 16:37:35.274117862 ProcessGroupNCCL.cpp:1553] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank7]:[W603 16:37:35.278545080 ProcessGroupNCCL.cpp:1553] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank6]:[W603 16:37:35.302280844 ProcessGroupNCCL.cpp:1553] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/tensor_parallel/data.py:50: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /skishore/github/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
  sizes_cuda = torch.cuda.LongTensor(sizes)
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/tensor_parallel/data.py:50: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /skishore/github/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
  sizes_cuda = torch.cuda.LongTensor(sizes)
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/tensor_parallel/data.py:50: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /skishore/github/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
  sizes_cuda = torch.cuda.LongTensor(sizes)
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/tensor_parallel/data.py:50: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /skishore/github/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
  sizes_cuda = torch.cuda.LongTensor(sizes)
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/tensor_parallel/data.py:50: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /skishore/github/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
  sizes_cuda = torch.cuda.LongTensor(sizes)
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/tensor_parallel/data.py:50: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /skishore/github/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
  sizes_cuda = torch.cuda.LongTensor(sizes)
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/tensor_parallel/data.py:50: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /skishore/github/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
  sizes_cuda = torch.cuda.LongTensor(sizes)
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/tensor_parallel/data.py:50: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /skishore/github/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
  sizes_cuda = torch.cuda.LongTensor(sizes)
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/common.py:295: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/common.py:295: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/common.py:295: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/common.py:295: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/common.py:295: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/common.py:295: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/common.py:295: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/common.py:295: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(
/skishore/github/pytorch/torch/autograd/graph.py:824: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /skishore/github/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:62.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/skishore/github/pytorch/torch/autograd/graph.py:824: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /skishore/github/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:62.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/skishore/github/pytorch/torch/autograd/graph.py:824: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /skishore/github/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:62.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/skishore/github/pytorch/torch/autograd/graph.py:824: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /skishore/github/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:62.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank7]:[W603 16:43:01.663759187 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 7]  using GPU 7 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank6]:[W603 16:43:01.663999103 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 6]  using GPU 6 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank5]:[W603 16:43:01.760702253 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 5]  using GPU 5 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank4]:[W603 16:43:01.760939295 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 4]  using GPU 4 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank3]:[W603 16:43:01.779388598 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank2]:[W603 16:43:01.780375512 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank1]:[W603 16:43:01.798539673 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank0]:[W603 16:43:01.798610107 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank2]:[W603 16:43:09.017878787 ProcessGroupNCCL.cpp:1553] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank6]:[W603 16:43:10.036188324 ProcessGroupNCCL.cpp:1553] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank4]:[W603 16:43:10.052884716 ProcessGroupNCCL.cpp:1553] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank7]:[W603 16:43:10.062708111 ProcessGroupNCCL.cpp:1553] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank1]:[W603 16:43:10.063044120 ProcessGroupNCCL.cpp:1553] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank3]:[W603 16:43:10.064505398 ProcessGroupNCCL.cpp:1553] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank5]:[W603 16:43:10.066178591 ProcessGroupNCCL.cpp:1553] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank0]:[W603 16:43:10.071842795 ProcessGroupNCCL.cpp:1553] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[rank3]:[W603 16:43:35.670623763 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank7]:[W603 16:43:35.670765383 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 7]  using GPU 7 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank1]:[W603 16:43:35.672929394 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank5]:[W603 16:43:35.676482316 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 5]  using GPU 5 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/utils.py:81: UserWarning: This function is only for unittest
  warnings.warn("This function is only for unittest")
[rank0]:[W603 16:43:35.691670551 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank6]:[W603 16:43:35.699256296 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 6]  using GPU 6 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank2]:[W603 16:43:36.063533624 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank4]:[W603 16:43:36.073806186 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 4]  using GPU 4 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/common.py:295: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/common.py:295: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/common.py:295: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/common.py:295: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/common.py:295: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/common.py:295: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/common.py:295: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/common.py:295: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/utils.py:81: UserWarning: This function is only for unittest
  warnings.warn("This function is only for unittest")
[rank0]:[W603 16:46:11.098930997 ProcessGroupNCCL.cpp:1553] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank4]:[W603 16:46:11.115717975 ProcessGroupNCCL.cpp:1553] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank2]:[W603 16:46:11.119934281 ProcessGroupNCCL.cpp:1553] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank6]:[W603 16:46:11.123500132 ProcessGroupNCCL.cpp:1553] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank1]:[W603 16:46:11.125988935 ProcessGroupNCCL.cpp:1553] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank5]:[W603 16:46:11.131786406 ProcessGroupNCCL.cpp:1553] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank3]:[W603 16:46:11.133387652 ProcessGroupNCCL.cpp:1553] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[rank7]:[W603 16:46:11.135553105 ProcessGroupNCCL.cpp:1553] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
ok
test_split_tensor_along_last_dim (test_transformer_utils.TransformerUtilsTest) ... #######################################################
# Python executable path: /opt/conda/envs/py_3.10/bin/python
# 3 tests: ['/skishore/github/apex/tests/L0/run_transformer/run_gpt_minimal_test.py', '/skishore/github/apex/tests/L0/run_transformer/run_bert_minimal_test.py', '/skishore/github/apex/tests/L0/run_transformer/run_dynamic_batchsize_test.py']
#######################################################
### 1 / 3: cmd: /opt/conda/envs/py_3.10/bin/python -m torch.distributed.run --nproc_per_node=8 /skishore/github/apex/tests/L0/run_transformer/run_gpt_minimal_test.py --micro-batch-size 2 --num-layers 16 --hidden-size 256 --num-attention-heads 8 --max-position-embeddings 512 --seq-length 512 --global-batch-size 128 --pipeline-model-parallel-size 4 --tensor-model-parallel-size 2
### 2 / 3: cmd: /opt/conda/envs/py_3.10/bin/python -m torch.distributed.run --nproc_per_node=8 /skishore/github/apex/tests/L0/run_transformer/run_bert_minimal_test.py --micro-batch-size 2 --num-layers 16 --hidden-size 256 --num-attention-heads 8 --max-position-embeddings 512 --seq-length 512 --global-batch-size 128 --pipeline-model-parallel-size 4 --tensor-model-parallel-size 2 --bert-no-binary-head
### 3 / 3: cmd: /opt/conda/envs/py_3.10/bin/python -m torch.distributed.run --nproc_per_node=8 /skishore/github/apex/tests/L0/run_transformer/run_dynamic_batchsize_test.py --micro-batch-size 2 --num-layers 16 --hidden-size 256 --num-attention-heads 8 --max-position-embeddings 512 --seq-length 512 --global-batch-size 128 --use-cpu-initialization
### PASSED
[rank3]:[W603 16:46:31.117388689 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank1]:[W603 16:46:31.130568896 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank0]:[W603 16:46:31.163744050 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank2]:[W603 16:46:31.206279493 ProcessGroupNCCL.cpp:4843] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[dist init] rank = 3, world_size = 4
[dist init] rank = 0, world_size = 4
[dist init] rank = 1, world_size = 4
[dist init] rank = 2, world_size = 4
ok

======================================================================
ERROR: test_inference_async_pipelining_with_interleaving (test_pipeline_parallel_fwd_bwd.NcclPipelineParallelForwardBackwardTest)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/skishore/github/pytorch/torch/testing/_internal/common_distributed.py", line 605, in wrapper
    self._join_processes(fn)
  File "/skishore/github/pytorch/torch/testing/_internal/common_distributed.py", line 845, in _join_processes
    self._check_return_codes(elapsed_time)
  File "/skishore/github/pytorch/torch/testing/_internal/common_distributed.py", line 894, in _check_return_codes
    raise RuntimeError(error)
RuntimeError: Process 3 exited with error code 10 and exception:
Traceback (most recent call last):
  File "/skishore/github/pytorch/torch/testing/_internal/common_distributed.py", line 734, in run_test
    getattr(self, test_name)()
  File "/skishore/github/pytorch/torch/testing/_internal/common_distributed.py", line 607, in wrapper
    fn()
  File "/skishore/github/pytorch/torch/testing/_internal/common_utils.py", line 3173, in wrapper
    method(*args, **kwargs)
  File "/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py", line 278, in test_inference_async_pipelining_with_interleaving
    self._forward_backward_test_impl(
  File "/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py", line 180, in _forward_backward_test_impl
    loss = fwd_bwd_func(
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_with_interleaving.py", line 219, in _forward_backward_pipelining_with_interleaving
    p2p_communication.recv_forward(
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 342, in recv_forward
    input_tensor, _ = _communicate(
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 259, in _communicate
    tensor_send_prev_req, tensor_recv_prev_req, tensor_send_next_req, tensor_recv_next_req = _run_p2pops(tensor_send_prev, tensor_send_next, tensor_recv_prev, tensor_recv_next, async_comm=async_comm)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 97, in _run_p2pops
    reqs = torch.distributed.batch_isend_irecv(ops)
  File "/skishore/github/pytorch/torch/distributed/distributed_c10d.py", line 2717, in batch_isend_irecv
    p2p_op.op(
  File "/skishore/github/pytorch/torch/distributed/distributed_c10d.py", line 2411, in irecv
    return group.recv([tensor], group_src, tag)
torch.distributed.DistBackendError: NCCL error in: /skishore/github/pytorch/torch/csrc/distributed/c10d/NCCLUtils.cpp:77, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.25.1
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:
Error while creating shared memory segment /dev/shm/nccl-BZatXC (size 8257920), error: No space left on device (28)

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_ROCM=1 python test_pipeline_parallel_fwd_bwd.py NcclPipelineParallelForwardBackwardTest.test_inference_async_pipelining_with_interleaving

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0



======================================================================
ERROR: test_inference_async_pipelining_without_interleaving (test_pipeline_parallel_fwd_bwd.NcclPipelineParallelForwardBackwardTest)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/skishore/github/pytorch/torch/testing/_internal/common_distributed.py", line 605, in wrapper
    self._join_processes(fn)
  File "/skishore/github/pytorch/torch/testing/_internal/common_distributed.py", line 845, in _join_processes
    self._check_return_codes(elapsed_time)
  File "/skishore/github/pytorch/torch/testing/_internal/common_distributed.py", line 894, in _check_return_codes
    raise RuntimeError(error)
RuntimeError: Process 1 exited with error code 10 and exception:
Traceback (most recent call last):
  File "/skishore/github/pytorch/torch/testing/_internal/common_distributed.py", line 734, in run_test
    getattr(self, test_name)()
  File "/skishore/github/pytorch/torch/testing/_internal/common_distributed.py", line 607, in wrapper
    fn()
  File "/skishore/github/pytorch/torch/testing/_internal/common_utils.py", line 3173, in wrapper
    method(*args, **kwargs)
  File "/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py", line 254, in test_inference_async_pipelining_without_interleaving
    self._forward_backward_test_impl(
  File "/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py", line 180, in _forward_backward_test_impl
    loss = fwd_bwd_func(
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_without_interleaving.py", line 349, in forward_backward_pipelining_without_interleaving
    send_forward(
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_without_interleaving.py", line 145, in send_forward
    p2p_communication.send_forward(
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 401, in send_forward
    _communicate(
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 259, in _communicate
    tensor_send_prev_req, tensor_recv_prev_req, tensor_send_next_req, tensor_recv_next_req = _run_p2pops(tensor_send_prev, tensor_send_next, tensor_recv_prev, tensor_recv_next, async_comm=async_comm)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 97, in _run_p2pops
    reqs = torch.distributed.batch_isend_irecv(ops)
  File "/skishore/github/pytorch/torch/distributed/distributed_c10d.py", line 2717, in batch_isend_irecv
    p2p_op.op(
  File "/skishore/github/pytorch/torch/distributed/distributed_c10d.py", line 2366, in isend
    return group.send([tensor], group_dst, tag)
torch.distributed.DistBackendError: NCCL error in: /skishore/github/pytorch/torch/csrc/distributed/c10d/NCCLUtils.cpp:77, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.25.1
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:
Error while creating shared memory segment /dev/shm/nccl-G7NK1l (size 8257920), error: No space left on device (28)

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_ROCM=1 python test_pipeline_parallel_fwd_bwd.py NcclPipelineParallelForwardBackwardTest.test_inference_async_pipelining_without_interleaving

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0



======================================================================
ERROR: test_inference_pipelining_with_interleaving (test_pipeline_parallel_fwd_bwd.NcclPipelineParallelForwardBackwardTest)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/skishore/github/pytorch/torch/testing/_internal/common_distributed.py", line 605, in wrapper
    self._join_processes(fn)
  File "/skishore/github/pytorch/torch/testing/_internal/common_distributed.py", line 845, in _join_processes
    self._check_return_codes(elapsed_time)
  File "/skishore/github/pytorch/torch/testing/_internal/common_distributed.py", line 894, in _check_return_codes
    raise RuntimeError(error)
RuntimeError: Process 7 exited with error code 10 and exception:
Traceback (most recent call last):
  File "/skishore/github/pytorch/torch/testing/_internal/common_distributed.py", line 734, in run_test
    getattr(self, test_name)()
  File "/skishore/github/pytorch/torch/testing/_internal/common_distributed.py", line 607, in wrapper
    fn()
  File "/skishore/github/pytorch/torch/testing/_internal/common_utils.py", line 3173, in wrapper
    method(*args, **kwargs)
  File "/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py", line 266, in test_inference_pipelining_with_interleaving
    self._forward_backward_test_impl(
  File "/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py", line 180, in _forward_backward_test_impl
    loss = fwd_bwd_func(
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_with_interleaving.py", line 219, in _forward_backward_pipelining_with_interleaving
    p2p_communication.recv_forward(
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 342, in recv_forward
    input_tensor, _ = _communicate(
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 259, in _communicate
    tensor_send_prev_req, tensor_recv_prev_req, tensor_send_next_req, tensor_recv_next_req = _run_p2pops(tensor_send_prev, tensor_send_next, tensor_recv_prev, tensor_recv_next, async_comm=async_comm)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 97, in _run_p2pops
    reqs = torch.distributed.batch_isend_irecv(ops)
  File "/skishore/github/pytorch/torch/distributed/distributed_c10d.py", line 2717, in batch_isend_irecv
    p2p_op.op(
  File "/skishore/github/pytorch/torch/distributed/distributed_c10d.py", line 2411, in irecv
    return group.recv([tensor], group_src, tag)
torch.distributed.DistBackendError: NCCL error in: /skishore/github/pytorch/torch/csrc/distributed/c10d/NCCLUtils.cpp:77, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.25.1
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:
Error while creating shared memory segment /dev/shm/nccl-lidQW9 (size 8257920), error: No space left on device (28)

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_ROCM=1 python test_pipeline_parallel_fwd_bwd.py NcclPipelineParallelForwardBackwardTest.test_inference_pipelining_with_interleaving

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0



======================================================================
ERROR: test_inference_pipelining_without_interleaving (test_pipeline_parallel_fwd_bwd.NcclPipelineParallelForwardBackwardTest)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/skishore/github/pytorch/torch/testing/_internal/common_distributed.py", line 605, in wrapper
    self._join_processes(fn)
  File "/skishore/github/pytorch/torch/testing/_internal/common_distributed.py", line 845, in _join_processes
    self._check_return_codes(elapsed_time)
  File "/skishore/github/pytorch/torch/testing/_internal/common_distributed.py", line 894, in _check_return_codes
    raise RuntimeError(error)
RuntimeError: Process 1 exited with error code 10 and exception:
Traceback (most recent call last):
  File "/skishore/github/pytorch/torch/testing/_internal/common_distributed.py", line 734, in run_test
    getattr(self, test_name)()
  File "/skishore/github/pytorch/torch/testing/_internal/common_distributed.py", line 607, in wrapper
    fn()
  File "/skishore/github/pytorch/torch/testing/_internal/common_utils.py", line 3173, in wrapper
    method(*args, **kwargs)
  File "/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py", line 244, in test_inference_pipelining_without_interleaving
    self._forward_backward_test_impl(
  File "/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py", line 180, in _forward_backward_test_impl
    loss = fwd_bwd_func(
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_without_interleaving.py", line 349, in forward_backward_pipelining_without_interleaving
    send_forward(
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_without_interleaving.py", line 145, in send_forward
    p2p_communication.send_forward(
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 401, in send_forward
    _communicate(
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 259, in _communicate
    tensor_send_prev_req, tensor_recv_prev_req, tensor_send_next_req, tensor_recv_next_req = _run_p2pops(tensor_send_prev, tensor_send_next, tensor_recv_prev, tensor_recv_next, async_comm=async_comm)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 97, in _run_p2pops
    reqs = torch.distributed.batch_isend_irecv(ops)
  File "/skishore/github/pytorch/torch/distributed/distributed_c10d.py", line 2717, in batch_isend_irecv
    p2p_op.op(
  File "/skishore/github/pytorch/torch/distributed/distributed_c10d.py", line 2366, in isend
    return group.send([tensor], group_dst, tag)
torch.distributed.DistBackendError: NCCL error in: /skishore/github/pytorch/torch/csrc/distributed/c10d/NCCLUtils.cpp:77, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.25.1
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:
Error while creating shared memory segment /dev/shm/nccl-G3jTgB (size 8257920), error: No space left on device (28)

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_ROCM=1 python test_pipeline_parallel_fwd_bwd.py NcclPipelineParallelForwardBackwardTest.test_inference_pipelining_without_interleaving

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0



======================================================================
ERROR: test_learning_async_pipelining_with_interleaving (test_pipeline_parallel_fwd_bwd.NcclPipelineParallelForwardBackwardTest)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/skishore/github/pytorch/torch/testing/_internal/common_distributed.py", line 605, in wrapper
    self._join_processes(fn)
  File "/skishore/github/pytorch/torch/testing/_internal/common_distributed.py", line 845, in _join_processes
    self._check_return_codes(elapsed_time)
  File "/skishore/github/pytorch/torch/testing/_internal/common_distributed.py", line 894, in _check_return_codes
    raise RuntimeError(error)
RuntimeError: Process 6 exited with error code 10 and exception:
Traceback (most recent call last):
  File "/skishore/github/pytorch/torch/testing/_internal/common_distributed.py", line 734, in run_test
    getattr(self, test_name)()
  File "/skishore/github/pytorch/torch/testing/_internal/common_distributed.py", line 607, in wrapper
    fn()
  File "/skishore/github/pytorch/torch/testing/_internal/common_utils.py", line 3173, in wrapper
    method(*args, **kwargs)
  File "/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py", line 272, in test_learning_async_pipelining_with_interleaving
    self._forward_backward_test_impl(
  File "/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py", line 180, in _forward_backward_test_impl
    loss = fwd_bwd_func(
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_with_interleaving.py", line 219, in _forward_backward_pipelining_with_interleaving
    p2p_communication.recv_forward(
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 342, in recv_forward
    input_tensor, _ = _communicate(
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 259, in _communicate
    tensor_send_prev_req, tensor_recv_prev_req, tensor_send_next_req, tensor_recv_next_req = _run_p2pops(tensor_send_prev, tensor_send_next, tensor_recv_prev, tensor_recv_next, async_comm=async_comm)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 97, in _run_p2pops
    reqs = torch.distributed.batch_isend_irecv(ops)
  File "/skishore/github/pytorch/torch/distributed/distributed_c10d.py", line 2717, in batch_isend_irecv
    p2p_op.op(
  File "/skishore/github/pytorch/torch/distributed/distributed_c10d.py", line 2411, in irecv
    return group.recv([tensor], group_src, tag)
torch.distributed.DistBackendError: NCCL error in: /skishore/github/pytorch/torch/csrc/distributed/c10d/NCCLUtils.cpp:77, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.25.1
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:
Error while creating shared memory segment /dev/shm/nccl-kcfrRq (size 8257920), error: No space left on device (28)

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_ROCM=1 python test_pipeline_parallel_fwd_bwd.py NcclPipelineParallelForwardBackwardTest.test_learning_async_pipelining_with_interleaving

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0



======================================================================
ERROR: test_learning_async_pipelining_without_interleaving (test_pipeline_parallel_fwd_bwd.NcclPipelineParallelForwardBackwardTest)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/skishore/github/pytorch/torch/testing/_internal/common_distributed.py", line 605, in wrapper
    self._join_processes(fn)
  File "/skishore/github/pytorch/torch/testing/_internal/common_distributed.py", line 845, in _join_processes
    self._check_return_codes(elapsed_time)
  File "/skishore/github/pytorch/torch/testing/_internal/common_distributed.py", line 894, in _check_return_codes
    raise RuntimeError(error)
RuntimeError: Process 1 exited with error code 10 and exception:
Traceback (most recent call last):
  File "/skishore/github/pytorch/torch/testing/_internal/common_distributed.py", line 734, in run_test
    getattr(self, test_name)()
  File "/skishore/github/pytorch/torch/testing/_internal/common_distributed.py", line 607, in wrapper
    fn()
  File "/skishore/github/pytorch/torch/testing/_internal/common_utils.py", line 3173, in wrapper
    method(*args, **kwargs)
  File "/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py", line 249, in test_learning_async_pipelining_without_interleaving
    self._forward_backward_test_impl(
  File "/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py", line 180, in _forward_backward_test_impl
    loss = fwd_bwd_func(
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_without_interleaving.py", line 349, in forward_backward_pipelining_without_interleaving
    send_forward(
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_without_interleaving.py", line 145, in send_forward
    p2p_communication.send_forward(
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 401, in send_forward
    _communicate(
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 259, in _communicate
    tensor_send_prev_req, tensor_recv_prev_req, tensor_send_next_req, tensor_recv_next_req = _run_p2pops(tensor_send_prev, tensor_send_next, tensor_recv_prev, tensor_recv_next, async_comm=async_comm)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 97, in _run_p2pops
    reqs = torch.distributed.batch_isend_irecv(ops)
  File "/skishore/github/pytorch/torch/distributed/distributed_c10d.py", line 2717, in batch_isend_irecv
    p2p_op.op(
  File "/skishore/github/pytorch/torch/distributed/distributed_c10d.py", line 2366, in isend
    return group.send([tensor], group_dst, tag)
torch.distributed.DistBackendError: NCCL error in: /skishore/github/pytorch/torch/csrc/distributed/c10d/NCCLUtils.cpp:77, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.25.1
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:
Error while creating shared memory segment /dev/shm/nccl-OSTKDz (size 8257920), error: No space left on device (28)

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_ROCM=1 python test_pipeline_parallel_fwd_bwd.py NcclPipelineParallelForwardBackwardTest.test_learning_async_pipelining_without_interleaving

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0



======================================================================
ERROR: test_learning_pipelining_with_interleaving (test_pipeline_parallel_fwd_bwd.NcclPipelineParallelForwardBackwardTest)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/skishore/github/pytorch/torch/testing/_internal/common_distributed.py", line 605, in wrapper
    self._join_processes(fn)
  File "/skishore/github/pytorch/torch/testing/_internal/common_distributed.py", line 845, in _join_processes
    self._check_return_codes(elapsed_time)
  File "/skishore/github/pytorch/torch/testing/_internal/common_distributed.py", line 894, in _check_return_codes
    raise RuntimeError(error)
RuntimeError: Process 3 exited with error code 10 and exception:
Traceback (most recent call last):
  File "/skishore/github/pytorch/torch/testing/_internal/common_distributed.py", line 734, in run_test
    getattr(self, test_name)()
  File "/skishore/github/pytorch/torch/testing/_internal/common_distributed.py", line 607, in wrapper
    fn()
  File "/skishore/github/pytorch/torch/testing/_internal/common_utils.py", line 3173, in wrapper
    method(*args, **kwargs)
  File "/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py", line 260, in test_learning_pipelining_with_interleaving
    self._forward_backward_test_impl(
  File "/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py", line 180, in _forward_backward_test_impl
    loss = fwd_bwd_func(
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_with_interleaving.py", line 219, in _forward_backward_pipelining_with_interleaving
    p2p_communication.recv_forward(
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 342, in recv_forward
    input_tensor, _ = _communicate(
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 259, in _communicate
    tensor_send_prev_req, tensor_recv_prev_req, tensor_send_next_req, tensor_recv_next_req = _run_p2pops(tensor_send_prev, tensor_send_next, tensor_recv_prev, tensor_recv_next, async_comm=async_comm)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 97, in _run_p2pops
    reqs = torch.distributed.batch_isend_irecv(ops)
  File "/skishore/github/pytorch/torch/distributed/distributed_c10d.py", line 2717, in batch_isend_irecv
    p2p_op.op(
  File "/skishore/github/pytorch/torch/distributed/distributed_c10d.py", line 2411, in irecv
    return group.recv([tensor], group_src, tag)
torch.distributed.DistBackendError: NCCL error in: /skishore/github/pytorch/torch/csrc/distributed/c10d/NCCLUtils.cpp:77, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.25.1
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:
Error while creating shared memory segment /dev/shm/nccl-UrziMF (size 8257920), error: No space left on device (28)

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_ROCM=1 python test_pipeline_parallel_fwd_bwd.py NcclPipelineParallelForwardBackwardTest.test_learning_pipelining_with_interleaving

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0



======================================================================
ERROR: test_learning_pipelining_without_interleaving (test_pipeline_parallel_fwd_bwd.NcclPipelineParallelForwardBackwardTest)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/skishore/github/pytorch/torch/testing/_internal/common_distributed.py", line 605, in wrapper
    self._join_processes(fn)
  File "/skishore/github/pytorch/torch/testing/_internal/common_distributed.py", line 845, in _join_processes
    self._check_return_codes(elapsed_time)
  File "/skishore/github/pytorch/torch/testing/_internal/common_distributed.py", line 894, in _check_return_codes
    raise RuntimeError(error)
RuntimeError: Process 1 exited with error code 10 and exception:
Traceback (most recent call last):
  File "/skishore/github/pytorch/torch/testing/_internal/common_distributed.py", line 734, in run_test
    getattr(self, test_name)()
  File "/skishore/github/pytorch/torch/testing/_internal/common_distributed.py", line 607, in wrapper
    fn()
  File "/skishore/github/pytorch/torch/testing/_internal/common_utils.py", line 3173, in wrapper
    method(*args, **kwargs)
  File "/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py", line 239, in test_learning_pipelining_without_interleaving
    self._forward_backward_test_impl(
  File "/skishore/github/apex/tests/L0/run_transformer/test_pipeline_parallel_fwd_bwd.py", line 180, in _forward_backward_test_impl
    loss = fwd_bwd_func(
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_without_interleaving.py", line 349, in forward_backward_pipelining_without_interleaving
    send_forward(
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_without_interleaving.py", line 145, in send_forward
    p2p_communication.send_forward(
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 401, in send_forward
    _communicate(
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 259, in _communicate
    tensor_send_prev_req, tensor_recv_prev_req, tensor_send_next_req, tensor_recv_next_req = _run_p2pops(tensor_send_prev, tensor_send_next, tensor_recv_prev, tensor_recv_next, async_comm=async_comm)
  File "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/apex/transformer/pipeline_parallel/p2p_communication.py", line 97, in _run_p2pops
    reqs = torch.distributed.batch_isend_irecv(ops)
  File "/skishore/github/pytorch/torch/distributed/distributed_c10d.py", line 2717, in batch_isend_irecv
    p2p_op.op(
  File "/skishore/github/pytorch/torch/distributed/distributed_c10d.py", line 2366, in isend
    return group.send([tensor], group_dst, tag)
torch.distributed.DistBackendError: NCCL error in: /skishore/github/pytorch/torch/csrc/distributed/c10d/NCCLUtils.cpp:77, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.25.1
ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
Last error:
Error while creating shared memory segment /dev/shm/nccl-abqPSD (size 8257920), error: No space left on device (28)

To execute this test, run the following from the base repo dir:
    PYTORCH_TEST_WITH_ROCM=1 python test_pipeline_parallel_fwd_bwd.py NcclPipelineParallelForwardBackwardTest.test_learning_pipelining_without_interleaving

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0



----------------------------------------------------------------------
Ran 102 tests in 2937.884s

FAILED (errors=8, skipped=44)
